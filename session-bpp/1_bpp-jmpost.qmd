---
title: "1. Calculate Bayesian Predictive Power"
author:
  - Daniel Sabanés Bové
  - Francois Mercier
date: last-modified
editor_options: 
  chunk_output_type: inline
format:
  html:
    code-fold: show
    html-math-method: mathjax
cache: true
---

The purpose of this document is to show how we can calculate the probability of success given interim data of a clinical trial, based on the TGI-OS joint model results.

## Setup and load data

Here we execute the R code from the setup and data preparation chapter, see the [full code here](0_setup.qmd).

```{r}
#| label: setup_and_load_data
#| echo: false
#| output: false
library(here)
options(knitr.duplicate.label = "allow")

knitr::purl(
    here("session-bpp/_setup.qmd"),
    output = here("session-bpp/_setup.R")
)
source(here("session-jm/_setup.R"))

knitr::purl(
    here("session-bpp/_load_data.qmd"),
    output = here("session-bpp/_load_data.R")
)
source(here("session-bpp/_load_data.R"))
```

## Fit our joint TGI-OS model

Let's fit again the same joint TGI-OS model as in the previous session. Now we just put all the code together in a single chunk, and it is a good repetition to see all the steps in one place:

```{r}
#| label: fit_joint_model

subj_df <- os_data |>
    mutate(study = "OAK") |>
    select(study, id, arm)
subj_data <- DataSubject(
    data = subj_df,
    subject = "id",
    arm = "arm",
    study = "study"
)
long_df <- tumor_data |>
    select(id, year, sld)
long_data <- DataLongitudinal(
    data = long_df,
    formula = sld ~ year
)
surv_data <- DataSurvival(
    data = os_data,
    formula = Surv(os_time, os_event) ~ ecog + age + race + sex
)
joint_data <- DataJoint(
    subject = subj_data,
    longitudinal = long_data,
    survival = surv_data
)

joint_mod <- JointModel(
    longitudinal = LongitudinalSteinFojo(
        mu_bsld = prior_normal(log(65), 1),
        mu_ks = prior_normal(log(0.52), 1),
        mu_kg = prior_normal(log(1.04), 1),
        omega_bsld = prior_normal(0, 3) |> set_limits(0, Inf),
        omega_ks = prior_normal(0, 3) |> set_limits(0, Inf),
        omega_kg = prior_normal(0, 3) |> set_limits(0, Inf),
        sigma = prior_normal(0, 3) |> set_limits(0, Inf)
    ),
    survival = SurvivalWeibullPH(
        lambda = prior_gamma(0.7, 1),
        gamma = prior_gamma(1.5, 1),
        beta = prior_normal(0, 20)
    ),
    link = linkGrowth(
        prior = prior_normal(0, 20)
    )
)

options("jmpost.prior_shrinkage" = 0.99)
initialValues(joint_mod, n_chains = CHAINS)

save_file <- here("session-bpp/jm1.rds")
if (file.exists(save_file)) {
    joint_results <- readRDS(save_file)
} else {
    joint_results <- sampleStanModel(
        joint_mod,
        data = joint_data,
        iter_sampling = ITER,
        iter_warmup = WARMUP,
        chains = CHAINS,
        parallel_chains = CHAINS,
        thin = CHAINS,
        seed = BAYES.SEED,
        refresh = REFRESH
    )
    saveObject(joint_results, file = save_file)
}
```

## Marginal Hazard Ratio Estimation

First, we will try to apply the methodology from [Oudenhoven et al (2020)](https://onlinelibrary.wiley.com/doi/10.1002/sim.8713) to estimate the marginal hazard ratio, using our joint model results.
We use the new `jmpost` function called `populationHR()` for this:

```{r}
save_file <- here("session-bpp/jm1hr.rds")
if (file.exists(save_file)) {
    pop_hr <- readRDS(save_file)
} else {
    pop_hr <- populationHR(
        joint_results,
        hr_formula = ~arm
    )
    saveRDS(pop_hr, file = save_file)
}
pop_hr$summary
```

So here we have the marginal log hazard ratio estimates of the baseline spline components and the treatment arm.
Therefore we can get the hazard ratio estimates by exponentiating the log hazard ratio estimates:

```{r}
hr_est <- pop_hr$summary["armMPDL3280A", ] |>
    sapply(exp)
```

So we get a marginal hazard ratio estimate of around `r round(hr_est["mean"], 3)` and a 95% credible interval of `r paste0(signif(hr_est[c("X2.5.", "X97.5.")], 2), collapse = " - ")`. 

## Comparison with simple Cox PH results

We can do a little sanity check by comparing this with a very simple Cox model:

```{r}
cox_mod <- survival::coxph(
    update(joint_results@data@survival@formula, ~ . + arm),
    data = joint_results@data@survival@data
)
summary(cox_mod)
```

Here we get a different HR estimate of around `r round(exp(coef(cox_mod)["armMPDL3280A"]), 2)`, which is lower than the one we got from the joint model. 
We should expect different results because the joint model takes into account the TGI effect, while the Cox model does not.

And if we just put the treatment arm into the Cox model we get:

```{r}
cox_mod_arm <- survival::coxph(
    update(joint_results@data@survival@formula, . ~ arm),
    data = joint_results@data@survival@data
)
cox_mod_arm
```

Here we get a HR of `r round(exp(coef(cox_mod_arm)["armMPDL3280A"]), 3)` that is close to the marginal HR estimate above (when we did not condition on the other covariates), which is reassuring.
On the other hand, the 95% confidence interval is:

```{r}
exp(confint(cox_mod_arm)["armMPDL3280A", ])
```

so actually overlaps the null hypothesis of 1. So we see that the joint model helped us to obtain a more precise estimate of the marginal hazard ratio.

## PTS based on frequentist HR properties

From now on we pretend that the data we have analyzed so far is the interim data of the Oak trial, and we want to calculate the predictive power of the trial based on this interim data.

Let's first try to use the log HR ($\delta$) frequentist distribution to calculate the predictive power of the trial. We have:

$$
\hat{\delta}_{\text{fin}} \vert \hat{\delta}_{\text{int}} 
\sim
\text{Normal}(\hat{\delta}_{\text{int}}, \sigma^2_{\text{int}}) 
$$

where the predictive variance is given by

$$
\sigma^2_{\text{int}} = \frac{1}{4\overline{p}r(1-r)} \frac{\overline{m}}{\overline{n} (\overline{n} + \overline{m})}
$$

On the other hand, assuming that we have observed the final data, then the variance estimate for the estimator $\hat{\delta}_{\text{fin}}$ is given by:

$$
\sigma^2_{\text{fin}} = \frac{1}{4\overline{p}r(1-r)} \frac{1}{\overline{n} + \overline{m}}
$$

So the $(1-\alpha)$-confidence interval for the final log HR is given by:

$$
\hat{\delta}_{\text{fin}} \pm z_{1 - \alpha/2} \sigma_{\text{fin}}
$$

and the null hypothesis is rejected if the upper bound of this confidence interval is below 0, i.e. if:

$$
\hat{\delta}_{\text{fin}} + z_{1 - \alpha/2} \sigma_{\text{fin}} < 0.
$$

Now let's again take the perspective that we are at the interim analysis and we want to calculate the predictive probability of this event, then:

$$
\begin{align*}
\mathbb{P}(\hat{\delta}_{\text{fin}} + z_{1 - \alpha/2} \sigma_{\text{fin}} < 0)
&= 
\mathbb{P}(\hat{\delta}_{\text{fin}} < - z_{1 - \alpha/2} \sigma_{\text{fin}}) \\
&= 
\mathbb{P}\left(
\frac{\hat{\delta}_{\text{fin}} - \hat{\delta}_{\text{int}}}{\sigma_{\text{int}}} <
\frac{- z_{1 - \alpha/2} \sigma_{\text{fin}} - \hat{\delta}_{\text{int}}}{\sigma_{\text{int}}}
\right) \\
&= 
\Phi\left(
  \frac{- z_{1 - \alpha/2} \sigma_{\text{fin}} - \hat{\delta}_{\text{int}}}{\sigma_{\text{int}}}
\right)
\end{align*}
$$

because the left-hand side is a standard normal variable according to our formula above.
Let's first write a little function to calculate this:

```{r}
pts_freq_hr <- function(delta_int, var_int, var_final, alpha = 0.05) {
    z_alpha <- qnorm(1 - alpha / 2)
    delta_final <- -z_alpha * sqrt(var_final)
    pnorm((delta_final - delta_int) / sqrt(var_int))
}
```

For $\hat{\delta}_{\text{int}}$ we are going to plug in our MCMC samples for the marginal log HR $\hat{\delta}_{\text{int}}$ to compute the PTS based on this:

```{r}
log_hr_samples <- pop_hr[[2]]["armMPDL3280A", ]
```

But first we also need the quantities that go into the variance formula:

- $\overline{p}$: average event rate
- $r$: proportion of patients in the treatment arm
- $\overline{n}$: average arm size in interim data
- $\overline{m}$: average arm size in follow up

So let's calculate these and the resulting variances:

```{r}
avg_event_rate <- mean(os_data$os_event)
prop_pts_treatment <- mean(os_data$arm == "MPDL3280A")
avg_arm_size_interim <- mean(table(os_data$arm))
total_size_final <- 850
avg_arm_size_followup <- (total_size_final - 2 * avg_arm_size_interim) / 2
var_int <- 1 / (4 * avg_event_rate * prop_pts_treatment * (1 - prop_pts_treatment)) *
    avg_arm_size_followup / (avg_arm_size_interim * (avg_arm_size_interim + avg_arm_size_followup))
var_final <- 1 / (4 * avg_event_rate * prop_pts_treatment * (1 - prop_pts_treatment)) *
    1 / (avg_arm_size_interim + avg_arm_size_followup)
```

Now we can calculate the PTS based on the frequentist HR properties:

```{r}
pts_freq_hr_result <- mean(pts_freq_hr(
    delta_int = log_hr_samples,
    var_int = var_int,
    var_final = var_final
))
pts_freq_hr_result
```

So we obtain a PTS of `r round(pts_freq_hr_result * 100, 1)`% here.

## PTS based on frequentist log-rank test properties

Now we want to leverage the `rpact` function [`getConditionalPower()`](https://rpact-com.github.io/rpact/reference/getConditionalPower.html) to calculate the predictive power of the trial based on the log-rank test statistic properties.

Looking at the function's example, we first have to create a `DataSet` object with the interim data.
Here it is important that we for `cumLogRanks` also the z scores from a Cox regression can be used, which means for our case that can insert here the z-scores defined as:

$$
z = \frac{\hat{\delta}_{\text{int}}}{\sigma_{\text{int}}}
$$

values.

But let's first try this with a simple fixed $z$ value to see how it works:

```{r}
library(rpact)

z <- log(0.9) / sqrt(var_int) # Instead of log(0.9) we put later the marginal log HR MCMC sample
data <- getDataset(
    cumEvents = sum(os_data$os_event),
    cumLogRanks = z,
    cumAllocationRatios = prop_pts_treatment
)
data
```

This is the data set from the first stage, i.e. our interim analysis.

Now we need to define the design of the group sequential trial:

```{r}
events_final <- total_size_final * avg_event_rate
design <- getDesignGroupSequential(
    kMax = 2,
    informationRates = c(sum(os_data$os_event) / events_final, 1),
    alpha = 0.05
)
design
```

We see that the significance level is almost full for the final analysis, because by default the O'Brien & Fleming design is used, which only assigns a very small $\alpha$ to the interim analysis. This is kind of what we need here.

Finally we put the `design` and `data` in a `StageResults` object, which is the input for the `getConditionalPower()` function:

```{r}
stageResults <- getStageResults(
    design = design,
    dataInput = data,
    stage = 1,
    directionUpper = FALSE
)
condPower <- getConditionalPower(
    stageResults = stageResults,
    thetaH1 = 0.9, # Here we put later the marginal hazard ratio MCMC sample
    nPlanned = total_size_final,
    allocationRatioPlanned = 1
)
summary(condPower)
condPower$conditionalPower[2]
```

OK so now that we know how it works we wrap this in a little function again.

```{r}
pts_freq_hr2 <- function(delta_int, var_int, var_final, events_int, alloc_int, events_final, alpha = 0.05) {
    data <- getDataset(
        cumEvents = events_int,
        cumLogRanks = delta_int / sqrt(var_int),
        cumAllocationRatios = alloc_int
    )
    design <- getDesignGroupSequential(
        kMax = 2,
        informationRates = c(events_int / events_final, 1),
        alpha = alpha
    )
    stageResults <- getStageResults(
        design = design,
        dataInput = data,
        stage = 1,
        directionUpper = FALSE
    )
    condPower <- getConditionalPower(
        stageResults = stageResults,
        thetaH1 = exp(delta_int),
        nPlanned = total_size_final,
        allocationRatioPlanned = 1
    )
    condPower$conditionalPower[2]
}
```

Let's try it out with the same example first again:

```{r}
pts_freq_hr2(
    delta_int = log(0.9),
    var_int = var_int,
    var_final = var_final,
    events_int = sum(os_data$os_event),
    alloc_int = prop_pts_treatment,
    events_final = total_size_final * avg_event_rate
)
```

OK so that gives the same result as before, which is good. Now we can plug in the MCMC samples for the marginal log HR:

```{r}
save_file <- here("session-bpp/jm1hr2.rds")
if (file.exists(save_file)) {
    pts_freq_hr2_result <- readRDS(save_file)
} else {
    pts_freq_hr2_samples <- sapply(
        log_hr_samples,
        pts_freq_hr2,
        var_int = var_int,
        var_final = var_final,
        events_int = sum(os_data$os_event),
        alloc_int = prop_pts_treatment,
        events_final = total_size_final * avg_event_rate
    )
    saveRDS(pts_freq_hr2_samples, file = save_file)
}
pts_freq_hr2_result <- mean(pts_freq_hr2_samples)
```

So here we get a PTS of `r round(pts_freq_hr2_result * 100, 1)`%, which is higher than the one we got before.

## Individual Samples based Predictive Power

This is a good motivation to try out the conditional sampling approach to calculate the predictive power, which is based on the individual samples of the joint model. It does not involved any frequentist assumptions to calculate the PTS.




- Sample events for the patients who did not have an event yet
  - based on their conditional survival distribution estimated by the TGI-OS model
- Determine the cutoff time based on the total number of events
- Plug this data set into e.g. log rank test and get test statistic
- Doing this over all the samples, we get a distribution of the test statistic
  - Maybe even do this 10 times for each sample because we have some uncertainty there too
- Then can just look at the proportion of test statistic samples which are above our critical value and then this is our predictive power  