---
title: "1. TGI model minimal workflow"
author:
  - Francois Mercier
  - Daniel Sabanés Bové
date: last-modified
editor_options: 
  chunk_output_type: inline
format:
  html:
    code-fold: show
math: true
---

## Note

The purpose of this document is to show a minimal workflow for fitting a TGI model using the `brms` package.

## Setup and load data

{{< include _setup_and_load.qmd >}}

{{< include _load_data.qmd >}}

For simplicity, we will for now just use studies 1 and 3 (Atezolizumab in NSCLC), and we rename the patient IDs:

```{r}
df <- tumor_data |> 
  filter(study %in% c("1", "3")) |> 
  na.omit() |>
  droplevels() |> 
  mutate(id = factor(as.numeric(id)))
```

Here we have `r nlevels(df$id)` patients. It is always a good idea to make a plot of the data. Let's look at the first 20 patients e.g.:

```{r}
df |> 
  filter(as.integer(id) <= 20) |>
  ggplot(aes(x = day, y = sld, group = id)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ id) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme(legend.position = "none")
```

## Generalized Stein-Fojo model

We start from the generalized Stein-Fojo model as shown in the slides:

$$
\mu(t_{ij}) = \mu_{0i} \{\phi_i \exp(-s_i \cdot t_{ij}) + (1 - \phi_i) \exp(g_i \cdot t_{ij}) \}
$$

### Mean

We will make one more tweak here. If the time $t$ is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. That is, we have then $\phi_i = 0$. Therefore, the final model for the mean SLD is:

$$
\mu(t_{ij}) = 
\begin{cases} 
\mu_{0i} \exp(g_i \cdot t_{ij}) & \text{if } t_{ij} < 0 \\
\mu_{0i} \{\phi_i \exp(-s_i \cdot t_{ij}) + (1 - \phi_i) \exp(g_i \cdot t_{ij}) \} & \text{if } t_{ij} \geq 0 
\end{cases}
$$

### Likelihood

For the likelihood given the mean SLD, we will assume a normal distribution with a constant coefficient of variation $\sigma$:

$$
y(t_{ij}) \sim \text{N}(\mu(t_{ij}), (\mu(t_{ij})\sigma)^2)
$$

### Random effects

Next, we define the distributions of the random effects $\mu_{0i}$, $s_i$, $g_i$, and $\phi_i$, $i = 1, \dotsc, n$:

$$
\begin{align*}
\mu_{0i} &\sim \text{LogNormal}(\mu_{0}, \omega_{0}^2) \\
s_i &\sim \text{LogNormal}(\mu_{s}, \omega_{s}^2) \\
g_i &\sim \text{LogNormal}(\mu_{g}, \omega_{g}^2) \\
\phi_i &\sim \text{LogitNormal}(\mu_{\phi}, \omega_{\phi}^2)
\end{align*}
$$

### Priors

Finally, we need to define the priors for the hyperparameters. We do this in a kind of empirical Bayes fashion, i.e. we use the data to inform the priors.

For the prior on the average baseline $\mu_0$ and its standard deviation $\omega_0$, we can use the mean and standard deviation of the baseline SLD:

```{r}
baseline <- df |>
  filter(day <= 0, day > -7) |> 
  group_by(id) |> 
  summarize(mean_sld = mean(sld)) |> 
  pull(mean_sld)
(mean_bl <- round(mean(baseline)))
(sd_bl <- round(sd(baseline)))
```

Therefore we take the priors as log normal priors around the log values as means: 

$$
\begin{align*}
\mu_{0} &\sim \text{LogNormal}(`r round(log(mean_bl))`, 5) \\
\omega_{0} &\sim \text{LogNormal}(`r round(log(sd_bl))`, 1) \\
$$

For the coefficient of variation $\sigma$, we try to use a non-informative prior:

$$
\sigma \sim \text{LogNormal}(\log(0.1), 1)
$$

For the average shrinkage and growth rates, $\mu_s$ and $\mu_g$, we consider that these are per day rates, so should be rather small. Since the growth will be more and more dominant for larger times $t$, we can assume that the growth rate is smaller than the shrinkage rate. Therefore, we can use the following priors:

$$
\begin{align*}
\mu_{s} &\sim \text{LogNormal}(\log(0.01), 1) \\
\mu_{g} &\sim \text{LogNormal}(\log(0.001), 1) 
\end{align*}
$$

For the average shrinkage proportion $\mu_{\phi}$, we center the prior around $\text{logit}^{-1}(-1) \approx `r round(plogis(-1), 2)`$ and truncate the normal prior to be below 0. This means $\phi < 0.5$. This turned out here to be necessary to avoid divergences in the model fitting - with an unconstrained prior, the model would sometimes converge to a value of $\phi$ close to 1 with corresponding very large growth samples to compensate.

$$
\mu_{\phi} \sim \text{LogitNormalNegative}(-1, 0.5)
$$

For the standard deviations, we use:

$$
\begin{align*}
\omega_{s} &\sim \text{LogNormal}(\log(0.1), 1) \\
\omega_{g} &\sim \text{LogNormal}(\log(1), 0.5) \\
\omega_{\phi} &\sim \text{LogNormal}(\log(1), 0.5)
\end{align*}
$$

## Fit model

We can now fit the model using `brms`. The structure is determined by the model formula:

```{r}
formula <- bf(sld ~ eta, nl = TRUE) +
  # Define the mean for the likelihood
  nlf(
    eta ~ 
      step(day > 0) * 
        (mu0 * (phi * exp(-s * day) + (1 - phi) * exp(g * day))) +
      step(day <= 0) * 
        (mu0 * exp(g * day))
  ) +
  # Define the standard deviation as a 
  # coefficient tau times the mean.
  # sigma is modelled on the log scale though, therefore:
  nlf(sigma ~ ltau + log(eta)) +
  lf(ltau ~ 1) +
  # Define nonlinear parameter transformations
  nlf(mu0 ~ exp(lmu0)) +
  nlf(phi ~ inv_logit(tphi)) +
  nlf(s ~ exp(ls)) +
  nlf(g ~ exp(lg)) +
  # Define random effect structure
  lf(lmu0 ~ 1 + (1 | id)) + 
  lf(tphi ~ 1 + (1 | id)) + 
  lf(ls ~ 1 + (1 | id)) +
  lf(lg ~ 1 + (1 | id))

# Define the priors
priors <- c(
  prior(normal(log(39), 1), nlpar = "lmu0"),
  prior(normal(log(0.01), 0.1), nlpar = "ls"),
  prior(normal(log(0.001), 1), nlpar = "lg"),
  prior(normal(-1, 0.5), nlpar = "tphi", ub = 0),
  prior(lognormal(log(25), 1), nlpar = "lmu0", class = "sd"),
  prior(lognormal(log(0.1), 1), nlpar = "ls", class = "sd"),
  prior(lognormal(0, 0.5), nlpar = "lg", class = "sd"),
  prior(lognormal(1, 0.5), nlpar = "tphi", class = "sd"),
  prior(normal(log(0.1), 1), nlpar = "ltau")
)

# Initial values to avoid problems at the beginning
n_patients <- nlevels(df$id)
inits <- list(
  b_lmu0 = array(log(34)),
  b_ls = array(log(0.01)),
  b_lg = array(log(0.001)),
  b_tphi = array(-1),
  sd_1 = array(10),
  sd_2 = array(0.1),
  sd_3 = array(1),
  sd_4 = array(1),
  b_ltau = array(log(0.1)),
  z_1 = matrix(0, nrow = 1, ncol = n_patients),
  z_2 = matrix(0, nrow = 1, ncol = n_patients),
  z_3 = matrix(0, nrow = 1, ncol = n_patients),
  z_4 = matrix(0, nrow = 1, ncol = n_patients)
)

# Fit the model
save_file <- here("session-tgi/fit3.RData")
if (file.exists(save_file)) {
  load(save_file)
} else {
  fit <- brm(
    formula = formula,
    data = df,
    prior = priors,
    family = gaussian(),
    init = rep(list(inits), CHAINS),
    chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES.SEED,
    control = CONTROL, refresh = REFRESH, thin = THIN
  )
  save(fit, file = save_file)
}

# Summarize the fit
summary(fit)
```

In total this took 77 minutes on my laptop. 

## Parameter estimates

Here we extract all parameter estimates using the `as_draws_df` method for `brmsfit` objects. Note that we could also just extract a subset of parameters, see `?as_draws.brmsfit` for more information.

```{r}
post_df <- as_draws_df(fit)
head(names(post_df), 10)

post_df <- post_df |>
  mutate(
    mu_0 = exp(b_lmu0_Intercept),
    mu_phi = plogis(b_tphi_Intercept),
    mu_s = exp(b_ls_Intercept),
    mu_g = exp(b_lg_Intercept),
    omega_0 = sd_id__lmu0_Intercept,
    omega_phi = sd_id__tphi_Intercept,
    omega_s = sd_id__ls_Intercept,
    omega_g = sd_id__lg_Intercept,
    sigma = exp(b_ltau_Intercept)
  )
```

Let's first look at the population level parameters:

```{r}
mcmc_trace(post_df, pars = c("mu_0", "mu_phi", "mu_s", "mu_g", "sigma"))
mcmc_pairs(post_df, pars = c("mu_0", "mu_phi", "mu_s", "mu_g", "sigma"))

post_sum <- post_df |>
  select(mu_0, mu_phi, mu_s, mu_g, omega_0, omega_phi, omega_s, omega_g, sigma) |>
  summarize_draws() |>
  gt() |>
  fmt_number(decimals = 3)
```

We can also look at individual level parameters. This is simplified by using the `spread_draws()` function:

```{r}
head(get_variables(fit), 20)
post_ind <- fit |> 
  spread_draws(
    b_lmu0_Intercept,
    r_id__lmu0[id, ],
    b_tphi_Intercept, 
    r_id__tphi[id, ],
    b_ls_Intercept,
    r_id__ls[id, ],
    b_lg_Intercept,
    r_id__lg[id, ]
  ) |> 
  mutate(
    mu0 = exp(b_lmu0_Intercept + r_id__lmu0),
    phi = plogis(b_tphi_Intercept + r_id__tphi),
    s = exp(b_ls_Intercept + r_id__ls),
    g = exp(b_lg_Intercept + r_id__lg)
  ) |> 
  select(
    .chain, .iteration, .draw,
    id, mu0, phi, s, g    
  )
```

With this we can e.g. report the estimates for the first patient:

```{r}
post_sum_id1 <- post_ind |>
  filter(id == "1") |>
  summarize_draws() |>
  gt() |>
  fmt_number(n_sigfig = 3)
```

## Observation vs model fit

We can now compare the model fit to the observations. Let's do this for the first 20 patients again:

```{r}
pt_subset <- as.character(1:20)
df_subset <- df |> 
  filter(id %in% pt_subset)

df_sim <- df_subset |> 
  data_grid(
    id = pt_subset, 
    day = seq_range(day, 101)
  ) |>
  add_epred_draws(fit) |>
  median_qi()

df_sim |>
  ggplot(aes(x = day, y = sld)) +
  facet_wrap(~ id) +
  geom_ribbon(
    aes(y = .epred, ymin = .lower, ymax = .upper), 
    alpha = 0.3, 
    fill = "deepskyblue"
  ) +
  geom_line(aes(y = .epred), color = "deepskyblue") +
  geom_point(data = df_subset, color = "tomato") +
  coord_cartesian(ylim = range(df_subset$sld)) +
  scale_fill_brewer(palette = "Greys") +
  labs(title = "GSF model fit")
```

This does not look right. 

Let's code the function that gives the GSF mean:

```{r}
gsf_mean <- function(mu0, phi, s, g, day) {
  ifelse(day < 0, mu0 * exp(g * day), mu0 * (phi * exp(-s * day) + (1 - phi) * exp(g * day)))
}
```

Let's apply this to the samples of patient number 1:

```{r}
post_ind_1 <- post_ind |>
  filter(id == "1") |> 
  cross_join(data.frame(day = (-7):100)) |> 
  mutate(mu = gsf_mean(mu0, phi, s, g, day)) |> 
  group_by(day) |> 
  summarize(med = median(mu), avg = mean(mu))

post_ind_1 |> 
  ggplot(aes(x = day)) +
  geom_line(aes(y = med), color = "deepskyblue") +
  geom_line(aes(y = avg), color = "tomato") +
  geom_point(data = df_subset |> filter(id == "1"), aes(y = sld), color = "black")
```


## Tips and tricks

- When the model breaks down completely, that is a sign that likely the model specification is wrong. For example, in above code I first did not understand how to model the `sigma` parameter correctly, which led to a model where each chain was completely stuck at its initial values.
- In that case and in general if you are not sure whether the `brms` model specification is correct, you can check the Stan code that is generated by `brms`:

  ```{r}
  # Good to check the stan code:
  # (This also helps to find the names of the parameters
  # for which to define the initial values below)
  stancode(formula, prior = priors, data = df, family = gaussian())
  ```

  Here e.g. it is important to see that the `sigma` parameter is modelled on the log scale.
- It is important to take divergence warnings seriously. For the model parameters where `Rhat` is larger than 1.05 or so, you can just have a look at the traceplot. For example, in an earlier model fit the $\log(\mu_{\phi})$ population parameter had this traceplot:

  ```{r}
  fit_save <- fit
  load(here("session-tgi/fit.RData"))
  plot(fit, pars = "b_tphi")
  fit <- fit_save
  ```

  We see that two chains led to a $\log(\mu_{\phi})$ value below 0, while two chains hovered around much larger values between 5 and 15, which corresponds to $\mu_{\phi} = 1$. This shows that the model is rather overparametrized. By restricting the range of the prior for $\log(\mu_{\phi})$ to be below 0, we could avoid this problem.
- Providing manually realistic initial values for the model parameters can help to speed up the convergence of the Markov chains. This is especially important for the nonlinear parameters. You can get the names of the parameters from the Stan code above - note that you cannot use the names of the `brms` R code. 
- Sometimes it is not easy to get the dimensions of the inital values right. Here it can be helpful to change the backend to `rstan`, which provides more detailed error messages. You can do this by setting `backend = "rstan"` in the `brm` call.
- The priors for this model are not really subjective Bayesian priors. Here, it took me a couple of tries, where I started with one prior, ran the model, saw where the parameter samples were, and then adjusted the priors to be in a similar range. Ideally, we could avoid this by either knowing more about the parameters e.g. from previous clinical trial analyses in the same molecule and therapeutic area, or by having some simple ad-hoc calculations for each subject which can then translate to the population level (in a similar way as we did for the baseline SLD).