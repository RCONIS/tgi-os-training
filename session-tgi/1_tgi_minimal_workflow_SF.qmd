---
title: "1. TGI model minimal workflow (SF)"
author:
  - Daniel Sabanés Bové
  - Francois Mercier
date: last-modified
editor_options: 
  chunk_output_type: inline
format:
  html:
    code-fold: show
math: true
---

## Note

The purpose of this document is to show a minimal workflow for fitting a TGI model using the `brms` package.

## Setup and load data

{{< include _setup_and_load.qmd >}}

{{< include _load_data.qmd >}}

For simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:

```{r}
#| label: subset_oak
df <- tumor_data |> 
  filter(study == "4") |> 
  na.omit() |>
  droplevels() |> 
  mutate(id = factor(as.numeric(id)))
```

Here we have `r nlevels(df$id)` patients. It is always a good idea to make a plot of the data. Let's look at the first 20 patients e.g.:

```{r}
#| label: plot_data
df |> 
  filter(as.integer(id) <= 20) |>
  ggplot(aes(x = year, y = sld, group = id)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ id) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme(legend.position = "none")
```

## Stein-Fojo model

We start from the Stein-Fojo model as shown in the slides:

$$
y^{*}(t_{ij}) = \psi_{b_{0}i} \{\exp(- \psi_{k_{s}i} \cdot t_{ij}) + \exp(\psi_{k_{g}i} \cdot t_{ij}) - 1\}
$$

### Mean

We will make one more tweak here. If the time $t$ is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. Therefore, the final model for the mean SLD is:

$$
y^{*}(t_{ij}) = 
\begin{cases} 
\psi_{b_{0}i} \exp(\psi_{k_{g}i} \cdot t_{ij}) & \text{if } t_{ij} < 0 \\
\psi_{b_{0}i} \{\exp(-\psi_{k_{s}i} \cdot t_{ij}) + \exp(\psi_{k_{g}i} \cdot t_{ij}) - 1\} & \text{if } t_{ij} \geq 0 
\end{cases}
$$

### Likelihood

For the likelihood given the mean SLD $y^{*}$, we will assume a normal distribution with a constant coefficient of variation $\sigma$:

$$
y(t_{ij}) \sim \text{N}(y^{*}(t_{ij}), (y^{*}(t_{ij})\sigma)^2)
$$

This can also be written as:

$$
y(t_{ij}) = (1 + \epsilon_{ij}) \cdot y^{*}(t_{ij})
$$

where $\epsilon_{ij} \sim \text{N}(0, \sigma^2)$.

Note that also the additive model is a possible choice, where

$$
y(t_{ij}) = y^{*}(t_{ij}) + \epsilon_{ij}
$$

such that the error does not depend on the scale of the SLD any longer.

### Random effects

Next, we define the distributions of the random effects $\psi_{b_{0}i}$, $\psi_{k_{s}i}$, $\psi_{k_{g}i}$, for $i = 1, \dotsc, n$:

$$
\begin{align*}
\psi_{b_{0}i} &\sim \text{LogNormal}(b_{0}, \omega_{0}^2) \\
\psi_{k_{s}i} &\sim \text{LogNormal}(k_{s}, \omega_{s}^2) \\
\psi_{k_{g}i} &\sim \text{LogNormal}(k_{g}, \omega_{g}^2)
\end{align*}
$$

### Priors

Finally, we need to define the priors for the hyperparameters. We do this in a kind of empirical Bayes fashion, i.e. we use the data to inform the priors.

For the prior on the average baseline $\mu_0$ and its standard deviation $\omega_0$, we can use the mean and standard deviation of the baseline SLD:

```{r}
#| label: calculate_baseline
baseline <- df |>
  filter(year <= 0) |> 
  group_by(id) |> 
  summarize(mean_sld = mean(sld)) |> 
  pull(mean_sld)
(mean_bl <- round(mean(baseline)))
(sd_bl <- round(sd(baseline)))
```

Therefore we take the priors as log normal priors around the log values as means: 

$$
\begin{align*}
b_{0} &\sim \text{LogNormal}(`r round(log(mean_bl))`, 5) \\
\omega_{0} &\sim \text{LogNormal}(`r round(log(sd_bl))`, 1) \\
$$

For the coefficient of variation $\sigma$, we try to use a non-informative prior:

$$
\sigma \sim \text{LogNormal}(\log(0.1), 1)
$$

For the average shrinkage and growth rates, $k_s$ and $k_g$, we parametrize the time $t$ in years, such that these are per year rates. Since the growth will be more and more dominant for larger times $t$, we can assume that the growth rate is smaller than the shrinkage rate. Therefore, we can use the following priors:

$$
\begin{align*}
k_{s} &\sim \text{LogNormal}(\log(1), 1) \\
k_{g} &\sim \text{LogNormal}(\log(0.1), 1) 
\end{align*}
$$

For the standard deviations, we use:

$$
\begin{align*}
\omega_{s} &\sim \text{LogNormal}(\log(0.1), 1) \\
\omega_{g} &\sim \text{LogNormal}(\log(1), 0.5)
\end{align*}
$$

## Fit model

We can now fit the model using `brms`. The structure is determined by the model formula:

```{r}
#| label: fit_brms
formula <- bf(sld ~ eta, nl = TRUE) +
  # Define the mean for the likelihood
  nlf(
    eta ~ 
      step(year > 0) * 
        (b0 * (exp(-ks * year) + exp(kg * year) -  1)) +
      step(year <= 0) * 
        (b0 * exp(kg * year))
  ) +
  # Define the standard deviation as a 
  # coefficient tau times the mean.
  # sigma is modelled on the log scale though, therefore:
  nlf(sigma ~ ltau + log(eta)) +
  lf(ltau ~ 1) +
  # Define nonlinear parameter transformations
  nlf(b0 ~ exp(lb0)) +
  nlf(ks ~ exp(lks)) +
  nlf(kg ~ exp(lkg)) +
  # Define random effect structure
  lf(lb0 ~ 1 + (1 | id)) + 
  lf(lks ~ 1 + (1 | id)) +
  lf(lkg ~ 1 + (1 | id))

# Define the priors
priors <- c(
  prior(normal(log(43), 1), nlpar = "lb0"),
  prior(normal(log(1), 0.1), nlpar = "lks"),
  prior(normal(log(0.1), 1), nlpar = "lkg"),
  prior(lognormal(log(26), 1), nlpar = "lb0", class = "sd"),
  prior(lognormal(log(0.1), 1), nlpar = "lks", class = "sd"),
  prior(lognormal(0, 0.5), nlpar = "lkg", class = "sd"),
  prior(normal(log(0.1), 1), nlpar = "ltau")
)

# Initial values to avoid problems at the beginning
n_patients <- nlevels(df$id)
inits <- list(
  b_lb0 = array(log(43)),
  b_lks = array(log(1)),
  b_lkg = array(log(0.1)),
  sd_1 = array(10),
  sd_2 = array(0.1),
  sd_3 = array(1),
  b_ltau = array(log(0.1)),
  z_1 = matrix(0, nrow = 1, ncol = n_patients),
  z_2 = matrix(0, nrow = 1, ncol = n_patients),
  z_3 = matrix(0, nrow = 1, ncol = n_patients)
)

# Fit the model
save_file <- here("session-tgi/fit5.RData")
if (file.exists(save_file)) {
  load(save_file)
} else {
  fit <- brm(
    formula = formula,
    data = df,
    prior = priors,
    family = gaussian(),
    init = rep(list(inits), CHAINS),
    chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES.SEED,
    control = CONTROL, refresh = REFRESH, thin = THIN
  )
  save(fit, file = save_file)
}

# Summarize the fit
summary(fit)
```

## Parameter estimates

Here we extract all parameter estimates using the `as_draws_df` method for `brmsfit` objects. Note that we could also just extract a subset of parameters, see `?as_draws.brmsfit` for more information.

```{r}
#| label: extract_parameters
post_df <- as_draws_df(fit)
head(names(post_df), 10)

post_df <- post_df |>
  mutate(
    b0 = exp(b_lb0_Intercept),
    ks = exp(b_lks_Intercept),
    kg = exp(b_lkg_Intercept),
    omega_0 = sd_id__lb0_Intercept,
    omega_s = sd_id__lks_Intercept,
    omega_g = sd_id__lkg_Intercept,
    sigma = exp(b_ltau_Intercept)
  )
```

Let's first look at the population level parameters:

```{r}
#| label: summarize_parameters

mcmc_trace(post_df, pars = c("b0", "ks", "kg", "sigma"))
mcmc_pairs(post_df, pars = c("b0", "ks", "kg", "sigma"))

post_sum <- post_df |>
  select(b0, ks, kg, omega_0, omega_s, omega_g, sigma) |>
  summarize_draws() |>
  gt() |>
  fmt_number(n_sigfig = 3)
```

We can also look at individual level parameters. This is simplified by using the `spread_draws()` function:

```{r}
#| label: extract_individual_parameters

head(get_variables(fit), 20)
post_ind <- fit |> 
  spread_draws(
    b_lb0_Intercept,
    r_id__lb0[id, ],
    b_lks_Intercept,
    r_id__lks[id, ],
    b_lkg_Intercept,
    r_id__lkg[id, ]
  ) |> 
  mutate(
    b0 = exp(b_lb0_Intercept + r_id__lb0),
    ks = exp(b_lks_Intercept + r_id__lks),
    kg = exp(b_lkg_Intercept + r_id__lkg)
  ) |> 
  select(
    .chain, .iteration, .draw,
    id, b0, ks, kg    
  )
```

With this we can e.g. report the estimates for the first patient:

```{r}
#| label: first_patient_estimates

post_sum_id1 <- post_ind |>
  filter(id == "1") |>
  summarize_draws() |>
  gt() |>
  fmt_number(n_sigfig = 3)
```

## Observation vs model fit

We can now compare the model fit to the observations. Let's do this for the first 20 patients again:

```{r}
#| label: obs_vs_fit_plots

pt_subset <- as.character(21:40)
df_subset <- df |> 
  filter(id %in% pt_subset)

df_sim <- df_subset |> 
  data_grid(
    id = pt_subset, 
    year = seq_range(year, 101)
  ) |>
  add_linpred_draws(fit) |> 
  median_qi() |> 
  mutate(sld_pred = .linpred)

df_sim_30 <- df_sim |> filter(id == "30")

df_sim |>
  ggplot(aes(x = year, y = sld)) +
  facet_wrap(~ id) +
  geom_ribbon(
    aes(y = sld_pred, ymin = .lower, ymax = .upper),
    alpha = 0.3, 
    fill = "deepskyblue"
  ) +
  geom_line(aes(y = sld_pred), color = "deepskyblue") +
  geom_point(data = df_subset, color = "tomato") +
  coord_cartesian(ylim = range(df_subset$sld)) +
  scale_fill_brewer(palette = "Greys") +
  labs(title = "SF model fit")
```

## Tips and tricks

- When the model breaks down completely, that is a sign that likely the model specification is wrong. For example, in above code I first did not understand how to model the `sigma` parameter correctly, which led to a model where each chain was completely stuck at its initial values.
- In that case and in general if you are not sure whether the `brms` model specification is correct, you can check the Stan code that is generated by `brms`:

  ```{r}
  #| label: check_stan_code

  # Good to check the stan code:
  # (This also helps to find the names of the parameters
  # for which to define the initial values below)
  stancode(formula, prior = priors, data = df, family = gaussian())
  ```

  Here e.g. it is important to see that the `sigma` parameter is modelled on the log scale.
- It is important to take divergence warnings seriously. For the model parameters where `Rhat` is larger than 1.05 or so, you can just have a look at the traceplot. For example, in an earlier model fit the $\log(\mu_{\phi})$ population parameter had this traceplot:

  ```{r}
  #| label: traceplot_example

  fit_save <- fit
  load(here("session-tgi/fit.RData"))
  plot(fit, pars = "b_tphi")
  fit <- fit_save
  ```

  We see that two chains led to a $\log(\mu_{\phi})$ value below 0, while two chains hovered around much larger values between 5 and 15, which corresponds to $\mu_{\phi} = 1$. This shows that the model is rather overparametrized. By restricting the range of the prior for $\log(\mu_{\phi})$ to be below 0, we could avoid this problem.
- Providing manually realistic initial values for the model parameters can help to speed up the convergence of the Markov chains. This is especially important for the nonlinear parameters. You can get the names of the parameters from the Stan code above - note that you cannot use the names of the `brms` R code. 
- Sometimes it is not easy to get the dimensions of the inital values right. Here it can be helpful to change the backend to `rstan`, which provides more detailed error messages. You can do this by setting `backend = "rstan"` in the `brm` call.
- The priors for this model are not really subjective Bayesian priors. Here, it took me a couple of tries, where I started with one prior, ran the model, saw where the parameter samples were, and then adjusted the priors to be in a similar range. Ideally, we could avoid this by either knowing more about the parameters e.g. from previous clinical trial analyses in the same molecule and therapeutic area, or by having some simple ad-hoc calculations for each subject which can then translate to the population level (in a similar way as we did for the baseline SLD).