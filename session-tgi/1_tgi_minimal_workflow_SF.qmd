---
title: "1. TGI model minimal workflow (SF)"
author:
  - Daniel Sabanés Bové
  - Francois Mercier
date: last-modified
editor_options: 
  chunk_output_type: inline
format:
  html:
    code-fold: show
math: true
---

## Note

The purpose of this document is to show a minimal workflow for fitting a TGI model using the `brms` package.

## Setup and load data

{{< include _setup_and_load.qmd >}}

{{< include _load_data.qmd >}}

For simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:

```{r}
#| label: subset_oak

df <- tumor_data |> 
  filter(study == "4") |> 
  na.omit() |>
  droplevels() |> 
  mutate(id = factor(as.numeric(id)))
```

Here we have `r nlevels(df$id)` patients. It is always a good idea to make a plot of the data. Let's look at the first 20 patients e.g.:

```{r}
#| label: plot_data

df |> 
  filter(as.integer(id) <= 20) |>
  ggplot(aes(x = year, y = sld, group = id)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ id) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme(legend.position = "none")
```

## Stein-Fojo model

We start from the Stein-Fojo model as shown in the slides:

$$
y^{*}(t_{ij}) = \psi_{b_{0}i} \{\exp(- \psi_{k_{s}i} \cdot t_{ij}) + \exp(\psi_{k_{g}i} \cdot t_{ij}) - 1\}
$$

### Mean

We will make one more tweak here. If the time $t$ is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. Therefore, the final model for the mean SLD is:

$$
y^{*}(t_{ij}) = 
\begin{cases} 
\psi_{b_{0}i} \exp(\psi_{k_{g}i} \cdot t_{ij}) & \text{if } t_{ij} < 0 \\
\psi_{b_{0}i} \{\exp(-\psi_{k_{s}i} \cdot t_{ij}) + \exp(\psi_{k_{g}i} \cdot t_{ij}) - 1\} & \text{if } t_{ij} \geq 0 
\end{cases}
$$

### Likelihood

For the likelihood given the mean SLD $y^{*}$, we will assume a normal distribution with a constant coefficient of variation $\tau$:

$$
y(t_{ij}) \sim \text{N}(y^{*}(t_{ij}), y^{*}(t_{ij})\tau)
$$

Note that for consistency with the `brms` and `Stan` convention, here we denote the standard deviation as the second parameter of the normal distribution. So in this case, the variance would be $(y^{*}(t_{ij})\tau)^2$.

This can also be written as:

$$
y(t_{ij}) = (1 + \epsilon_{ij}) \cdot y^{*}(t_{ij})
$$

where $\epsilon_{ij} \sim \text{N}(0, \tau)$.

Note that also the additive model is a possible choice, where

$$
y(t_{ij}) = y^{*}(t_{ij}) + \epsilon_{ij}
$$

such that the error does not depend on the scale of the SLD any longer.

### Random effects

Next, we define the distributions of the random effects $\psi_{b_{0}i}$, $\psi_{k_{s}i}$, $\psi_{k_{g}i}$, for $i = 1, \dotsc, n$:

$$
\begin{align*}
\psi_{b_{0}i} &\sim \text{LogNormal}(\mu_{b_{0}}, \omega_{0}) \\
\psi_{k_{s}i} &\sim \text{LogNormal}(\mu_{k_{s}}, \omega_{s}) \\
\psi_{k_{g}i} &\sim \text{LogNormal}(\mu_{k_{g}}, \omega_{g})
\end{align*}
$$

This can be rewritten as:

$$
\begin{align*}
\psi_{b_{0}i} &= \exp(\mu_{b_{0}} + \omega_{0} \cdot \eta_{b_{0}i}) \\
\psi_{k_{s}i} &= \exp(\mu_{k_{s}} + \omega_{s} \cdot \eta_{k_{s}i}) \\
\psi_{k_{g}i} &= \exp(\mu_{k_{g}} + \omega_{g} \cdot \eta_{k_{g}i})
\end{align*}
$$

where $\eta_{b_{0}i}$, $\eta_{k_{s}i}$, $\eta_{k_{g}i}$ are the standard normal distributed random effects.

This is important for two reasons:

1. This parametrization can help the sampler to converge faster. See [here](https://mc-stan.org/docs/stan-users-guide/efficiency-tuning.html#non-centered-parameterization) for more information.
2. This shows a bit more explicitly that the population mean of the random effects is not equal to the $\mu$ parameter, but to $\exp(\mu + \omega^2 / 2)$, because they are log-normally distributed (see e.g. [Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution#Arithmetic_moments) for the formulas). This is important for the interpretation of the parameter estimates.

### Priors

Finally, we need to define the priors for the hyperparameters. 

There are different principles we could use to define these priors:

1. **Non-informative priors**: We could use priors that are as non-informative as possible. This is especially useful if we do not have any prior knowledge about the parameters. For example, we could use normal priors with a large standard deviation for the population means of the random effects.
2. **Informative priors**: If we have some prior knowledge about the parameters, we can use this to define the priors. For example, if we have literature data about the $k_g$ parameter estimate, we could use this to define the prior for the population mean of the growth rate. (Here we just need to be careful to consider the time scale and the log-normal distribution, as mentioned above)

Here we use relatively informative priors for the log-normal location parameters, motivated by prior analyses of the same study:

$$
\begin{align*}
\mu_{b_{0}} &\sim \text{LogNormal}(\log(65), 1) \\
\mu_{k_{s}} &\sim \text{LogNormal}(\log(0.52), 0.1) \\
\mu_{k_{g}} &\sim \text{LogNormal}(\log(1.04), 1)
\end{align*}
$$

For all standard deviations we use truncated normal priors:

$$
\begin{align*}
\omega_{0} &\sim \text{PositiveNormal}(0, 3) \\
\omega_{s} &\sim \text{PositiveNormal}(0, 3) \\
\omega_{g} &\sim \text{PositiveNormal}(0, 3) \\
\tau &\sim \text{PositiveNormal}(0, 3)
\end{align*}
$$

where $\text{PositiveNormal}(0, 3)$ denotes a truncated normal distribution with mean $0$ and standard deviation $3$, truncated to the positive real numbers.

## Fit model

We can now fit the model using `brms`. The structure is determined by the model formula:

```{r}
#| label: fit_brms
formula <- bf(sld ~ eta, nl = TRUE) +
  # Define the mean for the likelihood
  nlf(
    eta ~ 
      int_step(year > 0) * 
        (b0 * (exp(-ks * year) + exp(kg * year) -  1)) +
      int_step(year <= 0) * 
        (b0 * exp(kg * year))
  ) +
  # Define the standard deviation (called sigma in brms) as a 
  # coefficient tau times the mean.
  # sigma is modelled on the log scale though, therefore:
  nlf(sigma ~ log(tau) + log(eta)) +
  lf(tau ~ 1) +
  # Define nonlinear parameter transformations
  nlf(b0 ~ exp(lb0)) +
  nlf(ks ~ exp(lks)) +
  nlf(kg ~ exp(lkg)) +
  # Define random effect structure
  lf(lb0 ~ 1 + (1 | id)) + 
  lf(lks ~ 1 + (1 | id)) +
  lf(lkg ~ 1 + (1 | id))

# Define the priors
priors <- c(
  prior(normal(log(65), 1), nlpar = "lb0"),
  prior(normal(log(0.52), 0.1), nlpar = "lks"),
  prior(normal(log(1.04), 1), nlpar = "lkg"),
  prior(normal(0, 3), lb = 0, nlpar = "lb0", class = "sd"),
  prior(normal(0, 3), lb = 0, nlpar = "lks", class = "sd"),
  prior(normal(0, 3), lb = 0, nlpar = "lkg", class = "sd"),
  prior(normal(0, 3), lb = 0, nlpar = "tau")
)

# Initial values to avoid problems at the beginning
n_patients <- nlevels(df$id)
inits <- list(
  b_lb0 = array(3.61),
  b_lks = array(-1.25),
  b_lkg = array(-1.33),
  sd_1 = array(0.58),
  sd_2 = array(1.6),
  sd_3 = array(0.994),
  b_tau = array(0.161),
  z_1 = matrix(0, nrow = 1, ncol = n_patients),
  z_2 = matrix(0, nrow = 1, ncol = n_patients),
  z_3 = matrix(0, nrow = 1, ncol = n_patients)
)

# Fit the model
save_file <- here("session-tgi/fit9.RData")
if (file.exists(save_file)) {
  load(save_file)
} else {
  fit <- brm(
    formula = formula,
    data = df,
    prior = priors,
    family = gaussian(),
    init = rep(list(inits), CHAINS),
    chains = CHAINS, 
    iter = ITER + WARMUP, 
    warmup = WARMUP, 
    seed = BAYES.SEED,
    refresh = REFRESH
  )
  save(fit, file = save_file)
}

# Summarize the fit
summary(fit)
```

Note that it is crucial to use here `int_step()` to properly define the two pieces of the linear predictor for negative and non-negative time values: If you used `step()` like I did for a few days, then you will have the wrong model! This is because in Stan, `step(false) = step(0) = 1` and not 0 as you would expect. Only `int_step(0) = 0` as we need it here. 

## Parameter estimates

Here we extract all parameter estimates using the `as_draws_df` method for `brmsfit` objects. Note that we could also just extract a subset of parameters, see `?as_draws.brmsfit` for more information.

As mentioned above, we use the expectation of the log-normal distribution to get the population level estimates for the $b_0$, $k_s$, and $k_g$ parameters. We also calculate the coefficient of variation for each parameter.

```{r}
#| label: extract_parameters
post_df <- as_draws_df(fit)
head(names(post_df), 10)

post_df <- post_df |>
  mutate(
    b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),
    ks = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),
    kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),
    omega_0 = sd_id__lb0_Intercept,
    omega_s = sd_id__lks_Intercept,
    omega_g = sd_id__lkg_Intercept,
    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),
    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),
    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),
    sigma = b_tau_Intercept
  )
```

For a graphical check of the convergence, we can use the `mcmc_trace` and `mcmc_pairs` functions from the `bayesplot` package:

```{r}
#| label: plot_convergence

mcmc_trace(post_df, pars = c("b0", "ks", "kg", "omega_0", "omega_s", "omega_g", "sigma"))
mcmc_pairs(post_df, pars = c("b0", "ks", "kg", "omega_0", "omega_s", "omega_g", "sigma"))
```

Next, we can create a nice summary table of the parameter estimates, using `summarize_draws` from the `posterior` package in combination with functions from the `gt` package:

```{r}
#| label: summarize_parameters

post_sum <- post_df |>
  select(b0, ks, kg, omega_0, omega_s, omega_g, cv_0, cv_s, cv_g, sigma) |>
  summarize_draws() |>
  gt() |>
  fmt_number(n_sigfig = 3)
post_sum
```

We can also look at parameters for individual patients. This is simplified by using the `spread_draws()` function:

```{r}
#| label: extract_individual_parameters

# Understand the names of the random effects:
head(get_variables(fit), 10)

post_ind <- fit |> 
  spread_draws(
    b_lb0_Intercept,
    r_id__lb0[id, ],
    b_lks_Intercept,
    r_id__lks[id, ],
    b_lkg_Intercept,
    r_id__lkg[id, ]
  ) |> 
  mutate(
    b0 = exp(b_lb0_Intercept + r_id__lb0),
    ks = exp(b_lks_Intercept + r_id__lks),
    kg = exp(b_lkg_Intercept + r_id__lkg)
  ) |> 
  select(
    .chain, .iteration, .draw,
    id, b0, ks, kg    
  )
```

With this we can e.g. report the estimates for the first patient:

```{r}
#| label: first_patient_estimates

post_sum_id1 <- post_ind |>
  filter(id == "1") |>
  summarize_draws() |>
  gt() |>
  fmt_number(n_sigfig = 3)
post_sum_id1
```

## Observation vs model fit

We can now compare the model fit to the observations. Let's do this for the first 20 patients again.

```{r}
#| label: obs_vs_fit_plots

pt_subset <- as.character(1:20)
df_subset <- df |> 
  filter(id %in% pt_subset)

df_sim <- df_subset |> 
  data_grid(
    id = pt_subset, 
    year = seq_range(year, 101)
  ) |>
  add_linpred_draws(fit) |> 
  median_qi() |> 
  mutate(sld_pred = .linpred)

df_sim |>
  ggplot(aes(x = year, y = sld)) +
  facet_wrap(~ id) +
  geom_ribbon(
    aes(y = sld_pred, ymin = .lower, ymax = .upper),
    alpha = 0.3, 
    fill = "deepskyblue"
  ) +
  geom_line(aes(y = sld_pred), color = "deepskyblue") +
  geom_point(data = df_subset, color = "tomato") +
  coord_cartesian(ylim = range(df_subset$sld)) +
  scale_fill_brewer(palette = "Greys") +
  labs(title = "SF model fit")
```

## Tips and tricks

- When the model breaks down completely, that is a sign that likely the model specification is wrong. For example, in above code I first did not understand how to model the `sigma` parameter correctly, which led to a model where each chain was completely stuck at its initial values.
- In that case and in general if you are not sure whether the `brms` model specification is correct, you can check the Stan code that is generated by `brms`:

  ```{r}
  #| label: check_stan_code

  # Good to check the stan code:
  # (This also helps to find the names of the parameters
  # for which to define the initial values below)
  stancode(formula, prior = priors, data = df, family = gaussian())
  ```

  Here e.g. it is important to see that the `sigma` parameter is modelled on the log scale.
- We can also extract the actual data that is passed to the Stan program:
  
  ```{r}
  #| label: extract_data

  # Extract the data that is passed to the Stan program
  stan_data <- standata(formula, prior = priors, data = df, family = gaussian())

  # Check that the time variable is correct
  all(stan_data$C_eta_1 == df$year)
  ```

  This can be useful to check if the data is passed correctly to the Stan program.
- It is important to take divergence warnings seriously. For the model parameters where `Rhat` is larger than 1.05 or so, you can just have a look at the traceplot. For example, in an earlier model fit the $\log(\mu_{\phi})$ population parameter had this traceplot:

  ```{r}
  #| label: traceplot_example

  fit_save <- fit
  load(here("session-tgi/fit.RData"))
  plot(fit, pars = "b_tphi")
  fit <- fit_save
  ```

  We see that two chains led to a $\log(\mu_{\phi})$ value below 0, while two chains hovered around much larger values between 5 and 15, which corresponds to $\mu_{\phi} = 1$. This shows that the model is rather overparametrized. By restricting the range of the prior for $\log(\mu_{\phi})$ to be below 0, we could avoid this problem.
- Providing manually realistic initial values for the model parameters can help to speed up the convergence of the Markov chains. This is especially important for the nonlinear parameters. You can get the names of the parameters from the Stan code above - note that you cannot use the names of the `brms` R code. 
- Sometimes it is not easy to get the dimensions of the inital values right. Here it can be helpful to change the backend to `rstan`, which provides more detailed error messages. You can do this by setting `backend = "rstan"` in the `brm` call.
- The priors for this model are not really subjective Bayesian priors. Here, it took me a couple of tries, where I started with one prior, ran the model, saw where the parameter samples were, and then adjusted the priors to be in a similar range. Ideally, we could avoid this by either knowing more about the parameters e.g. from previous clinical trial analyses in the same molecule and therapeutic area, or by having some simple ad-hoc calculations for each subject which can then translate to the population level (in a similar way as we did for the baseline SLD).
