[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Contents",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n2025-04-25\n\n\n0. Setup and data preparation\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n2025-04-25\n\n\n1. TGI-OS joint model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#session-3-tgi-os",
    "href": "index.html#session-3-tgi-os",
    "title": "Contents",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n2025-04-25\n\n\n0. Setup and data preparation\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n2025-04-25\n\n\n1. TGI-OS joint model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#session-2-os",
    "href": "index.html#session-2-os",
    "title": "Contents",
    "section": "Session 2: OS",
    "text": "Session 2: OS\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n2025-04-25\n\n\n0. Setup and data preparation\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n2025-04-25\n\n\n1. OS model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-04-25\n\n\n2. OS model minimal workflow with brms\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#session-1-tgi",
    "href": "index.html#session-1-tgi",
    "title": "Contents",
    "section": "Session 1: TGI",
    "text": "Session 1: TGI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n2025-04-25\n\n\n0. Setup\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n2025-04-25\n\n\n1. TGI model minimal workflow with brms\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-04-25\n\n\n2. TGI model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-04-25\n\n\n3. Generalized Stein-Fojo model\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-04-25\n\n\n4. Claret-Bruno model\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Contents",
    "section": "License",
    "text": "License\nThe training material is licensed under a Creative Commons Attribution 4.0 International License. If you wish to reuse any part of this material, please ensure proper attribution is given to the original authors as specified by the Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Contents",
    "section": "Copyright",
    "text": "Copyright\n© 2025 Genentech Inc. All rights reserved.",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html",
    "href": "session-jm/1_jm-jmpost.html",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "",
    "text": "The purpose of this document is to show a minimal workflow for fitting a joint Stein-Fojo TGI + Weibull OS model using the jmpost package.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#setup-and-load-data",
    "href": "session-jm/1_jm-jmpost.html#setup-and-load-data",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Setup and load data",
    "text": "Setup and load data\nHere we execute the R code from the setup and data preparation chapter, see the full code here.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#data-object-preparation",
    "href": "session-jm/1_jm-jmpost.html#data-object-preparation",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Data object preparation",
    "text": "Data object preparation\nFirst we again prepare the data objects, starting with the subject level data:\n\n\nShow the code\nsubj_df &lt;- os_data |&gt;\n    mutate(study = \"OAK\") |&gt;\n    select(study, id, arm)\n\nsubj_data &lt;- DataSubject(\n    data = subj_df,\n    subject = \"id\",\n    arm = \"arm\",\n    study = \"study\"\n)\n\n\nNext we prepare the longitudinal data object.\n\n\nShow the code\nlong_df &lt;- tumor_data |&gt;\n    select(id, year, sld)\nlong_data &lt;- DataLongitudinal(\n    data = long_df,\n    formula = sld ~ year\n)\n\n\nNext we prepare the DataSurvival object, where specify the covariates for the survival model:\n\n\nShow the code\nsurv_data &lt;- DataSurvival(\n    data = os_data,\n    formula = Surv(os_time, os_event) ~ ecog + age + race + sex\n)\n\n\nNote that we don’t include the log growth rate estimate log_kg_est here, because here we are fitting a joint model - the log growth rate will be included later in the JointModel specification instead. For now we also don’t include the treatment arm covariate, because we assume Working Assumption (1) from the previous session, i.e. the treatment effect is fully captured by the mediator (or link).\nNow we can create the JointData object for the TGI model:\n\n\nShow the code\njoint_data &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data,\n    survival = surv_data\n)",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#model-specification",
    "href": "session-jm/1_jm-jmpost.html#model-specification",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Model specification",
    "text": "Model specification\nWe specify the Stein-Fojo model for the TGI data, the Weibull model for the OS data, as well as the link, i.e. the log growth rate from the TGI model which shall influence the hazard in the OS model:\n\n\nShow the code\njoint_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    ),\n    survival = SurvivalWeibullPH(\n        lambda = prior_gamma(0.7, 1),\n        gamma = prior_gamma(1.5, 1),\n        beta = prior_normal(0, 20)\n    ),\n    link = linkGrowth(\n        prior = prior_normal(0, 20)\n    )\n)\n\n\nHere we use a normal prior with mean 0 and standard deviation 20 for the link coefficient, which is a very uninformative prior. This corresponds to the same prior used for the regression coefficients of the “fixed” covariates in SurvivalWeibullPH.\nFor the other parameters, we keep the same prior as in the previous sessions on separate TGI and OS models.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#model-fitting",
    "href": "session-jm/1_jm-jmpost.html#model-fitting",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Model fitting",
    "text": "Model fitting\nAgain we need to be careful with the automatic selection of initial values due to the large standard deviation on beta and the link coefficient. We therefore set the shrinkage option and check the initial values:\n\n\nShow the code\noptions(\"jmpost.prior_shrinkage\" = 0.99)\n\ninitialValues(joint_mod, n_chains = CHAINS)\n\n\n[[1]]\n[[1]]$lm_sf_mu_bsld\n[1] 4.181024\n\n[[1]]$lm_sf_mu_ks\n[1] -0.6407648\n\n[[1]]$lm_sf_mu_kg\n[1] 0.04008191\n\n[[1]]$lm_sf_omega_bsld\n[1] 0.04687451\n\n[[1]]$lm_sf_omega_ks\n[1] 0.02033251\n\n[[1]]$lm_sf_omega_kg\n[1] 0.02409268\n\n[[1]]$lm_sf_sigma\n[1] 0.06927281\n\n[[1]]$lm_sf_eta_tilde_bsld\n[1] -0.001737237\n\n[[1]]$lm_sf_eta_tilde_ks\n[1] 0.009434345\n\n[[1]]$lm_sf_eta_tilde_kg\n[1] 0.005402105\n\n[[1]]$sm_weibull_ph_lambda\n[1] 0.6930353\n\n[[1]]$sm_weibull_ph_gamma\n[1] 1.504945\n\n[[1]]$beta_os_cov\n[1] -0.2001199\n\n[[1]]$link_growth\n[1] 0.1170246\n\n\n[[2]]\n[[2]]$lm_sf_mu_bsld\n[1] 4.185331\n\n[[2]]$lm_sf_mu_ks\n[1] -0.6506003\n\n[[2]]$lm_sf_mu_kg\n[1] 0.04547341\n\n[[2]]$lm_sf_omega_bsld\n[1] 0.03082002\n\n[[2]]$lm_sf_omega_ks\n[1] 0.001811384\n\n[[2]]$lm_sf_omega_kg\n[1] 0.002092434\n\n[[2]]$lm_sf_sigma\n[1] 0.09111168\n\n[[2]]$lm_sf_eta_tilde_bsld\n[1] 0.006385964\n\n[[2]]$lm_sf_eta_tilde_ks\n[1] -0.02005912\n\n[[2]]$lm_sf_eta_tilde_kg\n[1] -0.001744672\n\n[[2]]$sm_weibull_ph_lambda\n[1] 0.7081186\n\n[[2]]$sm_weibull_ph_gamma\n[1] 1.494152\n\n[[2]]$beta_os_cov\n[1] 0.3567111\n\n[[2]]$link_growth\n[1] -0.3202098\n\n\n[[3]]\n[[3]]$lm_sf_mu_bsld\n[1] 4.172195\n\n[[3]]$lm_sf_mu_ks\n[1] -0.6525469\n\n[[3]]$lm_sf_mu_kg\n[1] 0.03253895\n\n[[3]]$lm_sf_omega_bsld\n[1] 0.0102947\n\n[[3]]$lm_sf_omega_ks\n[1] 0.004766669\n\n[[3]]$lm_sf_omega_kg\n[1] 0.004327734\n\n[[3]]$lm_sf_sigma\n[1] 0.01409515\n\n[[3]]$lm_sf_eta_tilde_bsld\n[1] -0.0003005472\n\n[[3]]$lm_sf_eta_tilde_ks\n[1] 0.008226709\n\n[[3]]$lm_sf_eta_tilde_kg\n[1] -0.01529545\n\n[[3]]$sm_weibull_ph_lambda\n[1] 0.6942424\n\n[[3]]$sm_weibull_ph_gamma\n[1] 1.490525\n\n[[3]]$beta_os_cov\n[1] 0.3023342\n\n[[3]]$link_growth\n[1] -0.08733611\n\n\n[[4]]\n[[4]]$lm_sf_mu_bsld\n[1] 4.159327\n\n[[4]]$lm_sf_mu_ks\n[1] -0.6535959\n\n[[4]]$lm_sf_mu_kg\n[1] 0.03220799\n\n[[4]]$lm_sf_omega_bsld\n[1] 0.04461592\n\n[[4]]$lm_sf_omega_ks\n[1] 0.01606186\n\n[[4]]$lm_sf_omega_kg\n[1] 0.0399481\n\n[[4]]$lm_sf_sigma\n[1] 0.03011004\n\n[[4]]$lm_sf_eta_tilde_bsld\n[1] 0.000442411\n\n[[4]]$lm_sf_eta_tilde_ks\n[1] -0.004481701\n\n[[4]]$lm_sf_eta_tilde_kg\n[1] 0.003641929\n\n[[4]]$sm_weibull_ph_lambda\n[1] 0.6998378\n\n[[4]]$sm_weibull_ph_gamma\n[1] 1.491266\n\n[[4]]$beta_os_cov\n[1] -0.04021382\n\n[[4]]$link_growth\n[1] 0.1508573\n\n\nIf we don’t do this, then it is very likely that some chains will diverge because of very unrealistic initial values for beta and/or the link coefficient.\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-jm/jm1.rds\")\nif (file.exists(save_file)) {\n    joint_results &lt;- readRDS(save_file)\n} else {\n    joint_results &lt;- sampleStanModel(\n        joint_mod,\n        data = joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(joint_results, file = save_file)\n}\n\n\nAs mentioned before, also here we can get warnings at the beginning of the chains’ sampling process (“The current Metropolis proposal is about to be rejected …”). As long as this only happens in the beginning, and not during the sampling later, then this is not a cause for concern.\nWe note that the MCMC sampling process takes much longer here (about factor 10 more) compared to just fitting the TGI or the OS data separately. This is due to the more complex likelihood function calculations for the joint TGI-OS model.\nLet’s check the convergence of the population parameters. If we don’t remember their names, we can query them as follows:\n\n\nShow the code\njoint_results\n\n\n\n   JointModelSamples Object with:\n  \n      # of samples per chain = 1000\n      # of chains            = 4\n  \n      Variables:\n          beta_os_cov[5]\n          link_coefficients\n          link_function_inputs[203, 3]\n          link_growth\n          lm_sf_eta_tilde_bsld[203]\n          lm_sf_eta_tilde_kg[203]\n          lm_sf_eta_tilde_ks[203]\n          lm_sf_mu_bsld\n          lm_sf_mu_kg[2]\n          lm_sf_mu_ks[2]\n          lm_sf_omega_bsld\n          lm_sf_omega_kg[2]\n          lm_sf_omega_ks[2]\n          lm_sf_psi_bsld[203]\n          lm_sf_psi_kg[203]\n          lm_sf_psi_ks[203]\n          lm_sf_sigma\n          log_surv_fit_at_obs_times[203]\n          long_obvs_log_lik[1093]\n          lp__\n          os_cov_contribution[203]\n          os_subj_log_lik[203]\n          pars_os[2]\n          sm_weibull_ph_gamma\n          sm_weibull_ph_lambda\n          Ypred[1093] \n\n\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\",\n    \"beta_os_cov\",\n    \"sm_weibull_ph_gamma\",\n    \"sm_weibull_ph_lambda\",\n    \"link_growth\"\n)\n\nmcmc_joint_results &lt;- cmdstanr::as.CmdStanMCMC(joint_results)\nmcmc_joint_results$summary(vars)\n\n\n# A tibble: 19 × 10\n   variable         mean   median      sd     mad      q5     q95  rhat ess_bulk\n   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu_bs…  3.76     3.76    0.0371  0.0355   3.69    3.82   1.02      192.\n 2 lm_sf_mu_ks… -0.0188   0.00943 0.235   0.228   -0.445   0.339  1.00      462.\n 3 lm_sf_mu_ks… -1.06    -1.03    0.330   0.314   -1.66   -0.591  1.01      559.\n 4 lm_sf_mu_kg… -0.585   -0.576   0.139   0.146   -0.837  -0.375  0.999     500.\n 5 lm_sf_mu_kg… -0.892   -0.890   0.139   0.138   -1.14   -0.656  1.00      552.\n 6 lm_sf_sigma   0.129    0.129   0.00371 0.00366  0.123   0.135  1.00      840.\n 7 lm_sf_omega…  0.530    0.529   0.0265  0.0256   0.488   0.576  1.01      340.\n 8 lm_sf_omega…  0.936    0.913   0.178   0.169    0.680   1.27   1.01      409.\n 9 lm_sf_omega…  1.36     1.33    0.252   0.247    0.989   1.81   1.00      566.\n10 lm_sf_omega…  0.684    0.677   0.0812  0.0744   0.561   0.831  1.00      781.\n11 lm_sf_omega…  0.956    0.952   0.0953  0.0973   0.810   1.12   1.01      585.\n12 beta_os_cov…  0.842    0.829   0.234   0.229    0.464   1.24   1.00      812.\n13 beta_os_cov…  0.00316  0.00296 0.00981 0.00996 -0.0128  0.0197 0.998     975.\n14 beta_os_cov…  0.658    0.656   0.431   0.428   -0.0678  1.36   1.00      901.\n15 beta_os_cov… -0.00443 -0.00251 0.237   0.247   -0.371   0.371  1.00      760.\n16 beta_os_cov…  0.313    0.305   0.215   0.217   -0.0363  0.677  1.00      735.\n17 sm_weibull_…  1.81     1.81    0.167   0.170    1.55    2.09   1.00      794.\n18 sm_weibull_…  0.261    0.213   0.176   0.132    0.0733  0.635  0.998     805.\n19 link_growth   0.880    0.861   0.235   0.242    0.525   1.29   1.00      571.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nShow the code\ndraws_joint_results &lt;- mcmc_joint_results$draws(vars)\nmcmc_trace(draws_joint_results)\n\n\n\n\n\n\n\n\n\nSo this looks good.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#interpret-covariate-effects",
    "href": "session-jm/1_jm-jmpost.html#interpret-covariate-effects",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Interpret covariate effects",
    "text": "Interpret covariate effects\nIn order to better see which of the survival model coefficients relate to which covariates, we can again rename them as follows:\n\n\nShow the code\nos_cov_name_mapping &lt;- function(surv_data) {\n    surv_data_design &lt;- as_stan_list(surv_data)$os_cov_design\n    os_cov_names &lt;- colnames(surv_data_design)\n    old_coef_names &lt;- as.character(glue::glue(\"beta_os_cov[{seq_along(os_cov_names)}]\"))\n    setNames(old_coef_names, os_cov_names)\n}\nos_cov_renaming &lt;- os_cov_name_mapping(surv_data)\ndraws_joint_results &lt;- do.call(\n    rename_variables,\n    c(list(draws_joint_results), os_cov_renaming)\n)\nmcmc_dens_overlay(draws_joint_results) +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nIf we compare this with the covariate effect estimates from the 2-stage OS model we did in the last session, then we can see:\n\nthe log_kg_est effect (see link_growth here) is stronger here, but also with larger uncertainty\nthe ecog effect is similar (clearly higher risk with ECOG 1)\nthe age effect is similar (no effect)\nthe race effect is similar (almost no effect)\nthe sex effect is similar (higher risk for males)\n\nIt is also interesting to look at the shrinkage and growth rate estimates from the SF model part: We see e.g. 0 shrinkage in arm 1, which is the control arm, while we see a strong shrinkage in arm 2, which is the Atezo arm.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#sld-vs-longitudinal-model-fit",
    "href": "session-jm/1_jm-jmpost.html#sld-vs-longitudinal-model-fit",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "SLD vs longitudinal model fit",
    "text": "SLD vs longitudinal model fit\nLet’s first check the fit of the Stein-Fojo model to the SLD data.\nThe first step is to generate the predictions at the subject level. We can do this using the LongitudinalQuantities() function, which takes the MCMC results and the grid at which the predictions should be made. Here we use the GridObserved() function, which takes the IDs of the subjects for which the predictions should be made. Since each patient has its own plot, we sample a small subset of patient IDs here as an example only. In a real application we could write a simple loop that then processes batches of patients in sequence.\n\n\nShow the code\nset.seed(521)\npt_subset &lt;- as.character(sample(subj_df$id, 20))\n\ntgi_fit_pred &lt;- LongitudinalQuantities(\n    joint_results,\n    grid = GridObserved(subjects = pt_subset)\n)\n\n\nNote that here again a Stan program needs to be compiled, which can take some time (but only the first time, because the executable is cached). This is because we pass the posterior samples to a Stan program which then generates the quantities of interest, here the Stein-Fojo model fit for each patient at the observed time points.\nNow we can plot the predictions:\n\n\nShow the code\nautoplot(tgi_fit_pred) +\n    labs(x = \"Time (years)\", y = \"SLD (mm)\")\n\n\n\n\n\n\n\n\n\nWe can see that the model fits the data well, with the estimates Stein-Fojo model curves closely following the observed values.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#kaplan-meier-vs-survival-model-fit",
    "href": "session-jm/1_jm-jmpost.html#kaplan-meier-vs-survival-model-fit",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Kaplan-Meier vs survival model fit",
    "text": "Kaplan-Meier vs survival model fit\nAnother useful plot displays the model predicted survival function and overlays the non-parametric Kaplan-Meier plot to it.\nThe first step consists in generating the survival predictions at the group level with the SurvivalQuantities() function. In order to do this, we use now the GridGrouped() function, which takes the time points at which the predictions should be made and the groups for which the predictions should be made (as a list containing the IDs in each element defining the group). This works the same way as in the previous session with the OS model.\n\n\nShow the code\ntime_grid &lt;- seq(from = 0, to = max(os_data$os_time), length = 100)\nos_surv_group_grid &lt;- GridGrouped(\n    times = time_grid,\n    groups = with(\n        subj_df,\n        split(as.character(id), arm)\n    )\n)\nos_surv_pred &lt;- SurvivalQuantities(\n    object = joint_results,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\n\n\nNow we can use the autoplot() method:\n\n\nShow the code\nautoplot(os_surv_pred, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nWe can see that the fit looks adequate, with the modelled survival functions closely following the Kaplan-Meier curves in each treatment group. Of note, this fit looks better than in the 2-stage TGI-OS model from the previous session.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#hazard-and-hazard-rate-estimation",
    "href": "session-jm/1_jm-jmpost.html#hazard-and-hazard-rate-estimation",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Hazard and hazard rate estimation",
    "text": "Hazard and hazard rate estimation\nSimilarly to the survival function estimation, we can also estimate the hazard function by treatment group.\n\n\nShow the code\nos_hazard_pred &lt;- SurvivalQuantities(\n    object = joint_results,\n    grid = os_surv_group_grid,\n    type = \"haz\"\n)\n\n\nAlso this can be plotted using the autoplot() method:\n\n\nShow the code\nautoplot(os_hazard_pred, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nWe can already see here that this looks slightly different than the same plot from the 2-step TGI-OS model:\n\nThe hazards are higher\nThe uncertainty is considerably larger\n\nNow let’s look at the estimated hazard ratio:\n\n\nShow the code\nos_hr_est &lt;- os_hazard_pred |&gt;\n    as.data.frame() |&gt;\n    group_by(group, time) |&gt;\n    mutate(sample = row_number()) |&gt;\n    pivot_wider(names_from = group, values_from = values) |&gt;\n    mutate(hr = MPDL3280A / Docetaxel) |&gt;\n    group_by(time) |&gt;\n    summarize(\n        mean = mean(hr, na.rm = TRUE),\n        lower = quantile(hr, 0.05, na.rm = TRUE),\n        upper = quantile(hr, 0.95, na.rm = TRUE)\n    ) |&gt;\n    na.omit() # Omit the time = 0 which has NA\nsummary(os_hr_est)\n\n\n      time              mean            lower            upper      \n Min.   :0.02276   Min.   :0.8703   Min.   :0.6966   Min.   :1.063  \n 1st Qu.:0.58038   1st Qu.:0.8703   1st Qu.:0.6966   1st Qu.:1.063  \n Median :1.13801   Median :0.8703   Median :0.6966   Median :1.063  \n Mean   :1.13801   Mean   :0.8703   Mean   :0.6966   Mean   :1.063  \n 3rd Qu.:1.69563   3rd Qu.:0.8703   3rd Qu.:0.6966   3rd Qu.:1.063  \n Max.   :2.25325   Max.   :0.8703   Max.   :0.6966   Max.   :1.063  \n\n\nAlso here the hazard ratio is indeed constant over time, which was the same in the 2-step TGI-OS model. This is because the link between the longitudinal and the survival model is here the log growth rate of the Stein-Fojo model, which is constant over time. For other link functions that are time-varying, e.g. the derivative of the longitudinal model, the hazard ratio could change over time.\nSo here the estimated hazard ratio is 0.87 with a 90% credible interval of 0.7 to 1.06. We can see that this hazard ratio estimate is slightly lower, representing a slightly stronger effect estimate. Due to the larger uncertainty, which is due to the propagation of the log growth rate uncertainty to the OS model, the 90% CI now actually includes 1, in contrast to the 2-step TGI-OS model.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#alternative-covariate-specification",
    "href": "session-jm/1_jm-jmpost.html#alternative-covariate-specification",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Alternative covariate specification",
    "text": "Alternative covariate specification\nIn the previous OS session we tried to include the treatment arm as a direct covariate. We can also try this here.\n\n\nShow the code\nsurv_data_with_arm &lt;- DataSurvival(\n    data = os_data,\n    # Here we add the arm covariate:\n    formula = update(surv_data@formula, . ~ . + arm)\n)\njoint_data_with_arm &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data,\n    survival = surv_data_with_arm\n)\n\nsave_file &lt;- here(\"session-jm/jm2.rds\")\nif (file.exists(save_file)) {\n    joint_results_with_arm &lt;- readRDS(save_file)\n} else {\n    joint_results_with_arm &lt;- sampleStanModel(\n        joint_mod,\n        data = joint_data_with_arm,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(joint_results_with_arm, file = save_file)\n}\n\nmcmc_joint_arm_results &lt;- cmdstanr::as.CmdStanMCMC(joint_results_with_arm)\nmcmc_joint_arm_results$summary(vars)\n\n\n# A tibble: 20 × 10\n   variable         mean   median      sd     mad      q5     q95  rhat ess_bulk\n   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu_bs…  3.75     3.76    0.0373  0.0377   3.69    3.82   1.02      196.\n 2 lm_sf_mu_ks… -0.120   -0.0763  0.274   0.269   -0.590   0.271  1.01      455.\n 3 lm_sf_mu_ks… -1.00    -0.972   0.328   0.315   -1.59   -0.525  1.00      538.\n 4 lm_sf_mu_kg… -0.643   -0.638   0.154   0.153   -0.908  -0.402  1.01      500.\n 5 lm_sf_mu_kg… -0.868   -0.862   0.146   0.146   -1.12   -0.640  1.01      522.\n 6 lm_sf_sigma   0.129    0.129   0.00356 0.00356  0.124   0.136  1.00      944.\n 7 lm_sf_omega…  0.531    0.530   0.0279  0.0268   0.489   0.583  1.01      342.\n 8 lm_sf_omega…  0.979    0.959   0.191   0.181    0.713   1.34   1.00      481.\n 9 lm_sf_omega…  1.32     1.29    0.240   0.232    0.972   1.74   1.00      580.\n10 lm_sf_omega…  0.687    0.680   0.0871  0.0896   0.560   0.844  1.00      911.\n11 lm_sf_omega…  0.957    0.951   0.0951  0.0898   0.820   1.13   1.00      647.\n12 beta_os_cov…  0.864    0.858   0.235   0.237    0.477   1.26   1.00     1028.\n13 beta_os_cov…  0.00372  0.00329 0.00996 0.0104  -0.0125  0.0197 0.999     964.\n14 beta_os_cov…  0.690    0.694   0.445   0.447   -0.0367  1.44   1.00      930.\n15 beta_os_cov…  0.0296   0.0232  0.248   0.239   -0.371   0.439  0.999     906.\n16 beta_os_cov…  0.310    0.307   0.216   0.218   -0.0299  0.681  0.999     958.\n17 beta_os_cov… -0.207   -0.204   0.239   0.229   -0.606   0.176  0.999     767.\n18 sm_weibull_…  1.82     1.81    0.163   0.160    1.56    2.10   1.01      836.\n19 sm_weibull_…  0.272    0.228   0.193   0.148    0.0755  0.608  0.999    1000.\n20 link_growth   0.870    0.862   0.239   0.239    0.499   1.27   1.01      617.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nShow the code\ndraws_joint_arm_results &lt;- mcmc_joint_arm_results$draws(vars)\nmcmc_trace(draws_joint_arm_results)\n\n\n\n\n\n\n\n\n\nWe can easily plot the survival functions and compare them with the Kaplan-Meier curves of the treatment arms, because we can reuse the above os_surv_group_grid:\n\n\nShow the code\njoint_mod_with_arm_os_pred &lt;- SurvivalQuantities(\n    object = joint_results_with_arm,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\nautoplot(joint_mod_with_arm_os_pred, add_km = TRUE, add_wrap = FALSE)",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#alternative-time-varying-link",
    "href": "session-jm/1_jm-jmpost.html#alternative-time-varying-link",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Alternative time-varying link",
    "text": "Alternative time-varying link\nWe would like to illustrate how the hazard ratio could change over time if we used a time-varying link function. For this we will use the derivative of the Stein-Fojo model as the link function, utilizing the linkDLSD class in jmpost.\nNote that at the moment, we can unfortunately not conveniently reuse the slots from the JointModel object (joint_mod@longitudinal and joint_mod@survival) to create a new model with a different link function, because during the object creation already Stan parameter names are created, which don’t fit then anymore here. So if needed, it would be better to save the object from LongitudinalSteinFojo() first and then use it in both models, and similarly for the SurvivalWeibullPH() object. Here we just copy/paste the model specification now.\n\n\nShow the code\njoint_mod_dsld &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    ),\n    survival = SurvivalWeibullPH(\n        lambda = prior_gamma(0.7, 1),\n        gamma = prior_gamma(1.5, 1),\n        beta = prior_normal(0, 20)\n    ),\n    link = linkDSLD(\n        prior = prior_normal(0, 10) # Reduce here a bit to help with convergence ...\n    )\n)\n\n\nLet’s check the initial values again:\n\n\nShow the code\noptions(\"jmpost.prior_shrinkage\" = 0.999)\n\ninitialValues(joint_mod_dsld, n_chains = CHAINS)\n\n\n[[1]]\n[[1]]$lm_sf_mu_bsld\n[1] 4.174281\n\n[[1]]$lm_sf_mu_ks\n[1] -0.6536928\n\n[[1]]$lm_sf_mu_kg\n[1] 0.0384735\n\n[[1]]$lm_sf_omega_bsld\n[1] 0.002839366\n\n[[1]]$lm_sf_omega_ks\n[1] 0.0001291699\n\n[[1]]$lm_sf_omega_kg\n[1] 0.0007976942\n\n[[1]]$lm_sf_sigma\n[1] 0.005996095\n\n[[1]]$lm_sf_eta_tilde_bsld\n[1] 0.000470039\n\n[[1]]$lm_sf_eta_tilde_ks\n[1] -0.00154869\n\n[[1]]$lm_sf_eta_tilde_kg\n[1] 0.0008745867\n\n[[1]]$sm_weibull_ph_lambda\n[1] 0.6995336\n\n[[1]]$sm_weibull_ph_gamma\n[1] 1.498641\n\n[[1]]$beta_os_cov\n[1] 0.009185111\n\n[[1]]$link_dsld\n[1] -0.002447209\n\n\n[[2]]\n[[2]]$lm_sf_mu_bsld\n[1] 4.172685\n\n[[2]]$lm_sf_mu_ks\n[1] -0.6525602\n\n[[2]]$lm_sf_mu_kg\n[1] 0.04002759\n\n[[2]]$lm_sf_omega_bsld\n[1] 0.001985338\n\n[[2]]$lm_sf_omega_ks\n[1] 0.005169107\n\n[[2]]$lm_sf_omega_kg\n[1] 0.001121513\n\n[[2]]$lm_sf_sigma\n[1] 0.004110273\n\n[[2]]$lm_sf_eta_tilde_bsld\n[1] -0.0006774594\n\n[[2]]$lm_sf_eta_tilde_ks\n[1] -0.0008570452\n\n[[2]]$lm_sf_eta_tilde_kg\n[1] -0.0005290799\n\n[[2]]$sm_weibull_ph_lambda\n[1] 0.6993776\n\n[[2]]$sm_weibull_ph_gamma\n[1] 1.499257\n\n[[2]]$beta_os_cov\n[1] 0.001056433\n\n[[2]]$link_dsld\n[1] -0.01053035\n\n\n[[3]]\n[[3]]$lm_sf_mu_bsld\n[1] 4.173984\n\n[[3]]$lm_sf_mu_ks\n[1] -0.654667\n\n[[3]]$lm_sf_mu_kg\n[1] 0.03969482\n\n[[3]]$lm_sf_omega_bsld\n[1] 0.005543763\n\n[[3]]$lm_sf_omega_ks\n[1] 0.005430364\n\n[[3]]$lm_sf_omega_kg\n[1] 0.0001870963\n\n[[3]]$lm_sf_sigma\n[1] 0.0009273888\n\n[[3]]$lm_sf_eta_tilde_bsld\n[1] -0.0005258746\n\n[[3]]$lm_sf_eta_tilde_ks\n[1] 0.001356768\n\n[[3]]$lm_sf_eta_tilde_kg\n[1] 0.0001322014\n\n[[3]]$sm_weibull_ph_lambda\n[1] 0.7011391\n\n[[3]]$sm_weibull_ph_gamma\n[1] 1.499553\n\n[[3]]$beta_os_cov\n[1] -0.04009133\n\n[[3]]$link_dsld\n[1] 0.02460469\n\n\n[[4]]\n[[4]]$lm_sf_mu_bsld\n[1] 4.173544\n\n[[4]]$lm_sf_mu_ks\n[1] -0.6541084\n\n[[4]]$lm_sf_mu_kg\n[1] 0.03994846\n\n[[4]]$lm_sf_omega_bsld\n[1] 0.001749874\n\n[[4]]$lm_sf_omega_ks\n[1] 0.0004688517\n\n[[4]]$lm_sf_omega_kg\n[1] 0.002471432\n\n[[4]]$lm_sf_sigma\n[1] 0.0006564231\n\n[[4]]$lm_sf_eta_tilde_bsld\n[1] -0.00169486\n\n[[4]]$lm_sf_eta_tilde_ks\n[1] -5.820686e-05\n\n[[4]]$lm_sf_eta_tilde_kg\n[1] -0.0002779012\n\n[[4]]$sm_weibull_ph_lambda\n[1] 0.7002806\n\n[[4]]$sm_weibull_ph_gamma\n[1] 1.499579\n\n[[4]]$beta_os_cov\n[1] 0.02254746\n\n[[4]]$link_dsld\n[1] 0.00770938\n\n\nNow we fit this model:\n\n\nShow the code\nsave_file &lt;- here(\"session-jm/jm3.rds\")\nif (file.exists(save_file)) {\n    joint_results_dsld &lt;- readRDS(save_file)\n} else {\n    joint_results_dsld &lt;- sampleStanModel(\n        joint_mod_dsld,\n        data = joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(joint_results_dsld, file = save_file)\n}\n\n\nLet’s check the convergence of the population parameters:\n\n\nShow the code\nvars_dsld &lt;- c(\"link_dsld\", setdiff(vars, \"link_growth\"))\nmcmc_joint_results_dsld &lt;- cmdstanr::as.CmdStanMCMC(joint_results_dsld)\nmcmc_joint_results_dsld$summary(vars_dsld)\n\n\n# A tibble: 19 × 10\n   variable        mean   median      sd     mad       q5     q95  rhat ess_bulk\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 link_dsld    0.00695  0.00669 0.00240 0.00212  0.00350  0.0111 1.00      932.\n 2 lm_sf_mu_b…  3.76     3.76    0.0379  0.0360   3.69     3.82   1.02      329.\n 3 lm_sf_mu_k… -0.0327  -0.00146 0.244   0.234   -0.488    0.319  1.00      638.\n 4 lm_sf_mu_k… -1.31    -1.28    0.361   0.349   -1.90    -0.744  1.00      648.\n 5 lm_sf_mu_k… -0.591   -0.581   0.142   0.138   -0.834   -0.385  1.00      633.\n 6 lm_sf_mu_k… -1.01    -1.01    0.143   0.142   -1.24    -0.772  1.00      619.\n 7 lm_sf_sigma  0.130    0.130   0.00365 0.00360  0.125    0.136  1.00      731.\n 8 lm_sf_omeg…  0.529    0.529   0.0276  0.0283   0.488    0.577  1.00      568.\n 9 lm_sf_omeg…  0.951    0.932   0.176   0.164    0.702    1.28   1.00      700.\n10 lm_sf_omeg…  1.54     1.51    0.279   0.270    1.12     2.02   1.00      709.\n11 lm_sf_omeg…  0.689    0.683   0.0803  0.0748   0.570    0.825  1.01      752.\n12 lm_sf_omeg…  0.964    0.956   0.107   0.106    0.803    1.15   1.00      630.\n13 beta_os_co…  0.810    0.812   0.239   0.243    0.422    1.21   0.998    1001.\n14 beta_os_co… -0.00132 -0.00149 0.0106  0.00989 -0.0183   0.0168 1.00     1121.\n15 beta_os_co…  0.589    0.613   0.436   0.434   -0.118    1.27   1.00      889.\n16 beta_os_co…  0.0833   0.0861  0.251   0.245   -0.350    0.507  1.00     1017.\n17 beta_os_co…  0.334    0.329   0.223   0.227   -0.0253   0.699  1.00     1109.\n18 sm_weibull…  1.53     1.52    0.144   0.144    1.30     1.78   1.00     1014.\n19 sm_weibull…  0.140    0.114   0.0996  0.0760   0.0359   0.334  1.00     1039.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nSo that looks ok.\nLet’s now calculate the hazard function and hazard ratio for this model. Again we will need to wait for the compilation, because now it is a different model.\n\n\nShow the code\nos_hazard_dsld_pred &lt;- SurvivalQuantities(\n    object = joint_results_dsld,\n    grid = os_surv_group_grid,\n    type = \"haz\"\n)\n\nos_hr_est_dsld &lt;- os_hazard_dsld_pred |&gt;\n    as.data.frame() |&gt;\n    group_by(group, time) |&gt;\n    mutate(sample = row_number()) |&gt;\n    mutate(values = pmin(values, 100)) |&gt;\n    pivot_wider(names_from = group, values_from = values) |&gt;\n    mutate(hr = MPDL3280A / Docetaxel) |&gt;\n    group_by(time) |&gt;\n    summarize(\n        mean = mean(hr, na.rm = TRUE),\n        median = median(hr, na.rm = TRUE),\n        lower = quantile(hr, 0.05, na.rm = TRUE),\n        upper = quantile(hr, 0.95, na.rm = TRUE)\n    ) |&gt;\n    na.omit() # Omit the time = 0 which has NA\nsummary(os_hr_est_dsld)\n\n\n      time              mean             median           lower       \n Min.   :0.02276   Min.   :  1.100   Min.   :  1.00   Min.   : 1.000  \n 1st Qu.:0.58038   1st Qu.:  1.713   1st Qu.:  1.00   1st Qu.: 1.000  \n Median :1.13801   Median :  6.817   Median :  1.00   Median : 1.000  \n Mean   :1.13801   Mean   : 41.550   Mean   : 36.39   Mean   : 1.621  \n 3rd Qu.:1.69563   3rd Qu.: 56.551   3rd Qu.: 24.21   3rd Qu.: 1.087  \n Max.   :2.25325   Max.   :207.570   Max.   :244.91   Max.   :10.414  \n     upper        \n Min.   :  1.000  \n 1st Qu.:  1.493  \n Median : 41.445  \n Mean   :100.978  \n 3rd Qu.:180.477  \n Max.   :402.655  \n\n\nHere we truncate the hazard values at an upper bound of 100, because due to the time-varying link function, the hazard can become very large at some time points, which can lead to numerical issues.\nSo now we can see that the estimated hazard ratio is no longer constant over time. Let’s try to plot it:\n\n\nShow the code\nos_hr_est_dsld |&gt;\n    ggplot(aes(x = time, y = median)) +\n    geom_line() +\n    geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +\n    labs(\n        y = \"Hazard ratio (MPDL3280A / Docetaxel)\",\n        title = \"Time-varying hazard ratio\"\n    )\n\n\n\n\n\n\n\n\n\nAlso here we see very large hazard ratio values between 0.25 and 1 year. So in this case this model would not be very useful in practice.\nLet’s still quickly have a look at the fitted survival functions if they look reasonable:\n\n\nShow the code\nos_surv_dsld_pred &lt;- SurvivalQuantities(\n    object = joint_results_dsld,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\nautoplot(os_surv_dsld_pred, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nSo this looks reasonable. So we might need to revisit the derivative calculation again for this model to see if there is something to improve in the jmpost code.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#model-comparison",
    "href": "session-jm/1_jm-jmpost.html#model-comparison",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Model comparison",
    "text": "Model comparison\nWe can again use the Brier score to compare the three different survival models as part of the joint models. The Brier score is a measure of the mean squared difference between the predicted survival probability and the actual survival status. The lower the Brier score, the better the model.\nTo calculate it, we need to use the GridFixed input for SurvivalQuantities(). This is because the Brier score is calculated at fixed time points across all patients, and not at the observed time points of specific patients. Because we don’t specify patient IDs, the quantities are generated for all patients.\n\n\nShow the code\nos_surv_fixed &lt;- SurvivalQuantities(\n    object = joint_results,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_bs &lt;- brierScore(os_surv_fixed)\n\n\nLet’s first compare this with the second model, which includes the treatment arm as a direct covariate in the survival model:\n\n\nShow the code\nos_surv_with_arm_fixed &lt;- SurvivalQuantities(\n    object = joint_results_with_arm,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_with_arm_bs &lt;- brierScore(os_surv_with_arm_fixed)\n\n\nNow let’s compare this with the third model where we have the derivative link. We have a suspicion that the third model will perform worse, because of the hazard ratio results, but let’s have a look.\n\n\nShow the code\nos_surv_dsld_fixed &lt;- SurvivalQuantities(\n    object = joint_results_dsld,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_dsld_bs &lt;- brierScore(os_surv_dsld_fixed)\n\n\nWe can plot then all three Brier scores over time to compare them visually:\n\n\nShow the code\ndata.frame(\n    time = time_grid,\n    `1 - 2` = os_bs - os_with_arm_bs,\n    `1 - 3` = os_bs - os_dsld_bs,\n    `2 - 3` = os_with_arm_bs - os_dsld_bs,\n    check.names = FALSE\n) |&gt;\n    pivot_longer(\n        cols = c(\"1 - 2\", \"1 - 3\", \"2 - 3\"),\n        names_to = \"diff\",\n        values_to = \"brier_score_diff\"\n    ) |&gt;\n    ggplot(aes(x = time, y = brier_score_diff, color = diff, group = diff)) +\n    geom_line() +\n    labs(y = \"Brier score difference\") +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nAs expected, we see that especially for the times between 0.5 and 2 years the first and second models with the growth link performs better than the third model with the derivative link. On the other hand, there is almost no difference between the first and second model, which includes the treatment arm covariate.\nCurrently, the LOOIC does not work yet for joint models, because the log_lik is not included in the samples. There is an ongoing discussion on GitHub about this topic.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html",
    "href": "session-tgi/0_setup.html",
    "title": "0. Setup",
    "section": "",
    "text": "This is a repository with training material for Tumor Growth Inhibition (TGI) and joint TGI-OS (Overall Survival) modeling.\nHere is an overview of the required setup steps, which are described in more detail below:\n\nInstall RTools (if you are on Windows)\nInstall necessary R packages\nInstall cmdstanr (optional but highly recommended)\nClone the repository from GitHub (https://github.com/RCONIS/tgi-os-training)\nOpen the folder in RStudio or VSCode\n\n\n\nIf you are running the examples on a Windows PC, you will need to install RTools, see\nhere.\nYou need to use the version of RTools that matches your R version. You can check your R version by running R.Version()$version.string.\nYou can then afterwards check the installation of RTools with:\nif(!require(pkgbuild)) install.packages(\"pkgbuild\")\npkgbuild::has_build_tools()\n\n\n\nThe following code will install the packages that are required to run the examples in this repository.\ninstall_if_not_available &lt;- function(pkg, min_version = NULL) {\n    is_installed &lt;- suppressWarnings(suppressPackageStartupMessages(\n      require(pkg, character.only = TRUE)\n    ))\n    if (is_installed & !is.null(min_version)) {\n      version_ok &lt;- packageVersion(pkg) &gt;= min_version\n    }    \n    if (!is_installed | !version_ok) {\n        install.packages(pkg)\n    }\n  }\npackages &lt;- c(\n  \"bayesplot\", \n  \"brms\", \n  \"ggplot2\",\n  \"gt\",\n  \"here\", \n  \"janitor\",\n  \"modelr\",\n  \"posterior\",\n  \"readxl\",\n  \"rstan\",\n  \"tidybayes\", \n  \"tidyverse\",   \n  \"truncnorm\",\n  \"sn\",\n  \"fuzzyjoin\",\n  \"glue\"\n)\nsapply(packages, install_if_not_available)\nremotes::install_github(\"genentech/jmpost\")\n\n\n\nOptionally, you can use cmdstanr as the backend of brms for fitting the models.\nThere are a few advantages of using cmdstanr over the default rstan:\n\nWith cmdstanr, you can cache the compiled model, therefore you don’t need to recompile the model as long as the same formula is used, even after restarting R session (or re-opening RStudio).\ncmdstanr is more up-to-date and more actively maintained compared to rstan.\n\nA detailed installation guide for cmdstanr is available here. Here is a brief summary:\n\nInstall cmdstanr with:\n# Typically, you install cmdstanr from the R-universe as follows:\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\n\n# If the above does not work, e.g. because you in a company network or VPN, you can install cmdstanr from GitHub instead:\nremotes::install_github(\"stan-dev/cmdstanr@*release\")    \nCheck the compiler toolchain with:\ncmdstanr::check_cmdstan_toolchain()\n\nThis should not be a problem on Mac and Linux (including RStudio Cloud instances), but might be a problem on Windows.\nIf you have problems, please check the installation guide for Windows.\n\nRTools is the easiest way.\nEven if you have RTools, you might still see an error like \"Rtools44 installation found but the toolchain was not installed.\".\nIn this case, you can run cmdstanr::check_cmdstan_toolchain(fix = TRUE) and this will likely resolve the issue.\n\n\nInstall the CmdStan backend with:\ncmdstanr::install_cmdstan(cores = 2)\nTesting the cmdstanr installation\n\nIf the above installation was successful, you should now be able to run the following simple model.\n\nlibrary(cmdstanr)\nfile &lt;- file.path(\n  cmdstan_path(), \n  \"examples\", \"bernoulli\", \"bernoulli.stan\"\n)\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500\n)",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#install-rtools",
    "href": "session-tgi/0_setup.html#install-rtools",
    "title": "0. Setup",
    "section": "",
    "text": "If you are running the examples on a Windows PC, you will need to install RTools, see\nhere.\nYou need to use the version of RTools that matches your R version. You can check your R version by running R.Version()$version.string.\nYou can then afterwards check the installation of RTools with:\nif(!require(pkgbuild)) install.packages(\"pkgbuild\")\npkgbuild::has_build_tools()",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#install-necessary-r-packages",
    "href": "session-tgi/0_setup.html#install-necessary-r-packages",
    "title": "0. Setup",
    "section": "",
    "text": "The following code will install the packages that are required to run the examples in this repository.\ninstall_if_not_available &lt;- function(pkg, min_version = NULL) {\n    is_installed &lt;- suppressWarnings(suppressPackageStartupMessages(\n      require(pkg, character.only = TRUE)\n    ))\n    if (is_installed & !is.null(min_version)) {\n      version_ok &lt;- packageVersion(pkg) &gt;= min_version\n    }    \n    if (!is_installed | !version_ok) {\n        install.packages(pkg)\n    }\n  }\npackages &lt;- c(\n  \"bayesplot\", \n  \"brms\", \n  \"ggplot2\",\n  \"gt\",\n  \"here\", \n  \"janitor\",\n  \"modelr\",\n  \"posterior\",\n  \"readxl\",\n  \"rstan\",\n  \"tidybayes\", \n  \"tidyverse\",   \n  \"truncnorm\",\n  \"sn\",\n  \"fuzzyjoin\",\n  \"glue\"\n)\nsapply(packages, install_if_not_available)\nremotes::install_github(\"genentech/jmpost\")",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#optional-install-cmdstanr",
    "href": "session-tgi/0_setup.html#optional-install-cmdstanr",
    "title": "0. Setup",
    "section": "",
    "text": "Optionally, you can use cmdstanr as the backend of brms for fitting the models.\nThere are a few advantages of using cmdstanr over the default rstan:\n\nWith cmdstanr, you can cache the compiled model, therefore you don’t need to recompile the model as long as the same formula is used, even after restarting R session (or re-opening RStudio).\ncmdstanr is more up-to-date and more actively maintained compared to rstan.\n\nA detailed installation guide for cmdstanr is available here. Here is a brief summary:\n\nInstall cmdstanr with:\n# Typically, you install cmdstanr from the R-universe as follows:\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\n\n# If the above does not work, e.g. because you in a company network or VPN, you can install cmdstanr from GitHub instead:\nremotes::install_github(\"stan-dev/cmdstanr@*release\")    \nCheck the compiler toolchain with:\ncmdstanr::check_cmdstan_toolchain()\n\nThis should not be a problem on Mac and Linux (including RStudio Cloud instances), but might be a problem on Windows.\nIf you have problems, please check the installation guide for Windows.\n\nRTools is the easiest way.\nEven if you have RTools, you might still see an error like \"Rtools44 installation found but the toolchain was not installed.\".\nIn this case, you can run cmdstanr::check_cmdstan_toolchain(fix = TRUE) and this will likely resolve the issue.\n\n\nInstall the CmdStan backend with:\ncmdstanr::install_cmdstan(cores = 2)\nTesting the cmdstanr installation\n\nIf the above installation was successful, you should now be able to run the following simple model.\n\nlibrary(cmdstanr)\nfile &lt;- file.path(\n  cmdstan_path(), \n  \"examples\", \"bernoulli\", \"bernoulli.stan\"\n)\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500\n)",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html",
    "href": "session-tgi/4_tgi_cb_brms.html",
    "title": "4. Claret-Bruno model",
    "section": "",
    "text": "This appendix shows how the Claret-Bruno model can be implemented in a Bayesian framework using the brms package in R.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#setup-and-load-data",
    "href": "session-tgi/4_tgi_cb_brms.html#setup-and-load-data",
    "title": "4. Claret-Bruno model",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#claret-bruno-model",
    "href": "session-tgi/4_tgi_cb_brms.html#claret-bruno-model",
    "title": "4. Claret-Bruno model",
    "section": "Claret-Bruno model",
    "text": "Claret-Bruno model\nIn the Claret-Bruno model we have again the baseline SLD and the growth rate as in the Stein-Fojo model. Then in addition we have the inhibition response rate \\(\\psi_{p}\\) and the treatment resistance rate \\(\\psi_{c}\\). The model is then:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\exp \\left\\{\n  \\psi_{k_{g}i} t_{ij} -\n  \\frac{\\psi_{pi}}{\\psi_{ci}} (1 - \\exp(-\\psi_{ci} t_{ij}))\n\\right\\}\n\\]\nfor positive times \\(t_{ij}\\). Again, if the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. That is, we have then \\(\\psi_{pi} = 0\\). Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\exp \\{\n  \\psi_{k_{g}i} t_{ij} -\n  \\frac{\\psi_{pi}}{\\psi_{ci}} (1 - \\exp(-\\psi_{ci} t_{ij}))\n\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\nFor the new model parameters we can again use log-normal prior distributions.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#fit-model",
    "href": "session-tgi/4_tgi_cb_brms.html#fit-model",
    "title": "4. Claret-Bruno model",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * exp(kg * year - (p / c) * (1 - exp(-c * year)))) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # Define the standard deviation (called sigma in brms) as a \n  # coefficient tau times the mean.\n  # sigma = tau * ystar is modelled on the log scale though, therefore:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations\n  nlf(b0 ~ exp(lb0)) +\n  nlf(kg ~ exp(lkg)) +\n  nlf(p ~ exp(lp)) +\n  nlf(c ~ exp(lc)) +\n  # Define random effect structure\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lkg ~ 1 + (1 | id)) +\n  lf(lp ~ 1 + (1 | id)) + \n  lf(lc ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.5), 0.1), nlpar = \"lkg\"),\n  prior(normal(0, 1), nlpar = \"lp\"),\n  prior(normal(log(0.5), 0.1), nlpar = \"lc\"),\n  prior(normal(2, 1), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(1, 1), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 0.5), lb = 0, nlpar = \"lp\", class = \"sd\"),\n  prior(normal(0, 0.5), lb = 0, nlpar = \"lc\", class = \"sd\"),\n  prior(normal(0, 1), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lkg = array(-0.69),\n  b_lp = array(0),\n  b_lc = array(-0.69),\n  sd_1 = array(0.5),\n  sd_2 = array(0.5),\n  sd_3 = array(0.1),\n  sd_4 = array(0.1),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients),\n  z_4 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/cb3.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else if (interactive()) {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = WARMUP + ITER, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH,\n    adapt_delta = 0.9,\n    max_treedepth = 15\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsave_fit_sum_file &lt;- here(\"session-tgi/cb3_fit_sum.RData\")\nif (file.exists(save_fit_sum_file)) {\n  load(save_fit_sum_file)\n} else {\n  fit_sum &lt;- summary(fit)\n  save(fit_sum, file = save_fit_sum_file)\n}\nfit_sum\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ ystar \n         ystar ~ int_step(year &gt; 0) * (b0 * exp(kg * year - (p/c) * (1 - exp(-c * year)))) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(ystar)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         kg ~ exp(lkg)\n         p ~ exp(lp)\n         c ~ exp(lc)\n         lb0 ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n         lp ~ 1 + (1 | id)\n         lc ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.01      419     1044\nsd(lkg_Intercept)     1.04      0.06     0.92     1.17 1.01      696     1181\nsd(lp_Intercept)      1.58      0.11     1.39     1.80 1.01      443     1110\nsd(lc_Intercept)      1.57      0.15     1.28     1.87 1.01      657      935\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.15      0.00     0.15     0.16 1.00     1345     3193\nlb0_Intercept     3.62      0.02     3.57     3.66 1.01      200      330\nlkg_Intercept    -1.00      0.07    -1.14    -0.86 1.01     1020     1906\nlp_Intercept     -0.82      0.14    -1.10    -0.57 1.01      643     1594\nlc_Intercept     -0.13      0.10    -0.34     0.07 1.00     1335     2478\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe did obtain here a warning about divergent transitions, see stan documentation for details:\nWarning message:\nThere were 4489 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup \nHowever, the effective sample size is high, i.e. the Rhat values are close to 1. This indicates that the chains have converged. We can proceed with the post-processing.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#parameter-estimates",
    "href": "session-tgi/4_tgi_cb_brms.html#parameter-estimates",
    "title": "4. Claret-Bruno model",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\n\nShow the code\npost_df_file &lt;- here(\"session-tgi/cb3_post_df.RData\")\nif (file.exists(post_df_file)) {\n  load(post_df_file)\n} else {\n  post_df &lt;- as_draws_df(fit) |&gt; \n    subset_draws(iteration = (1:1000) * 2)\n  save(post_df, file = post_df_file)\n}\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lkg_Intercept\"       \n [4] \"b_lp_Intercept\"         \"b_lc_Intercept\"         \"sd_id__lb0_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"sd_id__lp_Intercept\"    \"sd_id__lc_Intercept\"   \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_p = exp(b_lp_Intercept + sd_id__lp_Intercept^2 / 2),\n    theta_c = exp(b_lc_Intercept + sd_id__lc_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    omega_p = sd_id__lp_Intercept,\n    omega_c = sd_id__lc_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    cv_p = sqrt(exp(sd_id__lp_Intercept^2) - 1),\n    cv_c = sqrt(exp(sd_id__lc_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nLet’s first look at the population level parameters:\n\n\nShow the code\ncb_pop_params &lt;- c(\"theta_b0\", \"theta_kg\", \"theta_p\", \"theta_c\", \"sigma\")\n\nmcmc_trace(post_df, pars = cb_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_dens_overlay(post_df, pars = cb_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = cb_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nThe trace plots look good. The chains seem to have converged and the pairs plot shows no strong correlations between the parameters. Let’s check the table:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  dplyr::select(theta_b0, theta_kg, theta_p, theta_c, omega_0, omega_g, omega_p, omega_c,\n  cv_0, cv_g, cv_p, cv_c,\n  sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.142\n44.118\n1.093\n1.090\n42.411\n45.949\n1.003\n198.908\n390.883\n\n\ntheta_kg\n0.636\n0.634\n0.042\n0.041\n0.570\n0.711\n1.000\n1,093.102\n1,749.378\n\n\ntheta_p\n1.539\n1.515\n0.183\n0.179\n1.284\n1.871\n1.002\n361.463\n395.589\n\n\ntheta_c\n3.098\n2.984\n0.678\n0.607\n2.234\n4.366\n1.001\n552.128\n669.150\n\n\nomega_0\n0.581\n0.581\n0.016\n0.016\n0.556\n0.608\n1.003\n429.108\n1,002.446\n\n\nomega_g\n1.043\n1.040\n0.063\n0.062\n0.943\n1.149\n1.004\n664.633\n1,063.643\n\n\nomega_p\n1.576\n1.570\n0.105\n0.104\n1.413\n1.757\n1.001\n434.576\n976.113\n\n\nomega_c\n1.569\n1.565\n0.150\n0.153\n1.326\n1.822\n1.001\n638.934\n941.209\n\n\ncv_0\n0.634\n0.634\n0.020\n0.021\n0.602\n0.669\n1.003\n429.108\n1,002.446\n\n\ncv_g\n1.409\n1.396\n0.140\n0.136\n1.197\n1.655\n1.004\n664.633\n1,063.643\n\n\ncv_p\n3.383\n3.278\n0.645\n0.578\n2.521\n4.577\n1.001\n434.576\n976.113\n\n\ncv_c\n3.415\n3.254\n0.930\n0.838\n2.192\n5.165\n1.001\n638.934\n941.209\n\n\nsigma\n0.154\n0.154\n0.002\n0.002\n0.150\n0.158\n1.000\n1,258.184\n2,273.384\n\n\n\n\n\n\n\nWe see similar estimated values as before for \\(\\theta_{b_{0}}\\), \\(\\theta_{k_{g}}\\) and \\(\\sigma\\).",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#observation-vs-model-fit",
    "href": "session-tgi/4_tgi_cb_brms.html#observation-vs-model-fit",
    "title": "4. Claret-Bruno model",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim_save_file &lt;- here(\"session-tgi/cb3_sim_df.RData\")\nif (file.exists(df_sim_save_file)) {\n  load(df_sim_save_file)\n} else {\n  df_sim &lt;- df_subset |&gt; \n    data_grid(\n      id = pt_subset, \n      year = seq_range(year, 101)\n    ) |&gt;\n    add_epred_draws(fit) |&gt;\n    median_qi()\n  save(df_sim, file = df_sim_save_file)\n}\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = .epred, ymin = .lower, ymax = .upper), \n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = .epred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"CB model fit\")\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThis also looks good. The model seems to capture the data well.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#with-jmpost",
    "href": "session-tgi/4_tgi_cb_brms.html#with-jmpost",
    "title": "4. Claret-Bruno model",
    "section": "With jmpost",
    "text": "With jmpost\nThis model can also be fit with the jmpost package. The corresponding function is LongitudinalClaretBruno. The statistical model is specified in the vignette here.\nHomework: Implement the generalized Claret-Bruno model with jmpost and compare the results with the brms implementation.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-os/0_setup.html",
    "href": "session-os/0_setup.html",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "This part is required for both the jmpost chapter as well as the brms chapter.\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 2: OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-os/0_setup.html#setup",
    "href": "session-os/0_setup.html#setup",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "This part is required for both the jmpost chapter as well as the brms chapter.\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 2: OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-os/0_setup.html#data-preparation",
    "href": "session-os/0_setup.html#data-preparation",
    "title": "0. Setup and data preparation",
    "section": "Data preparation",
    "text": "Data preparation\nThis part is only required for the jmpost chapter.\nWe will again use the OAK study data.\nFor the tumor growth data, we are using the S1 data set from here. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\ntumor_data &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\") |&gt; \n    read_excel(sheet = \"Study4\") |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm)),\n        arm = fct_recode(arm, \"Docetaxel\" = \"1\", \"MPDL3280A\" = \"2\")\n    ) |&gt; \n    select(id, year, sld, arm)\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 4\n  id                       year   sld arm      \n  &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 -594245265511412736  -0.00821  42   Docetaxel\n2 -594245265511412736   0.0986   44   Docetaxel\n3 -594245265511412736   0.222    44   Docetaxel\n4 -4078394139718744064 -0.0110   65.8 Docetaxel\n5 -4078394139718744064  0.832    66   Docetaxel\n6 -4078394139718744064  1.07     68   Docetaxel\n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        \n 5394902984416364544 :  16   Min.   :-0.1068   Min.   :  2.00  \n 7160320731596789760 :  16   1st Qu.: 0.1123   1st Qu.: 19.05  \n -4159496062492130816:  15   Median : 0.2519   Median : 32.00  \n -7900338178541499392:  15   Mean   : 0.3995   Mean   : 38.16  \n 4878279915140903936 :  15   3rd Qu.: 0.5749   3rd Qu.: 51.00  \n 9202154259772598272 :  15   Max.   : 2.0753   Max.   :228.00  \n (Other)             :4034                     NA's   :27      \n        arm      \n Docetaxel:1658  \n MPDL3280A:2468  \n                 \n                 \n                 \n                 \n                 \n\n\nHere we have 701 patients, and we know from Fig. 1 in the publication that this is the subset of patients with at least 3 tumor size measurements. We have guessed from the second Excel sheet with parameter estimates that arm 1 means docetaxel and arm 2 means MPDL3280A. This also seems reasonable given the larger number of measurements per patient:\n\n\nShow the code\n# make a table\ntumor_data |&gt; \n    group_by(id) |&gt; \n    summarize(arm = arm[1], n = n())  |&gt; \n    group_by(arm) |&gt; \n    summarize(mean_n = mean(n))\n\n\n# A tibble: 2 × 2\n  arm       mean_n\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Docetaxel   5.10\n2 MPDL3280A   6.56\n\n\nFor the OS data, we will be using the Supplementary Table 8 from here. Again, we have copied the data set in this GitHub repository.\nFrom the supplementary material, we know that:\n\nPFS/OS: time in months\nPFS.CNSR/OS.CNSR: 1 for censor, 0 for event\n\nTherefore we will convert the time to years and the censoring to a logical variable:\n\n\nShow the code\nos_data &lt;- here(\"data/41591_2018_134_MOESM3_ESM.xlsx\") |&gt; \n    read_excel(sheet = \"OAK_Clinical_Data\") |&gt; \n    clean_names() |&gt; \n    rename(\n        age = bage,\n        race = race2,\n        ecog = ecoggr,\n        arm = trt01p,\n        response = bcor\n    ) |&gt; \n    mutate(\n        id = factor(as.character(pt_id)),\n        sld = as.numeric(na_if(bl_sld, \".\")),\n        pfs_time = as.numeric(pfs) / 12,\n        pfs_event = as.numeric(pfs_cnsr) == 0,\n        os_time = as.numeric(os) / 12,\n        os_event = as.numeric(os_cnsr) == 0,\n        race = factor(race),\n        ecog = factor(ecog),\n        arm = factor(arm),\n        response = factor(\n          response, \n          levels = c(\"CR\", \"PR\", \"SD\", \"PD\", \"NE\")\n        ),\n        sex = factor(sex)\n    ) |&gt; \n    select(\n      id,\n      arm,\n      ecog,\n      age,\n      race,\n      sex,\n      sld,\n      response,\n      pfs_time,\n      pfs_event,\n      os_time,\n      os_event\n    )\n\nhead(os_data)\n\n\n# A tibble: 6 × 12\n  id    arm    ecog    age race  sex     sld response pfs_time pfs_event os_time\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;lgl&gt;       &lt;dbl&gt;\n1 318   Docet… 1        64 WHITE M     152   PD          0.107 TRUE        0.433\n2 1088  Docet… 0        65 WHITE M      36   PD          0.194 TRUE        0.402\n3 345   Docet… 1        75 WHITE M      92   &lt;NA&gt;        0.162 TRUE        0.162\n4 588   Docet… 0        61 WHITE F      36   SD          1.02  TRUE        2.05 \n5 306   MPDL3… 1        53 WHITE F      86   PD          0.118 TRUE        0.498\n6 718   Docet… 1        80 WHITE M      45.7 SD          0.712 TRUE        1.60 \n# ℹ 1 more variable: os_event &lt;lgl&gt;\n\n\nShow the code\nsummary(os_data)\n\n\n       id             arm      ecog         age           race     sex    \n 1000   :  1   Docetaxel:425   0:315   Min.   :33.00   ASIAN:180   F:330  \n 1001   :  1   MPDL3280A:425   1:535   1st Qu.:57.00   OTHER: 72   M:520  \n 1002   :  1                           Median :64.00   WHITE:598          \n 1003   :  1                           Mean   :63.23                      \n 1004   :  1                           3rd Qu.:70.00                      \n 1005   :  1                           Max.   :85.00                      \n (Other):844                                                              \n      sld         response      pfs_time        pfs_event      \n Min.   : 10.00   CR  :  7   Min.   :0.002738   Mode :logical  \n 1st Qu.: 41.00   PR  :108   1st Qu.:0.117728   FALSE:95       \n Median : 66.00   SD  :327   Median :0.240931   TRUE :755      \n Mean   : 76.69   PD  :304   Mean   :0.443174                  \n 3rd Qu.: 99.00   NE  : 24   3rd Qu.:0.580424                  \n Max.   :316.00   NA's: 80   Max.   :2.242300                  \n NA's   :1                                                     \n    os_time          os_event      \n Min.   :0.002738   Mode :logical  \n 1st Qu.:0.364134   FALSE:281      \n Median :0.803559   TRUE :569      \n Mean   :0.949841                  \n 3rd Qu.:1.620808                  \n Max.   :2.253251                  \n                                   \n\n\nNow we would like to join the two data sets, i.e. know which tumor growth data belongs to which baseline covariates and overall survival time and event indicator. Unfortunately we cannot use the id variable as it is not consistent between the two data sets, because both were anonymized. Instead, we will use the sld variable in combination with the treatment arm. We can also double check with the best overall response variable, because that is closely related to the tumor growth.\nFirst, we will summarize the patient information int he tumor data set. We will calculate the baseline SLD, determine the time of the last tumor measurement and approximate the best overall response by looking at the change from baseline:\n\n\nShow the code\nget_baseline &lt;- function(sld, year) {\n    which_base &lt;- tail(which(year &lt;= 0), 1L)\n    if (length(which_base) == 0) {\n      which_base &lt;- which.min(year)\n    }\n    sld[which_base]\n}\n\nget_contig_below_thresh &lt;- function(sld, year, bsld, thresh) {\n    rle_res &lt;- with(\n        rle(((sld[year &gt; 0] - bsld) / bsld) &lt; 0.2), \n        lengths[values]\n    )\n    if (length(rle_res) == 0) {\n      0\n    } else {\n      max(rle_res)\n    }\n}\n\ntumor_data_summary &lt;- tumor_data |&gt; \n    group_by(id) |&gt; \n    arrange(year) |&gt;\n    summarize(\n      arm = arm[1L],\n      bsld = get_baseline(sld, year),\n      last_year = tail(year, 1L),\n      nadir = min(sld[year &gt;= 0], na.rm = TRUE),\n      max_cfn = max((sld[year &gt;= 0] - nadir) / nadir, na.rm = TRUE),\n      min_cfb = min((sld[year &gt;= 0] - bsld) / bsld, na.rm = TRUE),\n      contig_below_0.2 = get_contig_below_thresh(sld, year, bsld, thresh = 0.2),\n      approx_response = case_when(\n        min_cfb &lt;= -0.3 ~ \"PR\",\n        contig_below_0.2 &gt;= 2 ~ \"SD\",\n        max_cfn &gt;= 0.2 ~ \"PD\",        \n        .default = \"NE\"\n      )   \n    )\nhead(tumor_data_summary)\n\n\n# A tibble: 6 × 9\n  id                arm    bsld last_year nadir max_cfn min_cfb contig_below_0.2\n  &lt;fct&gt;             &lt;fct&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n1 -100096848245159… Doce…    53     0.356    55  0       0.0377                3\n2 -102777882269110… MPDL…    11     0.901     2  2      -0.818                 7\n3 -105049343897269… MPDL…    20     0.345    21  1.62    0.05                  2\n4 -106934201929572… Doce…    15     1.72     11  0.273  -0.267                12\n5 -108949367808374… Doce…    33     0.709    31  0.0968 -0.0606                6\n6 -110624773747042… MPDL…    24     1.32     16  0.562  -0.333                 9\n# ℹ 1 more variable: approx_response &lt;chr&gt;\n\n\nNow let’s try to fuzzy join this with the OS data set, based on the baseline SLD and the treatment arm. In addition, we know that the last time point in the tumor data needs to be before the OS time point. We will also check that the best overall response is the same in both data sets. There will be some errors here: For example, we don’t know whether there were new lesions leading to a progression assessment, or whether the patient had later better results. So we need to expect that the matching will not be perfect.\n\n\nShow the code\nos_data_keys &lt;- os_data |&gt; \n    select(id, arm, sld, pfs_time, os_time, response)\nhead(os_data_keys)\n\n\n# A tibble: 6 × 6\n  id    arm         sld pfs_time os_time response\n  &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n1 318   Docetaxel 152      0.107   0.433 PD      \n2 1088  Docetaxel  36      0.194   0.402 PD      \n3 345   Docetaxel  92      0.162   0.162 &lt;NA&gt;    \n4 588   Docetaxel  36      1.02    2.05  SD      \n5 306   MPDL3280A  86      0.118   0.498 PD      \n6 718   Docetaxel  45.7    0.712   1.60  SD      \n\n\nShow the code\ndist_match &lt;- function(v1, v2) {\n    dist &lt;- abs(v1 - v2)\n    data.frame(include = (dist &lt;= 0.05))\n}\n\nless_match &lt;- function(lower, upper) {\n    data.frame(include = lower &lt;= upper)\n}\n\ntumor_os_data_joined &lt;- tumor_data_summary |&gt; \n    fuzzy_left_join(\n        os_data_keys,\n        by = c(\n          \"arm\" = \"arm\", \n          \"bsld\" = \"sld\",          \n          \"last_year\" = \"os_time\",\n          \"approx_response\" = \"response\"\n        ),\n        match_fun = list(\n          `==`, \n          dist_match, \n          less_match,\n          `==`\n        )\n    )\nnrow(tumor_os_data_joined)\n\n\n[1] 954\n\n\nWe see that we cannot match the two data sets exactly:\n\nWe have less patients to start with in the tumor size data set.\nWe have patients in the tumor size data set which match multiple patients in the OS data set based on the treatment arm and the baseline SLD, and also the last measurement time point cannot disambiguate them.\nWe have patients in the tumor size data set which do not match any patient in the OS data set.\n\nHowever, for the purpose of this training we don’t need to be able to recover the true full data set. Therefore we will just subset the matched data set to a reasonable combination of the two data sets:\n\n\nShow the code\ntumor_os_data_joined_subset &lt;- tumor_os_data_joined |&gt; \n    na.omit() |&gt; \n    filter(!duplicated(id.y)) |&gt;\n    filter(!duplicated(id.x))\nnrow(tumor_os_data_joined_subset)\n\n\n[1] 203\n\n\nNow let’s save the joining information:\n\n\nShow the code\ntgi_os_join_keys &lt;- tumor_os_data_joined_subset |&gt; \n    select(id.x, id.y, arm.x) |&gt; \n    rename(\n        id_tgi = id.x,\n        id_os = id.y,\n        arm = arm.x\n    )\ntable(tgi_os_join_keys$arm)\n\n\n\nDocetaxel MPDL3280A \n       96       107 \n\n\nSo we have about 100 patients in each arm.\nWe subset the two data sets accordingly:\n\n\nShow the code\ntumor_data &lt;- tumor_data |&gt; \n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_tgi\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id) |&gt; \n    rename(id = id_os)\n\nos_data &lt;- os_data |&gt;\n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_os\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id_tgi)",
    "crumbs": [
      "Session 2: OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html",
    "href": "session-os/1_os_weibull_jmpost.html",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "",
    "text": "The purpose of this document is to show a minimal workflow for fitting a Weibull OS model using the jmpost package.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#setup-and-load-data",
    "href": "session-os/1_os_weibull_jmpost.html#setup-and-load-data",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Setup and load data",
    "text": "Setup and load data\nHere we execute the R code from the setup and data preparation chapter, see the full code here.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#tgi-model-fitting",
    "href": "session-os/1_os_weibull_jmpost.html#tgi-model-fitting",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "TGI model fitting",
    "text": "TGI model fitting\nLet’s use jmpost to fit the Stein-Fojo model to the TGI dataset. This works analogously to what we showed in the previous session.\nFirst we again prepare the data objects, starting with the subject level data:\n\n\nShow the code\nsubj_df &lt;- os_data |&gt;\n    mutate(study = \"OAK\") |&gt;\n    select(study, id, arm)\n\nsubj_data &lt;- DataSubject(\n    data = subj_df,\n    subject = \"id\",\n    arm = \"arm\",\n    study = \"study\"\n)\n\n\nNext we prepare the longitudinal data object.\n\n\nShow the code\nlong_df &lt;- tumor_data |&gt;\n    select(id, year, sld)\nlong_data &lt;- DataLongitudinal(\n    data = long_df,\n    formula = sld ~ year\n)\n\n\nNow we can create the JointData object for the TGI model:\n\n\nShow the code\ntgi_joint_data &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data\n)\n\n\nWe specify the Stein-Fojo model together with the priors for the model parameters:\n\n\nShow the code\ntgi_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    )\n)\n\n\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-os/tgi1.rds\")\nif (file.exists(save_file)) {\n    tgi_results &lt;- readRDS(save_file)\n} else {\n    tgi_results &lt;- sampleStanModel(\n        tgi_mod,\n        data = tgi_joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(tgi_results, file = save_file)\n}\n\n\nThe function saveObject() was added to the package recently, please update your installation if it is not yet available.\nNote that this is considerably faster than fitting the larger dataset of 701 patients. Let’s check the convergence of the population parameters:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nmcmc_tgi_results &lt;- cmdstanr::as.CmdStanMCMC(tgi_results)\nmcmc_tgi_results$summary(vars)\n\n\n# A tibble: 11 × 10\n   variable    mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu…  3.75   3.75  0.0375  0.0376   3.69   3.81  1.01      298.     572.\n 2 lm_sf_mu… -0.270 -0.255 0.298   0.308   -0.789  0.163 1.00      641.     680.\n 3 lm_sf_mu… -1.22  -1.16  0.360   0.343   -1.90  -0.708 0.999     723.     852.\n 4 lm_sf_mu… -0.731 -0.723 0.174   0.167   -1.02  -0.462 1.00      604.     762.\n 5 lm_sf_mu… -0.973 -0.969 0.157   0.150   -1.25  -0.728 1.00      641.     869.\n 6 lm_sf_si…  0.129  0.129 0.00381 0.00373  0.123  0.135 1.00      883.     892.\n 7 lm_sf_om…  0.531  0.529 0.0288  0.0294   0.486  0.583 1.00      518.     751.\n 8 lm_sf_om…  1.09   1.08  0.212   0.207    0.771  1.44  1.00      704.     861.\n 9 lm_sf_om…  1.45   1.42  0.271   0.251    1.07   1.95  0.999     731.     994.\n10 lm_sf_om…  0.694  0.684 0.0963  0.0881   0.556  0.860 1.01      930.     904.\n11 lm_sf_om…  0.994  0.983 0.103   0.100    0.840  1.17  1.01      762.     869.\n\n\nShow the code\ndraws_tgi_results &lt;- mcmc_tgi_results$draws(vars)\nmcmc_trace(draws_tgi_results)\n\n\n\n\n\n\n\n\n\nSo this looks good.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#extract-individual-growth-rate-estimates",
    "href": "session-os/1_os_weibull_jmpost.html#extract-individual-growth-rate-estimates",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Extract individual growth rate estimates",
    "text": "Extract individual growth rate estimates\nWe can now extract the individual growth rate estimates from the model. Later, in the joint model, we are going to use the log of the growth parameter as the link. Therefore we also here first log transform the sampled values of the growth rate estimates \\(\\psi_{\\text{kg}, i}\\), and then take the mean. Since the relevant random effect parameter samples are already stored in the mcmc_tgi_results object, we can work with that via the rvars interface.\nThe only tricky part is that we need to match the IDs of the patients manually, because jmpost just numbers the patients in the order they appear in the data, which is then the index for all the random effects and individual growth parameters \\(\\psi_{\\text{kg}, i}\\).\nHowever, we need to be careful to extract the data order from the tgi_joint_data object, because the subject data set is reordered by the sorted patient ID during the creation of the DataJoint object. If we don’t do this correctly, then we would permute the growth rates randomly between the patients and thereby destroy the link between the growth rates and the patients.\n\n\nShow the code\nlog_growth_samples &lt;- mcmc_tgi_results |&gt;\n    # We use here `rvars` because it allows to apply the\n    # mutation across all subjects at once.\n    as_draws_rvars() |&gt;\n    mutate_variables(log_growth = log(lm_sf_psi_kg))\n\nsubj_log_kg_est &lt;- log_growth_samples |&gt;\n    subset_draws(variable = \"log_growth\") |&gt;\n    summary() |&gt;\n    # Important: Take the IDs from `tgi_joint_data` and not from `subj_data` here!\n    mutate(id = tgi_joint_data@subject@data$id)\n\nhead(subj_log_kg_est)\n\n\n# A tibble: 6 × 11\n  variable    mean median    sd   mad    q5    q95  rhat ess_bulk ess_tail id   \n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;\n1 log_grow… -0.982 -0.808 0.902 0.927 -2.65  0.130 1.00      937.    1037. 1008 \n2 log_grow… -1.09  -1.04  0.894 0.921 -2.62  0.246 0.999     929.     731. 1018 \n3 log_grow… -1.01  -0.911 0.367 0.264 -1.72 -0.590 1.00      888.     717. 1019 \n4 log_grow… -0.760 -0.729 0.594 0.619 -1.79  0.117 1.00      857.     950. 1021 \n5 log_grow… -1.09  -1.03  0.885 0.910 -2.62  0.217 1.01      976.     852. 1026 \n6 log_grow… -1.46  -1.38  0.814 0.845 -2.96 -0.313 1.00      905.     915. 1027 \n\n\nIn the latest version of jmpost this process has now been simplified, and we can use the LongitudinalRandomEffects() function as follows:\n\n\nShow the code\nsubj_log_kg_est_alt &lt;- LongitudinalRandomEffects(tgi_results) |&gt;\n    # We need to convert this to a data.frame to be able to do the log transformation.\n    as.data.frame() |&gt;\n    filter(parameter == \"g\") |&gt;\n    mutate(values = log(values)) |&gt;\n    group_by(subject) |&gt;\n    summarize(log_kg_est = mean(values))\nhead(subj_log_kg_est_alt)\n\n\n# A tibble: 6 × 2\n  subject log_kg_est\n  &lt;chr&gt;        &lt;dbl&gt;\n1 1008        -0.982\n2 1018        -1.09 \n3 1019        -1.01 \n4 1021        -0.760\n5 1026        -1.09 \n6 1027        -1.46 \n\n\nSo this gives the same resulting log growth rates, which is reassuring.\nWe now add the e.g. posterior mean estimate of the individual log growth rates to the OS data set, such that we will be able to use it below as a covariate in the OS model:\n\n\nShow the code\nos_data_with_log_kg_est &lt;- os_data |&gt;\n    select(id, arm, ecog, age, race, sex, os_time, os_event) |&gt;\n    left_join(select(subj_log_kg_est, mean, id), by = \"id\") |&gt;\n    rename(log_kg_est = mean)\nhead(os_data_with_log_kg_est)\n\n\n# A tibble: 6 × 9\n  id    arm       ecog    age race  sex   os_time os_event log_kg_est\n  &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;\n1 588   Docetaxel 0        61 WHITE F       2.05  FALSE       -0.571 \n2 330   MPDL3280A 1        56 WHITE F       1.68  FALSE       -1.89  \n3 791   Docetaxel 0        72 WHITE F       0.901 TRUE        -0.518 \n4 635   Docetaxel 0        42 OTHER F       1.66  TRUE        -0.642 \n5 365   MPDL3280A 0        64 WHITE F       1.43  TRUE        -0.427 \n6 773   Docetaxel 0        65 WHITE M       1.63  FALSE        0.0340\n\n\nShow the code\nsave_file &lt;- here(\"session-os/os_data_with_log_kg.rds\")\nif (!file.exists(save_file)) {\n    saveRDS(os_data_with_log_kg_est, file = save_file)\n}\n\n\nAs a sanity check to make sure we linked the growth rates correctly to the patients, let’s compare the average log growth rates computed from the above data set with the average we would expect based on the log normal distribution. Remember from the TGI session that we have:\n\\[\n\\log(\\psi_{k_{g}}) \\sim \\text{Normal}(\\mu_{k_{g}}, \\omega_{g})\n\\]\nwithin each treatment arm.\n\n\nShow the code\n# Compute the mean using the individual estimates:\nlog_growth_summary &lt;- os_data_with_log_kg_est |&gt;\n    group_by(arm) |&gt;\n    summarise(mean = mean(log_kg_est))\n\n# And now compute the mean from the original model parameter samples:\nlog_growth_check &lt;- mcmc_tgi_results$summary(\"lm_sf_mu_kg\") |&gt;\n    mutate(\n        arm = recode(\n            variable,\n            # The order here is given by the order of the arm factor levels.\n            \"lm_sf_mu_kg[1]\" = \"Docetaxel\",\n            \"lm_sf_mu_kg[2]\" = \"MPDL3280A\"\n        )\n    ) |&gt;\n    select(arm, mean)\n\n# We can compare:\nlog_growth_summary\n\n\n# A tibble: 2 × 2\n  arm         mean\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Docetaxel -0.738\n2 MPDL3280A -0.989\n\n\nShow the code\nlog_growth_check\n\n\n# A tibble: 2 × 2\n  arm         mean\n  &lt;chr&gt;      &lt;dbl&gt;\n1 Docetaxel -0.731\n2 MPDL3280A -0.973\n\n\nSo this looks good, and we can continue with this data set.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#os-model-fitting",
    "href": "session-os/1_os_weibull_jmpost.html#os-model-fitting",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "OS model fitting",
    "text": "OS model fitting\nNow we can fit the OS model.\nWe start by preparing the DataSurvival object:\n\n\nShow the code\nsurv_data &lt;- DataSurvival(\n    data = os_data_with_log_kg_est,\n    formula = Surv(os_time, os_event) ~\n        ecog + age + race + sex + log_kg_est\n)\n\n\nNote that we are not including the treatment arm here, but only the log growth rate estimates. In addition, the covariates in the model include the ECOG score, age, race and sex. The idea is that the treatment effect is fully captured in the log growth rate estimates, which is referred to as “Working assumption (1)” in the slides.\nNow we can create the JointData object for the OS model:\n\n\nShow the code\nos_joint_data &lt;- DataJoint(\n    subject = subj_data,\n    survival = surv_data\n)\n\n\nWe specify the Weibull model together with the priors for the model parameters. We take vague priors for the regression coefficients beta. For lambda and gamma, we start from the scale of the survival data at hand: the average survival time is 1.3 years, just taking a crude average of all survival times.\nWe can quickly write the function that gives the mean of the Weibull distribution with fixed lambda and gamma:\n\n\nShow the code\nweibull_mean &lt;- function(lambda, gamma) {\n    base::gamma(1 + 1 / gamma) / lambda\n}\n\n\nTherefore, playing around with this a bit, we can e.g. center the prior for lambda around 0.7 and the prior for gamma around 1.5, giving a mean survival time of 1.3 years.\nIf we want to use Gamma distributions e.g. for lambda and gamma, we can use the prior_gamma function. The two parameters of this distribution are the shape and the rate. The mean is shape divided by the rate. So easiest is to keep a rate of 1 and just set the shape to the mean value we need:\n\n\nShow the code\nos_mod &lt;- JointModel(\n    survival = SurvivalWeibullPH(\n        lambda = prior_gamma(0.7, 1),\n        gamma = prior_gamma(1.5, 1),\n        beta = prior_normal(0, 20)\n    )\n)\n\n\nBecause we use a large prior variance for beta, we need to adjust the default initial value construction used in jmpost. As explained here, we can change the shrinkage of the initial values to the mean. We can then check what the initial values will be, to make sure that they are reasonable:\n\n\nShow the code\noptions(\"jmpost.prior_shrinkage\" = 0.999)\n\ninitialValues(os_mod, n_chains = CHAINS)\n\n\n[[1]]\n[[1]]$sm_weibull_ph_lambda\n[1] 0.6995368\n\n[[1]]$sm_weibull_ph_gamma\n[1] 1.500504\n\n[[1]]$beta_os_cov\n[1] -0.02939312\n\n\n[[2]]\n[[2]]$sm_weibull_ph_lambda\n[1] 0.6993642\n\n[[2]]$sm_weibull_ph_gamma\n[1] 1.501167\n\n[[2]]$beta_os_cov\n[1] -0.01456326\n\n\n[[3]]\n[[3]]$sm_weibull_ph_lambda\n[1] 0.6997494\n\n[[3]]$sm_weibull_ph_gamma\n[1] 1.500216\n\n[[3]]$beta_os_cov\n[1] 0.02693967\n\n\n[[4]]\n[[4]]$sm_weibull_ph_lambda\n[1] 0.6998758\n\n[[4]]$sm_weibull_ph_gamma\n[1] 1.504811\n\n[[4]]$beta_os_cov\n[1] 0.0001799617\n\n\nSo the values are now close to the means of the respective prior distributions. We can then see later if the chains were converging well. If not, we could as an alternative also manually set initial values, as explained here.\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-os/os1.rds\")\nif (file.exists(save_file)) {\n    os_results &lt;- readRDS(save_file)\n} else {\n    os_results &lt;- sampleStanModel(\n        os_mod,\n        data = os_joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(os_results, file = save_file)\n}\n\n\nNote that here we can get warnings at the beginning of the chains’ sampling process (“The current Metropolis proposal is about to be rejected …”). As long as this only happens in the beginning, and not during the sampling later, then this is not a cause for concern.\nLet’s check the convergence of the population parameters:\n\n\nShow the code\nvars &lt;- c(\n    \"beta_os_cov\",\n    \"sm_weibull_ph_gamma\",\n    \"sm_weibull_ph_lambda\"\n)\n\nmcmc_os_results &lt;- cmdstanr::as.CmdStanMCMC(os_results)\nmcmc_os_results$summary(vars)\n\n\n# A tibble: 8 × 10\n  variable           mean   median      sd     mad      q5    q95  rhat ess_bulk\n  &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 beta_os_cov[1]  0.724    0.728   0.207   0.214    0.386  1.06   1.00      957.\n2 beta_os_cov[2]  0.00445  0.00463 0.00920 0.00937 -0.0101 0.0194 0.999    1043.\n3 beta_os_cov[3]  0.542    0.542   0.400   0.419   -0.124  1.17   0.998     942.\n4 beta_os_cov[4] -0.0268  -0.0307  0.221   0.221   -0.373  0.333  1.00     1046.\n5 beta_os_cov[5]  0.302    0.303   0.201   0.198   -0.0342 0.630  1.00      718.\n6 beta_os_cov[6]  0.597    0.587   0.172   0.170    0.321  0.880  1.00      742.\n7 sm_weibull_ph…  1.69     1.68    0.135   0.131    1.47   1.92   1.00      933.\n8 sm_weibull_ph…  0.248    0.208   0.164   0.115    0.0720 0.556  0.999     983.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nShow the code\ndraws_os_results &lt;- mcmc_os_results$draws(vars)\nmcmc_trace(draws_os_results)",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#interpret-covariate-effects",
    "href": "session-os/1_os_weibull_jmpost.html#interpret-covariate-effects",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Interpret covariate effects",
    "text": "Interpret covariate effects\nIn order to better see which of the coefficients relate to which covariates, we can rename them as follows:\n\n\nShow the code\nsurv_data_design &lt;- as_stan_list(surv_data)$os_cov_design\nos_cov_names &lt;- colnames(surv_data_design)\nold_coef_names &lt;- glue::glue(\"beta_os_cov[{seq_along(os_cov_names)}]\")\ndraws_os_results &lt;- do.call(\n    rename_variables,\n    c(list(draws_os_results), setNames(old_coef_names, os_cov_names))\n)\nmcmc_dens_overlay(draws_os_results) +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nShow the code\nsummary(draws_os_results)\n\n\n# A tibble: 8 × 10\n  variable           mean   median      sd     mad      q5    q95  rhat ess_bulk\n  &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 ecog1           0.724    0.728   0.207   0.214    0.386  1.06   1.00      957.\n2 age             0.00445  0.00463 0.00920 0.00937 -0.0101 0.0194 0.999    1043.\n3 raceOTHER       0.542    0.542   0.400   0.419   -0.124  1.17   0.998     942.\n4 raceWHITE      -0.0268  -0.0307  0.221   0.221   -0.373  0.333  1.00     1046.\n5 sexM            0.302    0.303   0.201   0.198   -0.0342 0.630  1.00      718.\n6 log_kg_est      0.597    0.587   0.172   0.170    0.321  0.880  1.00      742.\n7 sm_weibull_ph…  1.69     1.68    0.135   0.131    1.47   1.92   1.00      933.\n8 sm_weibull_ph…  0.248    0.208   0.164   0.115    0.0720 0.556  0.999     983.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nShow the code\nsave_file &lt;- here(\"session-os/os_draws.rds\")\nif (!file.exists(save_file)) {\n    saveRDS(draws_os_results, file = save_file)\n}\n\n\nSo we can see that the 90% credible interval (CI) for the log_kg_est and ecog1 covariate coefficients excludes 0, so both are “significant” predictors of the hazard rate. On the other hand, the race dummy variables’ and the age variable’s coefficient CIs clearly include 0. The situation is less clear for sex: here the CI barely includes 0.\nIn addition, we can also look at the posterior probabilities to have a hazard ratio above 1:\n\n\nShow the code\ndraws_os_results |&gt;\n    as_draws_df() |&gt;\n    summarise_all(~ mean(. &gt; 0))\n\n\n# A tibble: 1 × 11\n  ecog1   age raceOTHER raceWHITE  sexM log_kg_est sm_weibull_ph_gamma\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;               &lt;dbl&gt;\n1     1 0.677     0.912     0.432  0.93          1                   1\n# ℹ 4 more variables: sm_weibull_ph_lambda &lt;dbl&gt;, .chain &lt;dbl&gt;,\n#   .iteration &lt;dbl&gt;, .draw &lt;dbl&gt;\n\n\nSo we have a more than 90% posterior probability that male patients have a higher hazard than females, and we also see a similarly strong effect here for the OTHER category of race. As we saw from the CI already, the age effect is not so strong.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#observation-vs-model-fit",
    "href": "session-os/1_os_weibull_jmpost.html#observation-vs-model-fit",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nA useful plot displays the model predicted survival function and overlays the non-parametric Kaplan-Meier plot to it. Such a plot is easily obtained using the autoplot() function, as we will see below.\nThe first step consists in generating the survival predictions at the group level with the SurvivalQuantities() function. It is recommended to specify the sequence of time points at which the predictions should be made (using the argument times):\n\n\nShow the code\ntime_grid &lt;- seq(\n    from = 0,\n    to = max(os_data_with_log_kg_est$os_time),\n    length = 100\n)\nos_surv_group_grid &lt;- GridGrouped(\n    times = time_grid,\n    groups = with(\n        subj_df,\n        split(as.character(id), arm)\n    )\n)\nos_surv_pred &lt;- SurvivalQuantities(\n    object = os_results,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\n\n\nNow we can use the autoplot() method:\n\n\nShow the code\nautoplot(os_surv_pred, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nHere the fit seems ok but not perfect, especially for the Docetaxel arm it could be improved maybe. We will try below alternative models to see if we can improve the fit.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#hazard-and-hazard-rate-estimation",
    "href": "session-os/1_os_weibull_jmpost.html#hazard-and-hazard-rate-estimation",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Hazard and hazard rate estimation",
    "text": "Hazard and hazard rate estimation\nSimilarly to the survival function estimation, we can also estimate the hazard function by treatment group.\n\n\nShow the code\nos_hazard_pred &lt;- SurvivalQuantities(\n    object = os_results,\n    grid = os_surv_group_grid,\n    type = \"haz\"\n)\n\n\nAlso this can be plotted using the autoplot() method:\n\n\nShow the code\nautoplot(os_hazard_pred, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nFinally, we can also estimate the hazard rate, which is constant over time here - because we use the Weibull proportional hazards model. We still show this more complicated code here because it will also work later for joint TGI-OS models, where the hazard rate may not be constant any longer.\n\n\nShow the code\nos_hr_est &lt;- os_hazard_pred |&gt;\n    as.data.frame() |&gt;\n    group_by(group, time) |&gt;\n    mutate(sample = row_number()) |&gt;\n    pivot_wider(names_from = group, values_from = values) |&gt;\n    mutate(hr = MPDL3280A / Docetaxel) |&gt;\n    group_by(time) |&gt;\n    summarize(\n        mean = mean(hr, na.rm = TRUE),\n        lower = quantile(hr, 0.05, na.rm = TRUE),\n        upper = quantile(hr, 0.95, na.rm = TRUE)\n    ) |&gt;\n    na.omit() # Omit the time = 0 which has NA\nsummary(os_hr_est)\n\n\n      time              mean            lower          upper       \n Min.   :0.02276   Min.   :0.9028   Min.   :0.88   Min.   :0.9295  \n 1st Qu.:0.58038   1st Qu.:0.9028   1st Qu.:0.88   1st Qu.:0.9295  \n Median :1.13801   Median :0.9028   Median :0.88   Median :0.9295  \n Mean   :1.13801   Mean   :0.9028   Mean   :0.88   Mean   :0.9295  \n 3rd Qu.:1.69563   3rd Qu.:0.9028   3rd Qu.:0.88   3rd Qu.:0.9295  \n Max.   :2.25325   Max.   :0.9028   Max.   :0.88   Max.   :0.9295  \n\n\nNow we can plot this:\n\n\nShow the code\nggplot(os_hr_est, aes(x = time, y = mean, ymin = lower, ymax = upper)) +\n    geom_line() +\n    geom_ribbon(alpha = 0.3)",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#alternative-models",
    "href": "session-os/1_os_weibull_jmpost.html#alternative-models",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Alternative models",
    "text": "Alternative models\nAbove we felt that maybe we could improve the fit of the model further.\nOne idea is to add also the direct effect of the treatment arm as a covariate:\n\n\nShow the code\nsurv_data2 &lt;- DataSurvival(\n    data = os_data_with_log_kg_est,\n    # Here we add the arm covariate:\n    formula = update(surv_data@formula, . ~ . + arm)\n)\nos_joint_data2 &lt;- DataJoint(\n    subject = subj_data,\n    survival = surv_data2\n)\nsave_file &lt;- here(\"session-os/os2.rds\")\nif (file.exists(save_file)) {\n    os_results2 &lt;- readRDS(save_file)\n} else {\n    os_results2 &lt;- sampleStanModel(\n        os_mod,\n        data = os_joint_data2,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(os_results2, file = save_file)\n}\nmcmc_os_results2 &lt;- cmdstanr::as.CmdStanMCMC(os_results2)\n\n\nWe can easily plot the survival functions and compare them with the Kaplan-Meier curves of the treatment arms, because we can reuse the above os_surv_group_grid:\n\n\nShow the code\nos_surv_pred2 &lt;- SurvivalQuantities(\n    object = os_results2,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\nautoplot(os_surv_pred2, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nHere we see a bit better “coverage” of the Docetaxel Kaplan-Meier curve by the confidence intervals of the model.\nWe could also consider the model with only the direct treatment arm effect, without the log growth rate estimates:\n\n\nShow the code\nsurv_data3 &lt;- DataSurvival(\n    data = os_data_with_log_kg_est,\n    # Here we add the arm covariate:\n    formula = update(surv_data2@formula, . ~ . - log_kg_est)\n)\nos_joint_data3 &lt;- DataJoint(\n    subject = subj_data,\n    survival = surv_data3\n)\nsave_file &lt;- here(\"session-os/os3.rds\")\nif (file.exists(save_file)) {\n    os_results3 &lt;- readRDS(save_file)\n} else {\n    os_results3 &lt;- sampleStanModel(\n        os_mod,\n        data = os_joint_data3,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(os_results3, file = save_file)\n}\nmcmc_os_results3 &lt;- cmdstanr::as.CmdStanMCMC(os_results3)\n\n\nLet’s plot again the survival functions:\n\n\nShow the code\nos_surv_pred3 &lt;- SurvivalQuantities(\n    object = os_results3,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\nautoplot(os_surv_pred3, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nThis looks very similar to the previous model.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#model-comparison",
    "href": "session-os/1_os_weibull_jmpost.html#model-comparison",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Model comparison",
    "text": "Model comparison\nFor comparing models, we can use more formal tools, as we will see now.\nWe can use the Brier score to compare two different survival models. The Brier score is a measure of the mean squared difference between the predicted survival probability and the actual survival status. The lower the Brier score, the better the model.\nTo calculate it, we need to use the GridFixed input for SurvivalQuantities():\n\n\nShow the code\nos_fixed_surv &lt;- SurvivalQuantities(\n    object = os_results,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_mod1_bs &lt;- brierScore(os_fixed_surv)\n\n\nWe can also look at the LOOIC. As for the TGI model, we can use the loo() method in the CmdStanMCMC object to calculate it:\n\n\nShow the code\nos_mod1_looic &lt;- mcmc_os_results$loo(r_eff = FALSE)\n\n\nAlso for the two alternative models we can calculate the Brier score and LOOIC in the same way:\n\n\nShow the code\nos_fixed_surv2 &lt;- SurvivalQuantities(\n    object = os_results2,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_mod2_bs &lt;- brierScore(os_fixed_surv2)\nos_mod2_looic &lt;- mcmc_os_results2$loo(r_eff = FALSE)\n\nos_fixed_surv3 &lt;- SurvivalQuantities(\n    object = os_results3,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_mod3_bs &lt;- brierScore(os_fixed_surv3)\nos_mod3_looic &lt;- mcmc_os_results3$loo(r_eff = FALSE)\n\n\nOf course in a real application with many models we can easily write a little function that does this for us repeatedly instead of just copy/pasting the code as we do here.\nNow we can compare the three models.\nLet’s start with the LOOIC:\n\n\nShow the code\nos_mod1_looic\n\n\n\nComputed from 1000 by 203 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -188.6  6.5\np_loo         8.6  0.8\nlooic       377.3 13.0\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nShow the code\nos_mod2_looic\n\n\n\nComputed from 1000 by 203 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -189.4  6.6\np_loo         9.9  0.8\nlooic       378.8 13.1\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nShow the code\nos_mod3_looic\n\n\n\nComputed from 1000 by 203 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -193.4  5.6\np_loo         8.7  0.6\nlooic       386.8 11.1\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nShow the code\nloo_compare(os_mod1_looic, os_mod2_looic, os_mod3_looic)\n\n\n       elpd_diff se_diff\nmodel1  0.0       0.0   \nmodel2 -0.7       1.3   \nmodel3 -4.8       4.2   \n\n\nSo we see that according to the LOOIC, the first model with just the log_kg_est covariate is slightly better than the model with the additional arm covariate. If we omit the log_kg_est covariate altogether and only include the arm covariate, then the model is worse than both of the other two models.\nWe can plot the Brier scores:\n\n\nShow the code\ndata.frame(\n    time = time_grid,\n    `1 - 2` = os_mod1_bs - os_mod2_bs,\n    `1 - 3` = os_mod1_bs - os_mod3_bs,\n    `2 - 3` = os_mod2_bs - os_mod3_bs,\n    check.names = FALSE\n) |&gt;\n    pivot_longer(\n        cols = c(\"1 - 2\", \"1 - 3\", \"2 - 3\"),\n        names_to = \"diff\",\n        values_to = \"brier_score_diff\"\n    ) |&gt;\n    ggplot(aes(x = time, y = brier_score_diff, color = diff, group = diff)) +\n    geom_line() +\n    labs(y = \"Brier score difference\") +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nWhen we look at the difference model 1 minus model 2, we see that this is slightly positive. Since lower numbers of the Brier score are better, this means that model 2 is slightly better here.\nWhen we look at the difference model 1 minus model 3, we see that this is more clearly negative between times 0.25 and 1.75, meaning that model 1 is clearly better than model 3 there. Towards later times this reverses, and model 3 is better than model 1.\nThe Brier score provides a more nuanced and time-dependent way of comparing the different OS models. Here we could either select model 1 due to its simplicity and almost same performance as model 2.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html",
    "href": "session-os/2_os_weibull_brms.html",
    "title": "2. OS model minimal workflow with brms",
    "section": "",
    "text": "Let’s try to fit the same model now with brms. We will use the same data as in the previous notebook.",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html#setup-and-load-data",
    "href": "session-os/2_os_weibull_brms.html#setup-and-load-data",
    "title": "2. OS model minimal workflow with brms",
    "section": "Setup and load data",
    "text": "Setup and load data\nHere we directly start from the overall survival data with the log kg estimates, as we have obtained them in the previous notebook:\n\n\nShow the code\nos_data_with_log_kg &lt;- readRDS(here(\"session-os/os_data_with_log_kg.rds\"))\nhead(os_data_with_log_kg)\n\n\n# A tibble: 6 × 9\n  id    arm       ecog    age race  sex   os_time os_event log_kg_est\n  &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;\n1 588   Docetaxel 0        61 WHITE F       2.05  FALSE       -0.571 \n2 330   MPDL3280A 1        56 WHITE F       1.68  FALSE       -1.89  \n3 791   Docetaxel 0        72 WHITE F       0.901 TRUE        -0.518 \n4 635   Docetaxel 0        42 OTHER F       1.66  TRUE        -0.642 \n5 365   MPDL3280A 0        64 WHITE F       1.43  TRUE        -0.427 \n6 773   Docetaxel 0        65 WHITE M       1.63  FALSE        0.0340",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html#model-fitting-with-brms",
    "href": "session-os/2_os_weibull_brms.html#model-fitting-with-brms",
    "title": "2. OS model minimal workflow with brms",
    "section": "Model fitting with brms",
    "text": "Model fitting with brms\nLet’s first fit the model with brms. We will use the same (first) model as in the previous notebook, but we will use the brms package to fit it.\nAn important ingredient for the model formula is the censoring information, passed via the cens() syntax: This should point to a variable containing the value 0 for observed events, i.e. no censoring, and the value 1 for right censored times (see ?brmsformula for more details). Therefore we first add such a variable to the data set:\n\n\nShow the code\nos_data_with_log_kg &lt;- os_data_with_log_kg |&gt;\n    mutate(\n        os_cens = ifelse(os_event, 0, 1)\n    )\n\n\nWe define our own design matrix with a column of ones:\n\n\nShow the code\nos_data_with_log_kg_design &lt;- model.matrix(\n    ~ os_time + os_cens + ecog + age + race + sex + log_kg_est,\n    data = os_data_with_log_kg\n) |&gt;\n    as.data.frame() |&gt;\n    rename(ones = \"(Intercept)\")\nhead(os_data_with_log_kg_design)\n\n\n  ones   os_time os_cens ecog1 age raceOTHER raceWHITE sexM  log_kg_est\n1    1 2.0506502       1     0  61         0         1    0 -0.57141474\n2    1 1.6755647       1     1  56         0         1    0 -1.88737445\n3    1 0.9007529       0     0  72         0         1    0 -0.51807234\n4    1 1.6591376       0     0  42         1         0    0 -0.64165688\n5    1 1.4291581       0     0  64         0         1    0 -0.42656542\n6    1 1.6290212       1     0  65         0         1    1  0.03404769\n\n\nNow we can define the model formula:\n\n\nShow the code\nformula &lt;- bf(\n    os_time | cens(os_cens) ~\n        0 +\n        ones +\n        ecog1 +\n        age +\n        raceOTHER +\n        raceWHITE +\n        sexM +\n        log_kg_est\n)\n\n\nSo here we suppress the automatic intercept provided by brms by using the 0 + syntax, and instead we our “own” vector of ones. This is because we want to avoid the default centering of covariates which is performed by brms when using an automatic intercept. Otherwise it would be difficult to exactly match the prior distributions we used in the jmpost model further below.\nIn order to find out about the parametrization of the Weibull model with brms and Stan here, let’s check the Stan code generated by brms for it:\n\n\nShow the code\nstancode(formula, data = os_data_with_log_kg_design, family = weibull())\n\n\n// generated with brms 2.22.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; shape;  // shape parameter\n}\ntransformed parameters {\n  real lprior = 0;  // prior contributions to the log posterior\n  lprior += gamma_lpdf(shape | 0.01, 0.01);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += X * b;\n    mu = exp(mu);\n    // vectorized log-likelihood contributions of censored data\n    target += weibull_lpdf(Y[Jevent[1:Nevent]] | shape, mu[Jevent[1:Nevent]] / tgamma(1 + 1 / shape));\n    target += weibull_lccdf(Y[Jrcens[1:Nrcens]] | shape, mu[Jrcens[1:Nrcens]] / tgamma(1 + 1 / shape));\n    target += weibull_lcdf(Y[Jlcens[1:Nlcens]] | shape, mu[Jlcens[1:Nlcens]] / tgamma(1 + 1 / shape));\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\nWe can see that the Stan code uses the weibull_* function family to define the log-likelihood contributions.\nWe can check the Stan reference doc here for details of the parametrization. We can see that this is the so-called “standard” parametrization (see Wikipedia) with shape parameter \\(\\alpha\\) and scale parameter \\(\\sigma\\). The mean of this distribution is \\(\\sigma \\Gamma(1 + 1/\\alpha)\\), where \\(\\Gamma\\) is the gamma function. We can see in the brms generated code that accordingly the sigma parameter is defined as mu / tgamma(1 + 1 / shape), such that mu is really the mean of the distribution.\nNow the problem is that this is a different parametrization than what we have used in jmpost (see the specification), which is the proportional hazards parametrization (see Wikipedia), where the covariate effects are on the log hazard scale instead of on the log mean scale. This has been identified by other brms users as a gap in the package, see e.g. here. So we can hope that this will be added in the future, but for now we need to implement a workaround.\nFortunately, we can define a custom distribution in brms to use the proportional hazards parametrization. This parametrization relates to the Stan Weibull density definition with the transformation of \\(\\sigma := \\gamma^{-1 / \\alpha}\\). The code here has been first written by Bjoern Holzhauer and was extended by Sebastian Weber to integrate more tightly with brms (source). One thing to keep in mind here is that for technical reasons the first parameter of the custom distribution needs to be named mu and not gamma.\n\n\nShow the code\nfamily_weibull_ph &lt;- function(link_gamma = \"log\", link_alpha = \"log\") {\n    brms::custom_family(\n        name = \"weibull_ph\",\n        # first param needs to be \"mu\" cannot be \"gamma\"; alpha is the shape:\n        dpars = c(\"mu\", \"alpha\"),\n        links = c(link_gamma, link_alpha),\n        lb = c(0, 0),\n        # ub = c(NA, NA), # would be redundant\n        # no need for `vars` like for `cens`, brms can handle this.\n        type = \"real\",\n        loop = TRUE\n    )\n}\n\nsv_weibull_ph &lt;- brms::stanvar(\n    name = \"weibull_ph_stan_code\",\n    scode = \"\nreal weibull_ph_lpdf(real y, real mu, real alpha) {\n  // real sigma = pow(1 / mu, 1 / alpha);\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lpdf(y | alpha, sigma);\n}\nreal weibull_ph_lccdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lccdf(y | alpha, sigma);\n}\nreal weibull_ph_lcdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lcdf(y | alpha, sigma);\n}\nreal weibull_ph_rng(real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_rng(alpha, sigma);\n}\n\",\n    block = \"functions\"\n)\n\n## R definitions of auxilary helper functions of brms, these are based\n## on the respective weibull (internal) brms implementations:\n\nlog_lik_weibull_ph &lt;- function(i, prep) {\n    shape &lt;- get_dpar(prep, \"alpha\", i = i)\n    sigma &lt;- get_dpar(prep, \"mu\", i = i)^(-1 / shape)\n    args &lt;- list(shape = shape, scale = sigma)\n    out &lt;- brms:::log_lik_censor(\n        dist = \"weibull\",\n        args = args,\n        i = i,\n        prep = prep\n    )\n    out &lt;- brms:::log_lik_truncate(\n        out,\n        cdf = pweibull,\n        args = args,\n        i = i,\n        prep = prep\n    )\n    brms:::log_lik_weight(out, i = i, prep = prep)\n}\n\nposterior_predict_weibull_ph &lt;- function(i, prep, ntrys = 5, ...) {\n    shape &lt;- get_dpar(prep, \"alpha\", i = i)\n    sigma &lt;- get_dpar(prep, \"mu\", i = i)^(-1 / shape)\n    brms:::rcontinuous(\n        n = prep$ndraws,\n        dist = \"weibull\",\n        shape = shape,\n        scale = sigma,\n        lb = prep$data$lb[i],\n        ub = prep$data$ub[i],\n        ntrys = ntrys\n    )\n}\n\nposterior_epred_weibull_ph &lt;- function(prep) {\n    shape &lt;- get_dpar(prep, \"alpha\")\n    sigma &lt;- get_dpar(prep, \"mu\")^(-1 / shape)\n    sigma * gamma(1 + 1 / shape)\n}\n\n\nWe can again check the Stan code that is generated for this custom distribution:\n\n\nShow the code\nstancode(\n    formula,\n    data = os_data_with_log_kg_design,\n    stanvars = sv_weibull_ph, # We pass the custom Stan functions' code here.\n    family = family_weibull_ph()\n)\n\n\n// generated with brms 2.22.0\nfunctions {\n  \nreal weibull_ph_lpdf(real y, real mu, real alpha) {\n  // real sigma = pow(1 / mu, 1 / alpha);\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lpdf(y | alpha, sigma);\n}\nreal weibull_ph_lccdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lccdf(y | alpha, sigma);\n}\nreal weibull_ph_lcdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lcdf(y | alpha, sigma);\n}\nreal weibull_ph_rng(real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_rng(alpha, sigma);\n}\n\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; alpha;  // skewness parameter\n}\ntransformed parameters {\n  real lprior = 0;  // prior contributions to the log posterior\n  lprior += normal_lpdf(alpha | 0, 4)\n    - 1 * normal_lccdf(0 | 0, 4);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += X * b;\n    mu = exp(mu);\n    for (n in 1:N) {\n      // special treatment of censored data\n      if (cens[n] == 0) {\n        target += weibull_ph_lpdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == 1) {\n        target += weibull_ph_lccdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == -1) {\n        target += weibull_ph_lcdf(Y[n] | mu[n], alpha);\n      }\n    }\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\nIndeed we can now use the custom distribution. We also see the default priors in the transformed parameters block on the shape parameter (alpha). We don’t see an explicit prior on the regression coefficients (b), which means an improper flat prior is used by default.\nThe remaining challenge is that in jmpost we specified a Gamma prior for \\(\\lambda\\) which is now here the exponentiated intercept parameter. So in principle, we would need an ExpGamma prior on the intercept, meaning that if we exponentiate the intercept, it has a gamma distribution. However, this would again require a custom distribution. Let’s try to go with an approximation: we can just draw samples from the ExpGamma distribution (by sampling from a gamma distribution and taking the log) and then approximate this with a skewed normal distribution (see here for the Stan documentation):\n\n\nShow the code\nset.seed(123)\nintercept_samples &lt;- log(rgamma(1000, 0.7, 1))\n\nlibrary(sn)\nfit &lt;- selm(intercept_samples ~ 1, family = \"SN\")\nxi &lt;- coef(fit, \"DP\")[1]\nomega &lt;- coef(fit, \"DP\")[2]\nalpha &lt;- coef(fit, \"DP\")[3]\n\nhist(intercept_samples, probability = TRUE)\ncurve(dsn(x, xi, omega, alpha), add = TRUE, col = \"red\", lwd = 3)\n\n\n\n\n\n\n\n\n\nThe skew normal density curve approximates the histogram of the log gamma samples well.\nNow we can finally specify the priors:\n\n\nShow the code\npriors &lt;- c(\n    set_prior(\n        glue::glue(\"skew_normal({xi}, {omega}, {alpha})\"),\n        class = \"b\",\n        coef = \"ones\"\n    ),\n    prior(normal(0, 20), class = \"b\"),\n    prior(gamma(0.7, 1), class = \"alpha\")\n)\n\n\nLet’s do a final check of the Stan code:\n\n\nShow the code\nstancode(\n    formula,\n    data = os_data_with_log_kg_design,\n    prior = priors,\n    stanvars = sv_weibull_ph,\n    family = family_weibull_ph()\n)\n\n\n// generated with brms 2.22.0\nfunctions {\n  \nreal weibull_ph_lpdf(real y, real mu, real alpha) {\n  // real sigma = pow(1 / mu, 1 / alpha);\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lpdf(y | alpha, sigma);\n}\nreal weibull_ph_lccdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lccdf(y | alpha, sigma);\n}\nreal weibull_ph_lcdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lcdf(y | alpha, sigma);\n}\nreal weibull_ph_rng(real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_rng(alpha, sigma);\n}\n\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; alpha;  // skewness parameter\n}\ntransformed parameters {\n  real lprior = 0;  // prior contributions to the log posterior\n  lprior += skew_normal_lpdf(b[1] | 0.758392879263355, 2.57341668661882, -4.8061893638208);\n  lprior += normal_lpdf(b[2] | 0, 20);\n  lprior += normal_lpdf(b[3] | 0, 20);\n  lprior += normal_lpdf(b[4] | 0, 20);\n  lprior += normal_lpdf(b[5] | 0, 20);\n  lprior += normal_lpdf(b[6] | 0, 20);\n  lprior += normal_lpdf(b[7] | 0, 20);\n  lprior += gamma_lpdf(alpha | 0.7, 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += X * b;\n    mu = exp(mu);\n    for (n in 1:N) {\n      // special treatment of censored data\n      if (cens[n] == 0) {\n        target += weibull_ph_lpdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == 1) {\n        target += weibull_ph_lccdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == -1) {\n        target += weibull_ph_lcdf(Y[n] | mu[n], alpha);\n      }\n    }\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-os/brms1.rds\")\nif (file.exists(save_file)) {\n    fit &lt;- readRDS(save_file)\n} else {\n    fit &lt;- brm(\n        formula = formula,\n        data = os_data_with_log_kg_design,\n        prior = priors,\n        stanvars = sv_weibull_ph,\n        family = family_weibull_ph(),\n        chains = CHAINS,\n        iter = ITER + WARMUP,\n        warmup = WARMUP,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveRDS(fit, save_file)\n}\n\nsummary(fit)\n\n\n Family: weibull_ph \n  Links: mu = log; alpha = identity \nFormula: os_time | cens(os_cens) ~ 0 + ones + ecog1 + age + raceOTHER + raceWHITE + sexM + log_kg_est \n   Data: os_data_with_log_kg_design (Number of observations: 203) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nones          -1.59      0.63    -2.85    -0.38 1.00     1935     2531\necog1          0.71      0.21     0.29     1.13 1.00     3317     2840\nage            0.00      0.01    -0.01     0.02 1.00     1953     2532\nraceOTHER      0.54      0.40    -0.30     1.27 1.00     3071     2139\nraceWHITE     -0.03      0.23    -0.46     0.42 1.00     3052     2735\nsexM           0.31      0.20    -0.07     0.69 1.00     3740     2938\nlog_kg_est     0.59      0.17     0.25     0.94 1.00     3821     3144\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha     1.68      0.14     1.42     1.96 1.00     4072     2992\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSo the model converged fast and well.",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html#comparison-of-results",
    "href": "session-os/2_os_weibull_brms.html#comparison-of-results",
    "title": "2. OS model minimal workflow with brms",
    "section": "Comparison of results",
    "text": "Comparison of results\nLet’s compare the results of the brms model with the jmpost model.\nFirst we load again the jmpost results:\n\n\nShow the code\ndraws_jmpost &lt;- readRDS(here(\"session-os/os_draws.rds\")) |&gt;\n    rename_variables(\n        \"gamma\" = \"sm_weibull_ph_gamma\",\n        \"lambda\" = \"sm_weibull_ph_lambda\"\n    )\nsummary(draws_jmpost)\n\n\n# A tibble: 8 × 10\n  variable       mean   median      sd     mad      q5    q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 ecog1       0.724    0.728   0.207   0.214    0.386  1.06   1.00      957.\n2 age         0.00445  0.00463 0.00920 0.00937 -0.0101 0.0194 0.999    1043.\n3 raceOTHER   0.542    0.542   0.400   0.419   -0.124  1.17   0.998     942.\n4 raceWHITE  -0.0268  -0.0307  0.221   0.221   -0.373  0.333  1.00     1046.\n5 sexM        0.302    0.303   0.201   0.198   -0.0342 0.630  1.00      718.\n6 log_kg_est  0.597    0.587   0.172   0.170    0.321  0.880  1.00      742.\n7 gamma       1.69     1.68    0.135   0.131    1.47   1.92   1.00      933.\n8 lambda      0.248    0.208   0.164   0.115    0.0720 0.556  0.999     983.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nWe prepare above brms results in the same format:\n\n\nShow the code\ndraws_brms &lt;- as_draws_array(fit) |&gt;\n    mutate_variables(lambda = exp(b_ones)) |&gt;\n    rename_variables(\n        \"gamma\" = \"alpha\",\n        \"ecog1\" = \"b_ecog1\",\n        \"age\" = \"b_age\",\n        \"raceOTHER\" = \"b_raceOTHER\",\n        \"raceWHITE\" = \"b_raceWHITE\",\n        \"sexM\" = \"b_sexM\",\n        \"log_kg_est\" = \"b_log_kg_est\"\n    ) |&gt;\n    subset_draws(\n        variable = c(\n            \"ecog1\",\n            \"age\",\n            \"raceOTHER\",\n            \"raceWHITE\",\n            \"sexM\",\n            \"log_kg_est\",\n            \"gamma\",\n            \"lambda\"\n        )\n    )\nsummary(draws_brms)\n\n\n# A tibble: 8 × 10\n  variable       mean   median      sd     mad      q5    q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 ecog1       0.713    0.713   0.215   0.217    0.362  1.06    1.00    3317.\n2 age         0.00459  0.00455 0.00964 0.00952 -0.0112 0.0205  1.00    1953.\n3 raceOTHER   0.537    0.550   0.400   0.394   -0.155  1.16    1.00    3071.\n4 raceWHITE  -0.0259  -0.0250  0.226   0.228   -0.389  0.353   1.00    3052.\n5 sexM        0.312    0.317   0.199   0.200   -0.0149 0.631   1.00    3740.\n6 log_kg_est  0.589    0.593   0.174   0.172    0.310  0.874   1.00    3821.\n7 gamma       1.68     1.67    0.140   0.139    1.45   1.91    1.00    4072.\n8 lambda      0.249    0.206   0.170   0.125    0.0719 0.565   1.00    1935.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nSo the results agree well. We can also see this in density plots:\n\n\nShow the code\n# Combine the draws into one data frame\ndraws_combined &lt;- bind_rows(\n    mutate(as_draws_df(draws_jmpost), source = \"jmpost\"),\n    mutate(as_draws_df(draws_brms), source = \"brms\")\n) |&gt;\n    select(-.chain, -.iteration, -.draw)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\n# Convert to long format for ggplot2\ndraws_long &lt;- pivot_longer(\n    draws_combined,\n    cols = -source,\n    names_to = \"parameter\",\n    values_to = \"value\"\n)\n\n# Plot the densities\nggplot(draws_long, aes(x = value, fill = source)) +\n    geom_density(alpha = 0.5) +\n    facet_wrap(~parameter, scales = \"free\") +\n    theme_minimal() +\n    labs(\n        title = \"Posterior Parameter Samples Comparison\",\n        x = \"Value\",\n        y = \"Density\"\n    )\n\n\n\n\n\n\n\n\n\nGenerally these agree very well with each other. Overall we can expect a slight difference between the two results, because for the \\(\\lambda\\) parameter we only approximately used the same prior distribution in brms compared to jmpost.",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html",
    "href": "session-tgi/1_tgi_sf_brms.html",
    "title": "1. TGI model minimal workflow with brms",
    "section": "",
    "text": "The purpose of this document is to show a minimal workflow for fitting a TGI model using the brms package.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#setup-and-load-data",
    "href": "session-tgi/1_tgi_sf_brms.html#setup-and-load-data",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#stein-fojo-model",
    "href": "session-tgi/1_tgi_sf_brms.html#stein-fojo-model",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Stein-Fojo model",
    "text": "Stein-Fojo model\nWe start from the Stein-Fojo model as shown in the slides:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\{\\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) + \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) - 1\\}\n\\]\n\nMean\nWe will make one more tweak here. If the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\{\\exp(-\\psi_{k_{s}i} \\cdot t_{ij}) + \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) - 1\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\n\n\nLikelihood\nFor the likelihood given the mean SLD \\(y^{*}\\), we will assume a normal distribution with a constant coefficient of variation \\(\\tau\\):\n\\[\ny(t_{ij}) \\sim \\text{N}(y^{*}(t_{ij}), y^{*}(t_{ij})\\tau)\n\\]\nNote that for consistency with the brms and Stan convention, here we denote the standard deviation as the second parameter of the normal distribution. So in this case, the variance would be \\((y^{*}(t_{ij})\\tau)^2\\).\nThis can also be written as:\n\\[\ny(t_{ij}) = (1 + \\epsilon_{ij}) \\cdot y^{*}(t_{ij})\n\\]\nwhere \\(\\epsilon_{ij} \\sim \\text{N}(0, \\tau)\\).\nNote that also the additive model is a possible choice, where\n\\[\ny(t_{ij}) = y^{*}(t_{ij}) + \\epsilon_{ij}\n\\]\nsuch that the error does not depend on the scale of the SLD any longer.\n\n\nRandom effects\nNext, we define the distributions of the random effects \\(\\psi_{b_{0}i}\\), \\(\\psi_{k_{s}i}\\), \\(\\psi_{k_{g}i}\\), for \\(i = 1, \\dotsc, n\\):\n\\[\n\\begin{align*}\n\\psi_{b_{0}i} &\\sim \\text{LogNormal}(\\mu_{b_{0}}, \\omega_{0}) \\\\\n\\psi_{k_{s}i} &\\sim \\text{LogNormal}(\\mu_{k_{s}}, \\omega_{s}) \\\\\n\\psi_{k_{g}i} &\\sim \\text{LogNormal}(\\mu_{k_{g}}, \\omega_{g})\n\\end{align*}\n\\]\nThis can be rewritten as:\n\\[\n\\begin{align*}\n\\psi_{b_{0}i} &= \\exp(\\mu_{b_{0}} + \\omega_{0} \\cdot \\eta_{b_{0}i}) \\\\\n\\psi_{k_{s}i} &= \\exp(\\mu_{k_{s}} + \\omega_{s} \\cdot \\eta_{k_{s}i}) \\\\\n\\psi_{k_{g}i} &= \\exp(\\mu_{k_{g}} + \\omega_{g} \\cdot \\eta_{k_{g}i})\n\\end{align*}\n\\]\nwhere \\(\\eta_{b_{0}i}\\), \\(\\eta_{k_{s}i}\\), \\(\\eta_{k_{g}i}\\) are the standard normal distributed random effects.\nThis is important for two reasons:\n\nThis parametrization can help the sampler to converge faster. See here for more information.\nThis shows a bit more explicitly that the population mean of the random effects is not equal to the \\(\\mu\\) parameter, but to \\(\\theta = \\exp(\\mu + \\omega^2 / 2)\\), because they are log-normally distributed (see e.g. Wikipedia for the formulas). This is important for the interpretation of the parameter estimates.\n\n\n\nPriors\nFinally, we need to define the priors for the hyperparameters.\nThere are different principles we could use to define these priors:\n\nNon-informative priors: We could use priors that are as non-informative as possible. This is especially useful if we do not have any prior knowledge about the parameters. For example, we could use normal priors with a large standard deviation for the population means of the random effects.\nInformative priors: If we have some prior knowledge about the parameters, we can use this to define the priors. For example, if we have literature data about the \\(k_g\\) parameter estimate, we could use this to define the prior for the population mean of the growth rate. (Here we just need to be careful to consider the time scale and the log-normal distribution, as mentioned above)\n\nHere we use relatively informative priors for the log-normal location parameters, motivated by prior analyses of the same study:\n\\[\n\\begin{align*}\n\\mu_{b_{0}} &\\sim \\text{Normal}(\\log(65), 1) \\\\\n\\mu_{k_{s}} &\\sim \\text{Normal}(\\log(0.52), 0.1) \\\\\n\\mu_{k_{g}} &\\sim \\text{Normal}(\\log(1.04), 1)\n\\end{align*}\n\\]\nFor all standard deviations we use truncated normal priors:\n\\[\n\\begin{align*}\n\\omega_{0} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\omega_{s} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\omega_{g} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\tau &\\sim \\text{PositiveNormal}(0, 3)\n\\end{align*}\n\\]\nwhere \\(\\text{PositiveNormal}(0, 3)\\) denotes a truncated normal distribution with mean \\(0\\) and standard deviation \\(3\\), truncated to the positive real numbers.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#fit-model",
    "href": "session-tgi/1_tgi_sf_brms.html#fit-model",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood:\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (exp(-ks * year) + exp(kg * year) -  1)) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # Define the standard deviation (called sigma in brms) as a \n  # coefficient tau times the mean ystar.\n  # sigma is modelled on the log scale though, therefore:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  # This line is needed to declare tau as a model parameter:\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + (1 | id)) +\n  lf(lkg ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\"),\n  prior(normal(log(1.04), 1), nlpar = \"lkg\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lks = array(-1.25),\n  b_lkg = array(-1.33),\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/fit9.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsummary(fit)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.00      689     1248\nsd(lks_Intercept)     1.40      0.08     1.25     1.55 1.00      702     1586\nsd(lkg_Intercept)     0.96      0.04     0.88     1.05 1.01      902     1750\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.16      0.00     0.16     0.17 1.00     2250     2892\nlb0_Intercept     3.62      0.02     3.57     3.66 1.01      420      723\nlks_Intercept    -0.86      0.08    -1.01    -0.72 1.00     1661     2415\nlkg_Intercept    -1.20      0.07    -1.34    -1.08 1.01      676     1419\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote that it is crucial to use here int_step() to properly define the two pieces of the linear predictor for negative and non-negative time values: If you used step() like I did for a few days, then you will have the wrong model! This is because in Stan, step(false) = step(0) = 1 and not 0 as you would expect. Only int_step(0) = 0 as we need it here.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#parameter-estimates",
    "href": "session-tgi/1_tgi_sf_brms.html#parameter-estimates",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nHere we extract all parameter estimates using the as_draws_df method for brmsfit objects. Note that we could also just extract a subset of parameters, see ?as_draws.brmsfit for more information.\nAs mentioned above, we use the expectation of the log-normal distribution to get the population level estimates (called \\(\\theta\\) with the corresponding subscript) for the \\(b_0\\), \\(k_s\\), and \\(k_g\\) parameters. We also calculate the coefficient of variation for each parameter.\n\n\nShow the code\npost_df &lt;- as_draws_df(fit)\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"r_id__lb0[1,Intercept]\" \"r_id__lb0[2,Intercept]\"\n[10] \"r_id__lb0[3,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nFor a graphical check of the convergence, we can use the mcmc_trace and mcmc_pairs functions from the bayesplot package:\n\n\nShow the code\nsf_pop_params &lt;- c(\"theta_b0\", \"theta_ks\", \"theta_kg\", \"omega_0\", \"omega_s\", \"omega_g\", \"sigma\")\n\nmcmc_trace(post_df, pars = sf_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = sf_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nNext, we can create a nice summary table of the parameter estimates, using summarize_draws from the posterior package in combination with functions from the gt package:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  select(theta_b0, theta_ks, theta_kg, omega_0, omega_s, omega_g, cv_0, cv_s, cv_g, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.0\n44.0\n1.03\n1.01\n42.3\n45.7\n1.00\n423\n726\n\n\ntheta_ks\n1.13\n1.12\n0.104\n0.101\n0.972\n1.32\n1.00\n445\n915\n\n\ntheta_kg\n0.476\n0.475\n0.0311\n0.0318\n0.426\n0.528\n1.00\n941\n1,320\n\n\nomega_0\n0.579\n0.579\n0.0162\n0.0166\n0.552\n0.605\n1.00\n676\n1,220\n\n\nomega_s\n1.40\n1.40\n0.0753\n0.0745\n1.28\n1.52\n1.00\n699\n1,560\n\n\nomega_g\n0.958\n0.956\n0.0446\n0.0446\n0.888\n1.04\n1.00\n882\n1,720\n\n\ncv_0\n0.631\n0.631\n0.0208\n0.0212\n0.597\n0.665\n1.00\n676\n1,220\n\n\ncv_s\n2.49\n2.46\n0.311\n0.295\n2.02\n3.04\n1.00\n699\n1,560\n\n\ncv_g\n1.23\n1.22\n0.0883\n0.0869\n1.10\n1.39\n1.00\n882\n1,720\n\n\nsigma\n0.161\n0.161\n0.00230\n0.00229\n0.157\n0.165\n1.00\n2,190\n2,860\n\n\n\n\n\n\n\nWe can also look at parameters for individual patients. This is simplified by using the spread_draws() function:\n\n\nShow the code\n# Understand the names of the random effects:\nhead(get_variables(fit), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"r_id__lb0[1,Intercept]\" \"r_id__lb0[2,Intercept]\"\n[10] \"r_id__lb0[3,Intercept]\"\n\n\nShow the code\npost_ind &lt;- fit |&gt; \n  spread_draws(\n    b_lb0_Intercept,\n    r_id__lb0[id, ],\n    b_lks_Intercept,\n    r_id__lks[id, ],\n    b_lkg_Intercept,\n    r_id__lkg[id, ]\n  ) |&gt; \n  mutate(\n    b0 = exp(b_lb0_Intercept + r_id__lb0),\n    ks = exp(b_lks_Intercept + r_id__lks),\n    kg = exp(b_lkg_Intercept + r_id__lkg)\n  ) |&gt; \n  select(\n    .chain, .iteration, .draw,\n    id, b0, ks, kg    \n  )\n\n\nNote that here we do not need to use the log-normal expectation formula, because we just calculate the individual random effects here, in contrast to the population level parameters above.\nWith this we can e.g. report the estimates for the first patient:\n\n\nShow the code\npost_sum_id1 &lt;- post_ind |&gt;\n  filter(id == \"1\") |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_sum_id1\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\n1\n\n\nb0\n43.3\n42.8\n4.80\n4.35\n36.5\n52.2\n1.00\n9,080\n2,890\n\n\nks\n0.486\n0.313\n0.529\n0.309\n0.0392\n1.50\n1.00\n6,300\n3,140\n\n\nkg\n0.395\n0.294\n0.338\n0.241\n0.0620\n1.04\n1.00\n7,690\n3,080",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#observation-vs-model-fit",
    "href": "session-tgi/1_tgi_sf_brms.html#observation-vs-model-fit",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again.\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_linpred_draws(fit) |&gt; \n  median_qi() |&gt; \n  mutate(sld_pred = .linpred)\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = sld_pred, ymin = .lower, ymax = .upper),\n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = sld_pred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"SF model fit\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#summary-statistics",
    "href": "session-tgi/1_tgi_sf_brms.html#summary-statistics",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Summary statistics",
    "text": "Summary statistics\nThe time-to-growth formula is:\n\\[\n\\max \\left(\n    \\frac{\n        \\log(k_s) - \\log(k_g)\n    }{\n        k_s + k_g\n    },\n    0\n\\right)\n\\]\nSimilary, we can look at the tumor-ratio at time \\(t\\):\n\\[\n\\frac{y^{*}(t)}{y^{*}(0)} = \\exp(-k_s \\cdot t) + \\exp(k_g \\cdot t) - 1\n\\]\nSo with the posterior samples from above, we can calculate these statistics, separate for each patient, with the tumor-ratio e.g. for 12 weeks (and being careful with the year time scale we used in this model):\n\n\nShow the code\npost_ind_stat &lt;- post_ind |&gt; \n  mutate(\n    ttg = pmax((log(ks) - log(kg)) / (ks + kg), 0),\n    tr12 = exp(-ks * 12/52) + exp(kg * 12/52) - 1\n  )\n\n\nThen we can look at e.g. the first 3 patients parameter estimates:\n\n\nShow the code\npost_ind_stat_sum &lt;- post_ind_stat |&gt;\n  filter(id %in% c(\"1\", \"2\", \"3\")) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_ind_stat_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\n1\n\n\nb0\n43.3\n42.8\n4.80\n4.35\n36.5\n52.2\n1.00\n9,080\n2,890\n\n\nks\n0.486\n0.313\n0.529\n0.309\n0.0392\n1.50\n1.00\n6,300\n3,140\n\n\nkg\n0.395\n0.294\n0.338\n0.241\n0.0620\n1.04\n1.00\n7,690\n3,080\n\n\nttg\n0.785\n0.106\n1.36\n0.158\n0\n3.47\n1.00\n4,440\n3,180\n\n\ntr12\n0.999\n1.00\n0.115\n0.0834\n0.809\n1.18\n1.00\n5,740\n3,400\n\n\n2\n\n\nb0\n37.9\n37.6\n4.10\n3.79\n31.9\n45.1\n1.00\n7,520\n2,750\n\n\nks\n0.396\n0.246\n0.491\n0.237\n0.0355\n1.21\n1.00\n5,330\n3,380\n\n\nkg\n0.548\n0.416\n0.445\n0.357\n0.0735\n1.42\n1.00\n5,860\n2,870\n\n\nttg\n0.501\n0\n1.21\n0\n0\n2.92\n0.999\n3,500\n2,820\n\n\ntr12\n1.06\n1.04\n0.125\n0.0939\n0.886\n1.29\n1.00\n5,810\n3,220\n\n\n3\n\n\nb0\n21.0\n20.7\n2.49\n2.23\n17.6\n25.6\n1.00\n4,310\n2,900\n\n\nks\n1.00\n0.817\n0.662\n0.541\n0.229\n2.34\n1.00\n2,930\n2,760\n\n\nkg\n0.275\n0.249\n0.165\n0.185\n0.0529\n0.575\n1.00\n3,480\n2,790\n\n\nttg\n1.50\n1.01\n1.39\n0.681\n0.418\n4.27\n1.00\n4,440\n2,740\n\n\ntr12\n0.869\n0.884\n0.0815\n0.0720\n0.709\n0.980\n1.00\n3,170\n2,920",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#adding-covariates",
    "href": "session-tgi/1_tgi_sf_brms.html#adding-covariates",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Adding covariates",
    "text": "Adding covariates\nWith brms it is straightforward to add covariates to the model. In this example, an obvious choice for a covariate is the treatment the patients received in the study:\n\n\nShow the code\ndf |&gt; \n  select(id, arm) |&gt; \n  distinct() |&gt;\n  pull(arm) |&gt; \n  table()\n\n\n\n  1   2 \n325 376 \n\n\nHere it makes sense to assume that only the shrinkage and growth parameters differ systematically between the two arms. For example, the baseline SLD should be the same in expectation, because of the randomization between the treatment arms.\nTherefore we can add this binary arm covariate as follows as a fixed effect for the two population level mean parameters \\(\\mu_{k_s}\\) and \\(\\mu_{k_g}\\) on the log scale:\n\n\nShow the code\nformula_by_arm &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood:\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (exp(-ks * year) + exp(kg * year) -  1)) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # As above we also use these formulas here:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + arm + (1 | id)) +\n  lf(lkg ~ 1 + arm + (1 | id))\n\n\nIt is instructive to see that the Stan code that is generated in the background is actually identical to the previous model, except for the prior specification. This is because brms uses a design matrix to model the fixed effects, and the arm variable is just added to this design matrix, which before only contained the intercept column with 1s.\nHowever, now the coefficient vector is no longer of length 1 but of length 2, with the first element corresponding to the intercept and the second element to the effect of the arm variable (for the level “2”). For the latter, we want to assume a standard normal prior on the log scale.\nSo we need to adjust the prior and initial values accordingly:\n\n\nShow the code\npriors_by_arm &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  # Note the changes here:\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\", coef = \"Intercept\"),\n  prior(normal(0, 1), nlpar = \"lks\", coef = \"arm2\"), \n  prior(normal(log(1.04), 1), nlpar = \"lkg\", coef = \"Intercept\"),\n  prior(normal(0, 1), nlpar = \"lkg\", coef = \"arm2\"),\n  # Same as before:\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\ninits_by_arm &lt;- list(\n  b_lb0 = array(3.61),\n  # Note the changes here:\n  b_lks = array(c(-1.25, 0)),\n  b_lkg = array(c(-1.33, 0)),\n  # Same as before:\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n\nNow we can fit the model as before:\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/fit10.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit_by_arm &lt;- brm(\n    formula = formula_by_arm,\n    data = df,\n    prior = priors_by_arm,\n    family = gaussian(),\n    init = rep(list(inits_by_arm), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit_by_arm, file = save_file)\n}\nsummary(fit_by_arm)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + arm + (1 | id)\n         lkg ~ 1 + arm + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.01      356      987\nsd(lks_Intercept)     1.48      0.09     1.31     1.66 1.01      402      917\nsd(lkg_Intercept)     0.97      0.05     0.88     1.07 1.01      323      789\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.16      0.00     0.16     0.17 1.00     1725     2755\nlb0_Intercept     3.61      0.02     3.57     3.66 1.00      199      519\nlks_Intercept    -0.78      0.09    -0.95    -0.61 1.00     1204     2303\nlks_arm2         -0.40      0.16    -0.73    -0.09 1.01      383      823\nlkg_Intercept    -1.26      0.11    -1.48    -1.05 1.01      494      853\nlkg_arm2          0.03      0.13    -0.22     0.28 1.01      460      975\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s have a look at the parameter estimates then:\n\n\nShow the code\npost_df_by_arm &lt;- as_draws_df(fit_by_arm)\nhead(names(post_df_by_arm), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lks_arm2\"             \"b_lkg_Intercept\"        \"b_lkg_arm2\"            \n [7] \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"   \"sd_id__lkg_Intercept\"  \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df_by_arm &lt;- post_df_by_arm |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks_arm1 = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_ks_arm2 = exp(b_lks_Intercept + b_lks_arm2 + sd_id__lks_Intercept^2 / 2),\n    theta_kg_arm1 = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_kg_arm2 = exp(b_lkg_Intercept + b_lkg_arm2 + sd_id__lkg_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\npost_sum_by_arm &lt;- post_df_by_arm |&gt;\n  select(\n    theta_b0, theta_ks_arm1, theta_ks_arm2, theta_kg_arm1, theta_kg_arm2, \n    omega_0, omega_s, omega_g, cv_0, cv_s, cv_g, sigma\n  ) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum_by_arm\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.9\n43.9\n1.10\n1.11\n42.1\n45.8\n1.00\n195\n515\n\n\ntheta_ks_arm1\n1.39\n1.36\n0.193\n0.178\n1.11\n1.74\n1.01\n396\n841\n\n\ntheta_ks_arm2\n0.925\n0.918\n0.112\n0.108\n0.755\n1.12\n1.00\n306\n814\n\n\ntheta_kg_arm1\n0.456\n0.454\n0.0461\n0.0456\n0.382\n0.535\n1.01\n575\n1,000\n\n\ntheta_kg_arm2\n0.469\n0.468\n0.0425\n0.0423\n0.402\n0.544\n1.00\n496\n923\n\n\nomega_0\n0.578\n0.578\n0.0161\n0.0160\n0.552\n0.605\n1.00\n345\n979\n\n\nomega_s\n1.48\n1.48\n0.0909\n0.0899\n1.33\n1.63\n1.01\n397\n900\n\n\nomega_g\n0.968\n0.965\n0.0485\n0.0474\n0.892\n1.05\n1.00\n343\n808\n\n\ncv_0\n0.630\n0.630\n0.0206\n0.0205\n0.597\n0.665\n1.00\n345\n979\n\n\ncv_s\n2.84\n2.79\n0.445\n0.416\n2.21\n3.65\n1.01\n397\n900\n\n\ncv_g\n1.25\n1.24\n0.0976\n0.0929\n1.10\n1.43\n1.00\n343\n808\n\n\nsigma\n0.161\n0.161\n0.00229\n0.00230\n0.157\n0.165\n1.00\n1,700\n2,710\n\n\n\n\n\n\n\nSo we see that the shrinkage is stronger in arm 1 compared to arm 2, while the growth rates are similar. We could also calculate the posterior probability that the growth rate is different between the two arms, for example:\n\n\nShow the code\npost_df_by_arm |&gt;\n  mutate(diff_pos = theta_ks_arm1 - theta_ks_arm2 &gt; 0) |&gt;\n  summarize(diff_prob = mean(diff_pos))\n\n\n# A tibble: 1 × 1\n  diff_prob\n      &lt;dbl&gt;\n1     0.996\n\n\nSo we see that the posterior probability that the shrinkage rate is higher in arm 1 compared to arm 2 is quite high.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#model-comparison-with-loo",
    "href": "session-tgi/1_tgi_sf_brms.html#model-comparison-with-loo",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Model comparison with LOO",
    "text": "Model comparison with LOO\nThe LOO criterion is a widely used method for comparing models. It is based on the idea of leave-one-out cross-validation, but is more efficient to compute.\nWith brms, it is easy to compute the LOO criterion:\n\n\nShow the code\nloo(fit)\n\n\nWarning: Found 374 observations with a pareto_k &gt; 0.7 in model 'fit'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\n\nComputed from 4000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12990.0  97.3\np_loo      1185.2  40.4\nlooic     25980.0 194.7\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.1, 2.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     3725  90.9%   56      \n   (0.7, 1]   (bad)       303   7.4%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   71   1.7%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nA helpful glossary explaining the LOO statistics can be found here.\nDifferent criteria are available, for example the elpd_loo is the expected log pointwise predictive density, and the looic = -2 * elpd_loo is the LOO information criterion. For comparing models, the looic is often used, where smaller numbers are better. it is kind of an equivalent to the AIC, but based on the LOO criterion.\nLet’s e.g. compare the two models we fitted above, one for the whole dataset and one with the treatment arm as a covariate for the shrinkage and growth rates:\n\n\nShow the code\nfit &lt;- add_criterion(fit, \"loo\")\n\n\nWarning: Found 374 observations with a pareto_k &gt; 0.7 in model 'fit'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nShow the code\nfit_by_arm &lt;- add_criterion(fit_by_arm, \"loo\")\n\n\nWarning: Found 359 observations with a pareto_k &gt; 0.7 in model 'fit_by_arm'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nShow the code\nloo_compare(fit, fit_by_arm)\n\n\n           elpd_diff se_diff\nfit         0.0       0.0   \nfit_by_arm -0.2       5.9   \n\n\nSo the model without treatment arm seems to be very slightly preferred here by the LOO criterion. Nevertheless, the model with treatment arm might answer exactly the question we need to answer, so it is important to consider the context of the analysis.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#tips-and-tricks",
    "href": "session-tgi/1_tgi_sf_brms.html#tips-and-tricks",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Tips and tricks",
    "text": "Tips and tricks\n\nWhen the model breaks down completely, that is a sign that likely the model specification is wrong. For example, in above code I first did not understand how to model the sigma parameter correctly, which led to a model where each chain was completely stuck at its initial values.\nIn that case and in general if you are not sure whether the brms model specification is correct, you can check the Stan code that is generated by brms:\n\n\nShow the code\n# Good to check the stan code:\n# (This also helps to find the names of the parameters\n# for which to define the initial values below)\nstancode(formula, prior = priors, data = df, family = gaussian())\n\n\n// generated with brms 2.22.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=1&gt; K_tau;  // number of population-level effects\n  matrix[N, K_tau] X_tau;  // population-level design matrix\n  int&lt;lower=1&gt; K_lb0;  // number of population-level effects\n  matrix[N, K_lb0] X_lb0;  // population-level design matrix\n  int&lt;lower=1&gt; K_lks;  // number of population-level effects\n  matrix[N, K_lks] X_lks;  // population-level design matrix\n  int&lt;lower=1&gt; K_lkg;  // number of population-level effects\n  matrix[N, K_lkg] X_lkg;  // population-level design matrix\n  // covariates for non-linear functions\n  vector[N] C_ystar_1;\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_lb0_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_lks_1;\n  // data for group-level effects of ID 3\n  int&lt;lower=1&gt; N_3;  // number of grouping levels\n  int&lt;lower=1&gt; M_3;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_3;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_3_lkg_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector&lt;lower=0&gt;[K_tau] b_tau;  // regression coefficients\n  vector[K_lb0] b_lb0;  // regression coefficients\n  vector[K_lks] b_lks;  // regression coefficients\n  vector[K_lkg] b_lkg;  // regression coefficients\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_3] sd_3;  // group-level standard deviations\n  array[M_3] vector[N_3] z_3;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_lb0_1;  // actual group-level effects\n  vector[N_2] r_2_lks_1;  // actual group-level effects\n  vector[N_3] r_3_lkg_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_lb0_1 = (sd_1[1] * (z_1[1]));\n  r_2_lks_1 = (sd_2[1] * (z_2[1]));\n  r_3_lkg_1 = (sd_3[1] * (z_3[1]));\n  lprior += normal_lpdf(b_tau | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(b_lb0 | log(65), 1);\n  lprior += normal_lpdf(b_lks | log(0.52), 0.1);\n  lprior += normal_lpdf(b_lkg | log(1.04), 1);\n  lprior += normal_lpdf(sd_1 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(sd_2 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(sd_3 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] nlp_tau = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lb0 = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lks = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lkg = rep_vector(0.0, N);\n    // initialize non-linear predictor term\n    vector[N] nlp_b0;\n    // initialize non-linear predictor term\n    vector[N] nlp_ks;\n    // initialize non-linear predictor term\n    vector[N] nlp_kg;\n    // initialize non-linear predictor term\n    vector[N] nlp_ystar;\n    // initialize non-linear predictor term\n    vector[N] mu;\n    // initialize non-linear predictor term\n    vector[N] sigma;\n    nlp_tau += X_tau * b_tau;\n    nlp_lb0 += X_lb0 * b_lb0;\n    nlp_lks += X_lks * b_lks;\n    nlp_lkg += X_lkg * b_lkg;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lb0[n] += r_1_lb0_1[J_1[n]] * Z_1_lb0_1[n];\n    }\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lks[n] += r_2_lks_1[J_2[n]] * Z_2_lks_1[n];\n    }\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lkg[n] += r_3_lkg_1[J_3[n]] * Z_3_lkg_1[n];\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_b0[n] = (exp(nlp_lb0[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_ks[n] = (exp(nlp_lks[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_kg[n] = (exp(nlp_lkg[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_ystar[n] = (int_step(C_ystar_1[n] &gt; 0) * (nlp_b0[n] * (exp( - nlp_ks[n] * C_ystar_1[n]) + exp(nlp_kg[n] * C_ystar_1[n]) - 1)) + int_step(C_ystar_1[n] &lt;= 0) * (nlp_b0[n] * exp(nlp_kg[n] * C_ystar_1[n])));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      mu[n] = (nlp_ystar[n]);\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      sigma[n] = exp(log(nlp_tau[n]) + log(nlp_ystar[n]));\n    }\n    target += normal_lpdf(Y | mu, sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n  target += std_normal_lpdf(z_3[1]);\n}\ngenerated quantities {\n}\n\n\nHere e.g. it is important to see that the sigma parameter is modelled on the log scale.\nWe can also extract the actual data that is passed to the Stan program:\n\n\nShow the code\n# Extract the data that is passed to the Stan program\nstan_data &lt;- standata(formula, prior = priors, data = df, family = gaussian())\n\n# Check that the time variable is correct\nall(stan_data$C_eta_1 == df$year)\n\n\n[1] TRUE\n\n\nThis can be useful to check if the data is passed correctly to the Stan program.\nIt is important to take divergence warnings seriously. For the model parameters where Rhat is larger than 1.05 or so, you can just have a look at the traceplot. For example, in an earlier model fit the \\(\\log(\\mu_{\\phi})\\) population parameter had this traceplot:\n\n\nShow the code\nfit_save &lt;- fit\nload(here(\"session-tgi/fit.RData\"))\nplot(fit, pars = \"b_tphi\")\n\n\nWarning: Argument 'pars' is deprecated. Please use 'variable' instead.\n\n\n\n\n\n\n\n\n\nShow the code\nfit &lt;- fit_save\n\n\nWe see that two chains led to a \\(\\log(\\mu_{\\phi})\\) value below 0, while two chains hovered around much larger values between 5 and 15, which corresponds to \\(\\mu_{\\phi} = 1\\). This shows that the model is rather overparametrized. By restricting the range of the prior for \\(\\log(\\mu_{\\phi})\\) to be below 0, we could avoid this problem.\nProviding manually realistic initial values for the model parameters can help to speed up the convergence of the Markov chains. This is especially important for the nonlinear parameters. You can get the names of the parameters from the Stan code above - note that you cannot use the names of the brms R code.\nSometimes it is not easy to get the dimensions of the inital values right. Here it can be helpful to change the backend to rstan, which provides more detailed error messages. You can do this by setting backend = \"rstan\" in the brm call.\nWhen the Stan compiler gives you an error, that can be very informative: Just open the Stan code file that is mentioned in the error message and look at the line number that is mentioned. This can give you a good idea of what went wrong: Maybe a typo in the prior definition, or a missing semicolon, or a wrong dimension in the data block. With VScode e.g. this is very easy: Just hold Ctrl and click on the file name and number, and you will be taken to the right line in the Stan code.\nIn order to quickly get results for a model, e.g. for obtaining useful starting values or prior distributions, it can be worth trying the “Pathfinder” algorithm in brms. Pathfinder is a variational method for approximately sampling from differentiable log densities. You can use it by setting algorithm = \"pathfinder\" in the brm call. In this example it takes only a minute, compared to more than an hour for the full model fit. However, the results are still quite different, so it is likely only useful as a first approximation. Nevertheless, the individual model fits look quite encouraging, in the sense that they have found good parameter values - but they are still lacking any uncertainty, so could not be used for confidence or prediction intervals:\n\n\nShow the code\nfit_fast_file &lt;- here(\"session-tgi/fit_fast.RData\")\nif (file.exists(fit_fast_file)) {\n  load(fit_fast_file)\n} else {\n  fit_fast &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH,\n    algorithm = \"pathfinder\"\n  )\n  save(fit_fast, file = fit_fast_file)\n}\n\nsummary(fit_fast)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ ystar \n         ystar ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(ystar)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 1 chains, each with iter = 1000; warmup = 0; thin = 1;\n         total post-warmup draws = 1000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     3.37      0.00     3.37     3.37   NA       NA       NA\nsd(lks_Intercept)     9.16      0.00     9.16     9.16   NA       NA       NA\nsd(lkg_Intercept)     6.90      0.00     6.90     6.90   NA       NA       NA\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.13      0.00     0.13     0.13   NA       NA       NA\nlb0_Intercept     3.62      0.00     3.62     3.62   NA       NA       NA\nlks_Intercept    -0.49      0.00    -0.49    -0.49   NA       NA       NA\nlkg_Intercept    -0.66      0.00    -0.66    -0.66   NA       NA       NA\n\nDraws were sampled using pathfinder(). \n\n\nShow the code\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_linpred_draws(fit_fast) |&gt; \n  median_qi() |&gt; \n  mutate(sld_pred = .linpred)\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = sld_pred, ymin = .lower, ymax = .upper),\n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = sld_pred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"SF model fit (pathfinder approximation)\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html",
    "href": "session-tgi/3_tgi_gsf_brms.html",
    "title": "3. Generalized Stein-Fojo model",
    "section": "",
    "text": "This appendix shows how the generalized Stein-Fojo model can be implemented in a Bayesian framework using the brms package in R.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#setup-and-load-data",
    "href": "session-tgi/3_tgi_gsf_brms.html#setup-and-load-data",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#generalized-stein-fojo-model",
    "href": "session-tgi/3_tgi_gsf_brms.html#generalized-stein-fojo-model",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Generalized Stein-Fojo model",
    "text": "Generalized Stein-Fojo model\nHere we have an additional parameter \\(\\phi\\), which is the weight for the shrinkage in the double exponential model. The model is then:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\{\n  \\psi_{\\phi i} \\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) +\n  (1 - \\psi_{\\phi i}) \\exp(\\psi_{k_{g}i} \\cdot t_{ij})\n\\}\n\\]\nfor positive times \\(t_{ij}\\). Again, if the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. That is, we have then \\(\\phi_i = 0\\). Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\{\n  \\psi_{\\phi i} \\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) +\n  (1 - \\psi_{\\phi i}) \\exp(\\psi_{k_{g}i} \\cdot t_{ij})\n\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\nIn terms of likelihood and priors, we can use the same assumptions as in the previous model. The only difference is that we have to model the \\(\\phi\\) parameter. We can use a logit-normal distribution for this parameter. This is a normal distribution on the logit scale, which is then transformed to the unit interval. \\[\n\\psi_{\\phi i} \\sim \\text{LogitNormal}(\\text{logit}(0.5) = 0, 0.5)\n\\]",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#fit-model",
    "href": "session-tgi/3_tgi_gsf_brms.html#fit-model",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (phi * exp(-ks * year) + (1 - phi) * exp(kg * year))) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # As before:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(phi ~ inv_logit(tphi)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(tphi ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + (1 | id)) +\n  lf(lkg ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\"),\n  prior(normal(log(1.04), 1), nlpar = \"lkg\"),\n  prior(normal(0, 0.5), nlpar = \"tphi\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(student_t(3, 0, 22.2), lb = 0, nlpar = \"tphi\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lks = array(-1.25),\n  b_lkg = array(-1.33),\n  b_tphi = array(0),\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  sd_4 = array(0.1),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients),\n  z_4 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/gsf1.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = WARMUP + ITER, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsummary(fit)\n\n\nWarning: There were 41 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (phi * exp(-ks * year) + (1 - phi) * exp(kg * year))) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         phi ~ inv_logit(tphi)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         tphi ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)      0.58      0.02     0.55     0.61 1.01      382      606\nsd(tphi_Intercept)     2.12      0.18     1.78     2.49 1.01      611     1216\nsd(lks_Intercept)      2.16      0.14     1.90     2.46 1.00      520     1282\nsd(lkg_Intercept)      1.18      0.09     1.01     1.36 1.00      899     1766\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept      0.15      0.00     0.14     0.15 1.00     1611     2351\nlb0_Intercept      3.63      0.02     3.59     3.68 1.03      221      547\ntphi_Intercept    -0.14      0.22    -0.57     0.27 1.00      424      839\nlks_Intercept     -0.62      0.10    -0.81    -0.43 1.00     1170     1868\nlkg_Intercept     -1.15      0.14    -1.43    -0.89 1.00      699     1488\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn total this took 76 minutes on my laptop.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#parameter-estimates",
    "href": "session-tgi/3_tgi_gsf_brms.html#parameter-estimates",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\n\nShow the code\npost_df &lt;- as_draws_df(fit)\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_tphi_Intercept\"      \n [4] \"b_lks_Intercept\"        \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"  \n [7] \"sd_id__tphi_Intercept\"  \"sd_id__lks_Intercept\"   \"sd_id__lkg_Intercept\"  \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_phi = plogis(b_tphi_Intercept),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    omega_phi = sd_id__tphi_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nLet’s first look at the population level parameters:\n\n\nShow the code\ngsf_pop_params &lt;- c(\"theta_b0\", \"theta_ks\", \"theta_kg\", \"theta_phi\", \"sigma\")\n\nmcmc_trace(post_df, pars = gsf_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = gsf_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nThe trace plots look good. The chains seem to have converged and the pairs plot shows no strong correlations between the parameters. Let’s check the table:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  select(theta_b0, theta_ks, theta_kg, theta_phi, omega_0, omega_s, omega_g, omega_phi, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.766\n44.746\n1.057\n1.073\n43.074\n46.528\n1.007\n179.361\n378.906\n\n\ntheta_ks\n5.857\n5.444\n1.924\n1.582\n3.589\n9.560\n1.003\n384.622\n905.388\n\n\ntheta_kg\n0.640\n0.637\n0.061\n0.062\n0.541\n0.745\n1.000\n685.379\n1,885.264\n\n\ntheta_phi\n0.467\n0.466\n0.055\n0.058\n0.376\n0.555\n1.003\n419.761\n831.991\n\n\nomega_0\n0.578\n0.578\n0.016\n0.017\n0.553\n0.606\n1.001\n376.483\n628.807\n\n\nomega_s\n2.159\n2.155\n0.145\n0.145\n1.929\n2.412\n1.002\n521.866\n1,265.639\n\n\nomega_g\n1.177\n1.174\n0.090\n0.090\n1.037\n1.333\n1.002\n868.075\n1,746.640\n\n\nomega_phi\n2.117\n2.108\n0.182\n0.184\n1.833\n2.423\n1.003\n603.806\n1,215.066\n\n\nsigma\n0.147\n0.147\n0.002\n0.002\n0.143\n0.150\n1.000\n1,575.916\n2,168.010\n\n\n\n\n\n\n\nSo \\(\\theta_{\\phi}\\) is estimated around 0.5. The other parameters are similar to the previous Stein-Fojo model, but we see a larger \\(\\theta_{k_s}\\) e.g.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#observation-vs-model-fit",
    "href": "session-tgi/3_tgi_gsf_brms.html#observation-vs-model-fit",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_epred_draws(fit) |&gt;\n  median_qi()\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = .epred, ymin = .lower, ymax = .upper), \n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = .epred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"GSF model fit\")\n\n\n\n\n\n\n\n\n\nThis also looks good. The model seems to capture the data well.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#with-jmpost",
    "href": "session-tgi/3_tgi_gsf_brms.html#with-jmpost",
    "title": "3. Generalized Stein-Fojo model",
    "section": "With jmpost",
    "text": "With jmpost\nThis model can also be fit with the jmpost package. The corresponding function is LongitudinalGSF. The statistical model is specified in the vignette here.\nHomework: Implement the generalized Stein-Fojo model with jmpost and compare the results with the brms implementation.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html",
    "href": "session-tgi/2_tgi_sf_jmpost.html",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "",
    "text": "Let’s try to fit the same model now with jmpost. We will use the same data as in the previous notebook.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#setup-and-load-data",
    "href": "session-tgi/2_tgi_sf_jmpost.html#setup-and-load-data",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#data-preparation",
    "href": "session-tgi/2_tgi_sf_jmpost.html#data-preparation",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Data preparation",
    "text": "Data preparation\nWe start with the subject level data set. For the beginning, we want to treat all observations as if they come from a single arm and single study for now. Therefore we insert constant study and arm values here.\n\n\nShow the code\nsubj_df &lt;- data.frame(\n  id = unique(df$id),\n  arm = \"arm\",\n  study = \"study\"\n)\nsubj_data &lt;- DataSubject(\n  data = subj_df,\n  subject = \"id\",\n  arm = \"arm\",\n  study = \"study\"\n)\n\n\nNext we prepare the longitudinal data object.\n\n\nShow the code\nlong_df &lt;- df |&gt;\n  select(id, year, sld)\nlong_data &lt;- DataLongitudinal(\n  data = long_df,\n  formula = sld ~ year\n)\n\n\nNow we can create the JointData object:\n\n\nShow the code\njoint_data &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data\n)",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#model-specification",
    "href": "session-tgi/2_tgi_sf_jmpost.html#model-specification",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Model specification",
    "text": "Model specification\nThe statistical model is specified in the jmpost vignette here.\nHere we just want to fit the longitudinal data, therefore:\n\n\nShow the code\ntgi_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    )\n)\n\n\nNote that the priors on the standard deviations, omega_* and sigma, are truncated to the positive domain. So we used here truncated normal priors.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#fit-model",
    "href": "session-tgi/2_tgi_sf_jmpost.html#fit-model",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using jmpost.\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/jm5.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  mcmc_results &lt;- sampleStanModel(\n      tgi_mod,\n      data = joint_data,\n      iter_sampling = ITER,\n      iter_warmup = WARMUP,\n      chains = CHAINS,\n      parallel_chains = CHAINS,\n      thin = CHAINS,\n      seed = BAYES.SEED,\n      refresh = REFRESH\n  )\n  save(mcmc_results, file = save_file)\n}\n\n\nLet’s check the convergence of the population parameters:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nsave_overall_file &lt;- here(\"session-tgi/jm5_more.RData\")\nif (file.exists(save_overall_file)) {\n  load(save_overall_file)\n} else {\n  mcmc_res_cmdstan &lt;- cmdstanr::as.CmdStanMCMC(mcmc_results)\n  mcmc_res_sum &lt;- mcmc_res_cmdstan$summary(vars)\n  vars_draws &lt;- mcmc_res_cmdstan$draws(vars)\n  loo_res &lt;- mcmc_res_cmdstan$loo(r_eff = FALSE)\n  save(mcmc_res_sum, vars_draws, loo_res, file = save_overall_file)\n}\nmcmc_res_sum\n\n\n# A tibble: 7 × 10\n  variable     mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lm_sf_mu_…  3.61   3.61  0.0236  0.0232   3.57   3.65   1.02     142.     188.\n2 lm_sf_mu_… -1.27  -1.27  0.160   0.152   -1.54  -1.03   1.01     352.     500.\n3 lm_sf_mu_… -1.35  -1.35  0.0828  0.0789  -1.49  -1.22   1.00     230.     456.\n4 lm_sf_sig…  0.161  0.161 0.00233 0.00244  0.158  0.165  1.00     513.     875.\n5 lm_sf_ome…  0.579  0.578 0.0156  0.0153   0.555  0.608  1.00     341.     576.\n6 lm_sf_ome…  1.62   1.62  0.117   0.112    1.45   1.82   1.01     321.     509.\n7 lm_sf_ome…  0.997  0.993 0.0505  0.0503   0.920  1.08   1.00     390.     595.\n\n\nThis looks good, let’s check the traceplots:\n\n\nShow the code\n# vars_draws &lt;- mcmc_res_cmdstan$draws(vars)\nmcmc_trace(vars_draws)\n\n\n\n\n\n\n\n\n\nThey also look ok, all chains are mixing well in the same range of parameter values.\nAlso here we could look at the pairs plot:\n\n\nShow the code\nmcmc_pairs(\n  vars_draws,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#observation-vs.-model-fit",
    "href": "session-tgi/2_tgi_sf_jmpost.html#observation-vs.-model-fit",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Observation vs. model fit",
    "text": "Observation vs. model fit\nLet’s check the fit of the model to the data:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\n\nsave_fit_file &lt;- here(\"session-tgi/jm5_fit.RData\")\nif (file.exists(save_fit_file)) {\n  load(save_fit_file)\n} else {\n  fit_subset &lt;- LongitudinalQuantities(\n    mcmc_results, \n    grid = GridObserved(subjects = pt_subset)\n  )\n  save(fit_subset, file = save_fit_file)\n}\n\nautoplot(fit_subset)+\n  labs(x = \"Time (years)\", y = \"SLD (mm)\")\n\n\n\n\n\n\n\n\n\nSo this works very nicely.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#prior-vs.-posterior",
    "href": "session-tgi/2_tgi_sf_jmpost.html#prior-vs.-posterior",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Prior vs. posterior",
    "text": "Prior vs. posterior\nLet’s check the prior vs. posterior for the parameters:\n\n\nShow the code\npost_samples &lt;- as_draws_df(vars_draws) |&gt; \n  rename(\n    mu_bsld = \"lm_sf_mu_bsld[1]\",\n    mu_ks = \"lm_sf_mu_ks[1]\",\n    mu_kg = \"lm_sf_mu_kg[1]\",\n    omega_bsld = \"lm_sf_omega_bsld[1]\",\n    omega_ks = \"lm_sf_omega_ks[1]\",\n    omega_kg = \"lm_sf_omega_kg[1]\",\n    sigma = lm_sf_sigma\n  ) |&gt; \n  mutate(type = \"posterior\") |&gt; \n  select(mu_bsld, mu_ks, mu_kg, omega_bsld, omega_ks, omega_kg, sigma, type)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\nn_prior_samples &lt;- nrow(post_samples)\nprior_samples &lt;- data.frame(\n    mu_bsld = rnorm(n_prior_samples, log(65), 1),\n    mu_ks = rnorm(n_prior_samples, log(0.52), 1),\n    mu_kg = rnorm(n_prior_samples, log(1.04), 1),\n    omega_bsld = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    omega_ks = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    omega_kg = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    sigma = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3)\n  ) |&gt; \n  mutate(type = \"prior\")\n\n# Combine the two\ncombined_samples &lt;- rbind(post_samples, prior_samples) |&gt; \n  pivot_longer(cols = -type, names_to = \"parameter\", values_to = \"value\")\n\nggplot(combined_samples, aes(x = value, fill = type)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~parameter, scales = \"free\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis looks good, because the priors are covering the range of the posterior samples and are not too informative.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#parameter-estimates",
    "href": "session-tgi/2_tgi_sf_jmpost.html#parameter-estimates",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nHere we need again to be careful: We are interested in the posterior mean estimates of the baseline, shrinkage and growth population rates on the original scale. Because we model them on the log scale as normal distributed, we need to use the mean of the log-normal distribution to get the mean on the original scale.\n\n\nShow the code\npost_sum &lt;- post_samples |&gt;\n  mutate(\n    theta_b0 = exp(mu_bsld + omega_bsld^2 / 2), \n    theta_ks = exp(mu_ks + omega_ks^2 / 2), \n    theta_kg = exp(mu_kg + omega_kg^2 / 2),\n    cv_0 = sqrt(exp(omega_bsld^2) - 1),\n    cv_s = sqrt(exp(omega_ks^2) - 1),\n    cv_g = sqrt(exp(omega_kg^2) - 1)\n  ) |&gt;\n  select(theta_b0, theta_ks, theta_kg, omega_bsld, omega_ks, omega_kg, cv_0, cv_s, cv_g, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.7\n43.8\n1.10\n1.12\n42.0\n45.5\n1.00\n151\n201\n\n\ntheta_ks\n1.06\n1.05\n0.129\n0.120\n0.871\n1.30\n1.00\n226\n408\n\n\ntheta_kg\n0.428\n0.427\n0.0311\n0.0306\n0.378\n0.479\n1.00\n318\n472\n\n\nomega_bsld\n0.579\n0.578\n0.0156\n0.0153\n0.555\n0.608\n1.00\n346\n545\n\n\nomega_ks\n1.62\n1.62\n0.117\n0.112\n1.45\n1.82\n1.00\n318\n494\n\n\nomega_kg\n0.997\n0.993\n0.0505\n0.0503\n0.920\n1.08\n1.00\n390\n576\n\n\ncv_0\n0.631\n0.630\n0.0200\n0.0196\n0.600\n0.668\n1.00\n346\n545\n\n\ncv_s\n3.70\n3.57\n0.818\n0.685\n2.66\n5.16\n1.00\n318\n494\n\n\ncv_g\n1.31\n1.30\n0.106\n0.103\n1.15\n1.49\n1.00\n390\n576\n\n\nsigma\n0.161\n0.161\n0.00233\n0.00244\n0.158\n0.165\n1.00\n495\n844\n\n\n\n\n\n\n\nWe can see that these are consistent with the estimates from the brms model earlier.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#separate-arm-estimates",
    "href": "session-tgi/2_tgi_sf_jmpost.html#separate-arm-estimates",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Separate arm estimates",
    "text": "Separate arm estimates\nWhile there is no general covariates support in jmpost for the longitudinal models as of now, we can obtain separate estimates for the longitudinal model parameters: As detailed in the model specification here, as soon as we have the arm defined, then separate estimates for the arm-specific shrinkage and growth parameters will be obtained: Both the population means and standard deviation parameters are here arm-specific. (Note that this is slightly different from brms where we assumed earlier the same standard deviation independent of the treatment arm.)\nSo we need to define the subject data accordingly now with the arm information:\n\n\nShow the code\nsubj_df_by_arm &lt;- df |&gt;\n  select(id, arm) |&gt;\n  distinct() |&gt; \n  mutate(study = \"study\")\n\nsubj_data_by_arm &lt;- DataSubject(\n  data = subj_df_by_arm,\n  subject = \"id\",\n  arm = \"arm\",\n  study = \"study\"\n)\n\n\nWe redefine the JointData object and can then fit the model, because the prior specification does not need to change: We assume iid priors on the arm-specific parameters here.\n\n\nShow the code\njoint_data_by_arm &lt;- DataJoint(\n    subject = subj_data_by_arm,\n    longitudinal = long_data\n)\n\n\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/jm6.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  mcmc_results_by_arm &lt;- sampleStanModel(\n      tgi_mod,\n      data = joint_data_by_arm,\n      iter_sampling = ITER,\n      iter_warmup = WARMUP,\n      chains = CHAINS,\n      parallel_chains = CHAINS,\n      thin = CHAINS,\n      seed = BAYES.SEED,\n      refresh = REFRESH\n  )\n  save(mcmc_results_by_arm, file = save_file)\n}\n\n\nLet’s again check the convergence:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nsave_arm_file &lt;- here(\"session-tgi/jm6_more.RData\")\nif (file.exists(save_arm_file)) {\n  load(save_arm_file)\n} else {\n  mcmc_res_cmdstan_by_arm &lt;- cmdstanr::as.CmdStanMCMC(mcmc_results_by_arm)\n  mcmc_res_sum_by_arm &lt;- mcmc_res_cmdstan_by_arm$summary(vars)\n  vars_draws_by_arm &lt;- mcmc_res_cmdstan_by_arm$draws(vars)\n  loo_by_arm &lt;- mcmc_res_cmdstan_by_arm$loo(r_eff = FALSE)\n  save(mcmc_res_sum_by_arm, vars_draws_by_arm, loo_by_arm, file = save_arm_file)\n}\nmcmc_res_sum_by_arm\n\n\n# A tibble: 11 × 10\n   variable    mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu…  3.61   3.61  0.0211  0.0217   3.58   3.65  1.00      244.     502.\n 2 lm_sf_mu… -0.752 -0.730 0.222   0.216   -1.14  -0.398 1.01      592.     673.\n 3 lm_sf_mu… -1.52  -1.51  0.222   0.217   -1.90  -1.17  1.00      706.     850.\n 4 lm_sf_mu… -1.11  -1.11  0.138   0.139   -1.35  -0.900 1.00      594.     814.\n 5 lm_sf_mu… -1.35  -1.35  0.103   0.104   -1.52  -1.19  1.00      560.     776.\n 6 lm_sf_si…  0.161  0.161 0.00224 0.00228  0.157  0.165 1.00      892.     941.\n 7 lm_sf_om…  0.577  0.577 0.0159  0.0153   0.551  0.603 1.00      511.     672.\n 8 lm_sf_om…  1.34   1.33  0.150   0.152    1.11   1.60  1.00      630.     844.\n 9 lm_sf_om…  1.76   1.76  0.161   0.150    1.51   2.05  1.00      662.     990.\n10 lm_sf_om…  0.765  0.758 0.0710  0.0685   0.662  0.891 0.999     809.     951.\n11 lm_sf_om…  1.10   1.10  0.0705  0.0684   0.991  1.22  0.999     669.     736.\n\n\nLet’s again tabulate the parameter estimates:\n\n\nShow the code\npost_samples_by_arm &lt;- as_draws_df(vars_draws_by_arm) |&gt; \n  rename(\n    mu_bsld = \"lm_sf_mu_bsld[1]\",\n    mu_ks1 = \"lm_sf_mu_ks[1]\",\n    mu_ks2 = \"lm_sf_mu_ks[2]\",\n    mu_kg1 = \"lm_sf_mu_kg[1]\",\n    mu_kg2 = \"lm_sf_mu_kg[2]\",\n    omega_bsld = \"lm_sf_omega_bsld[1]\",\n    omega_ks1 = \"lm_sf_omega_ks[1]\",\n    omega_ks2 = \"lm_sf_omega_ks[2]\",\n    omega_kg1 = \"lm_sf_omega_kg[1]\",\n    omega_kg2 = \"lm_sf_omega_kg[2]\",\n    sigma = lm_sf_sigma\n  ) |&gt; \n  mutate(\n    theta_b0 = exp(mu_bsld + omega_bsld^2 / 2), \n    theta_ks1 = exp(mu_ks1 + omega_ks1^2 / 2), \n    theta_ks2 = exp(mu_ks2 + omega_ks2^2 / 2),\n    theta_kg1 = exp(mu_kg1 + omega_kg1^2 / 2),\n    theta_kg2 = exp(mu_kg2 + omega_kg2^2 / 2),\n    cv_0 = sqrt(exp(omega_bsld^2) - 1),\n    cv_s1 = sqrt(exp(omega_ks1^2) - 1),\n    cv_s2 = sqrt(exp(omega_ks2^2) - 1),\n    cv_g1 = sqrt(exp(omega_kg1^2) - 1),\n    cv_g2 = sqrt(exp(omega_kg2^2) - 1)\n  ) \n  \npost_sum_by_arm &lt;- post_samples_by_arm |&gt;\n  select(\n    theta_b0, theta_ks1, theta_ks2, theta_kg1, theta_kg2, \n    omega_bsld, omega_ks1, omega_ks2, omega_kg1, omega_kg2, \n    cv_0, cv_s1, cv_s2, cv_g1, cv_g2, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum_by_arm\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.7\n43.7\n0.999\n1.05\n42.1\n45.4\n1.00\n227\n558\n\n\ntheta_ks1\n1.18\n1.17\n0.152\n0.146\n0.961\n1.44\n1.00\n530\n696\n\n\ntheta_ks2\n1.07\n1.04\n0.209\n0.183\n0.779\n1.42\n1.00\n344\n443\n\n\ntheta_kg1\n0.444\n0.443\n0.0490\n0.0489\n0.365\n0.527\n1.00\n539\n712\n\n\ntheta_kg2\n0.479\n0.478\n0.0487\n0.0479\n0.399\n0.563\n1.00\n735\n908\n\n\nomega_bsld\n0.577\n0.577\n0.0159\n0.0153\n0.551\n0.603\n1.00\n504\n659\n\n\nomega_ks1\n1.34\n1.33\n0.150\n0.152\n1.11\n1.60\n1.00\n620\n838\n\n\nomega_ks2\n1.76\n1.76\n0.161\n0.150\n1.51\n2.05\n1.00\n651\n972\n\n\nomega_kg1\n0.765\n0.758\n0.0710\n0.0685\n0.662\n0.891\n0.999\n799\n943\n\n\nomega_kg2\n1.10\n1.10\n0.0705\n0.0684\n0.991\n1.22\n1.00\n649\n693\n\n\ncv_0\n0.629\n0.628\n0.0205\n0.0196\n0.596\n0.662\n1.00\n504\n659\n\n\ncv_s1\n2.33\n2.22\n0.601\n0.547\n1.55\n3.46\n1.00\n620\n838\n\n\ncv_s2\n4.90\n4.58\n1.64\n1.25\n2.96\n8.11\n1.00\n651\n972\n\n\ncv_g1\n0.897\n0.880\n0.112\n0.105\n0.742\n1.10\n0.999\n799\n943\n\n\ncv_g2\n1.55\n1.54\n0.175\n0.163\n1.29\n1.85\n1.00\n649\n693\n\n\nsigma\n0.161\n0.161\n0.00224\n0.00228\n0.157\n0.165\n1.00\n885\n881\n\n\n\n\n\n\n\nHere again the shrinkage rate in the treatment arm 1 seems higher than in the treatment arm 2. However, the difference is not as pronounced as in the brms model before with the same standard deviation for both arms. We can again calculate the posterior probability that the shrinkage rate in arm 1 is higher than in arm 2:\n\n\nShow the code\nprob_ks1_greater_ks2 &lt;- mean(post_samples_by_arm$theta_ks1 &gt; post_samples_by_arm$theta_ks2)\nprob_ks1_greater_ks2\n\n\n[1] 0.706\n\n\nSo the posterior probability is now only around 71%.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#model-comparison-with-loo",
    "href": "session-tgi/2_tgi_sf_jmpost.html#model-comparison-with-loo",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Model comparison with LOO",
    "text": "Model comparison with LOO\nAs we have seen for brms, also for jmpost we can easily compute the LOO criterion:\n\n\nShow the code\n# loo_res &lt;- mcmc_res_cmdstan$loo(r_eff = FALSE)\nloo_res\n\n\n\nComputed from 1000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12957.3  95.8\np_loo      1137.1  38.4\nlooic     25914.5 191.6\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nPareto k diagnostic values:\n                          Count Pct.    Min. ESS\n(-Inf, 0.67]   (good)     3665  89.4%   26      \n   (0.67, 1]   (bad)       363   8.9%   &lt;NA&gt;    \n    (1, Inf)   (very bad)   71   1.7%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nUnderneath this is using the $loo() method from cmdstanr.\nAnd we can compare this to the LOO of the model with separate arm estimates:\n\n\nShow the code\n# loo_by_arm &lt;- mcmc_res_cmdstan_by_arm$loo(r_eff = FALSE)\nloo_by_arm\n\n\n\nComputed from 1000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12931.8  95.6\np_loo      1121.5  36.8\nlooic     25863.5 191.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nPareto k diagnostic values:\n                          Count Pct.    Min. ESS\n(-Inf, 0.67]   (good)     3652  89.1%   48      \n   (0.67, 1]   (bad)       374   9.1%   &lt;NA&gt;    \n    (1, Inf)   (very bad)   73   1.8%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nSo the model by treatment arm performs here better than the model without treatment arm specific growth and shrinkage parameters.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#tipps-and-tricks",
    "href": "session-tgi/2_tgi_sf_jmpost.html#tipps-and-tricks",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Tipps and tricks",
    "text": "Tipps and tricks\n\nAlso here it is possible to look at the underlying Stan code:\n\n\nShow the code\ntmp &lt;- tempfile(fileext = \".stan\") # file extension for syntax highlighting\nwrite_stan(tgi_mod, destination = tmp)\nfile.edit(tmp) # opens the Stan file in the default editor\n\n\nIt is not trivial to transport saved models from one computer to another. This is because cmdstanr only loads the results it currently needs from disk into memory, and thus into the R session. If you want to transport the model to another computer, you need to save the Stan code and the data, and then re-run the model on the other computer. This is because the model object in R is only a reference to the model on disk, not the model itself. Note that there is the $save_object() method, see here, however this leads to very large files (here about 300 MB for one fit) and can thus not be uploaded to typical git repositories. Therefore above we saved interim result objects separately as needed.\nIt is important to explicitly define the truncation boundaries for the truncated normal priors, because otherwise the MCMC results will not be correct.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/0_setup.html",
    "href": "session-jm/0_setup.html",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "First we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 3: TGI-OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-jm/0_setup.html#setup",
    "href": "session-jm/0_setup.html#setup",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "First we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 3: TGI-OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-jm/0_setup.html#data-preparation",
    "href": "session-jm/0_setup.html#data-preparation",
    "title": "0. Setup and data preparation",
    "section": "Data preparation",
    "text": "Data preparation\nWe prepare now the same dataset as before in the OS session.\nWe will again use the OAK study data.\nFor the tumor growth data, we are using the S1 data set from here. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\ntumor_data &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\") |&gt; \n    read_excel(sheet = \"Study4\") |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm)),\n        arm = fct_recode(arm, \"Docetaxel\" = \"1\", \"MPDL3280A\" = \"2\")\n    ) |&gt; \n    select(id, year, sld, arm)\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 4\n  id                       year   sld arm      \n  &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 -594245265511412736  -0.00821  42   Docetaxel\n2 -594245265511412736   0.0986   44   Docetaxel\n3 -594245265511412736   0.222    44   Docetaxel\n4 -4078394139718744064 -0.0110   65.8 Docetaxel\n5 -4078394139718744064  0.832    66   Docetaxel\n6 -4078394139718744064  1.07     68   Docetaxel\n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        \n 5394902984416364544 :  16   Min.   :-0.1068   Min.   :  2.00  \n 7160320731596789760 :  16   1st Qu.: 0.1123   1st Qu.: 19.05  \n -4159496062492130816:  15   Median : 0.2519   Median : 32.00  \n -7900338178541499392:  15   Mean   : 0.3995   Mean   : 38.16  \n 4878279915140903936 :  15   3rd Qu.: 0.5749   3rd Qu.: 51.00  \n 9202154259772598272 :  15   Max.   : 2.0753   Max.   :228.00  \n (Other)             :4034                     NA's   :27      \n        arm      \n Docetaxel:1658  \n MPDL3280A:2468  \n                 \n                 \n                 \n                 \n                 \n\n\nHere we have 701 patients, and we know from Fig. 1 in the publication that this is the subset of patients with at least 3 tumor size measurements. We have guessed from the second Excel sheet with parameter estimates that arm 1 means docetaxel and arm 2 means MPDL3280A. This also seems reasonable given the larger number of measurements per patient:\n\n\nShow the code\n# make a table\ntumor_data |&gt; \n    group_by(id) |&gt; \n    summarize(arm = arm[1], n = n())  |&gt; \n    group_by(arm) |&gt; \n    summarize(mean_n = mean(n))\n\n\n# A tibble: 2 × 2\n  arm       mean_n\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Docetaxel   5.10\n2 MPDL3280A   6.56\n\n\nFor the OS data, we will be using the Supplementary Table 8 from here. Again, we have copied the data set in this GitHub repository.\nFrom the supplementary material, we know that:\n\nPFS/OS: time in months\nPFS.CNSR/OS.CNSR: 1 for censor, 0 for event\n\nTherefore we will convert the time to years and the censoring to a logical variable:\n\n\nShow the code\nos_data &lt;- here(\"data/41591_2018_134_MOESM3_ESM.xlsx\") |&gt; \n    read_excel(sheet = \"OAK_Clinical_Data\") |&gt; \n    clean_names() |&gt; \n    rename(\n        age = bage,\n        race = race2,\n        ecog = ecoggr,\n        arm = trt01p,\n        response = bcor\n    ) |&gt; \n    mutate(\n        id = factor(as.character(pt_id)),\n        sld = as.numeric(na_if(bl_sld, \".\")),\n        pfs_time = as.numeric(pfs) / 12,\n        pfs_event = as.numeric(pfs_cnsr) == 0,\n        os_time = as.numeric(os) / 12,\n        os_event = as.numeric(os_cnsr) == 0,\n        race = factor(race),\n        ecog = factor(ecog),\n        arm = factor(arm),\n        response = factor(\n          response, \n          levels = c(\"CR\", \"PR\", \"SD\", \"PD\", \"NE\")\n        ),\n        sex = factor(sex)\n    ) |&gt; \n    select(\n      id,\n      arm,\n      ecog,\n      age,\n      race,\n      sex,\n      sld,\n      response,\n      pfs_time,\n      pfs_event,\n      os_time,\n      os_event\n    )\n\nhead(os_data)\n\n\n# A tibble: 6 × 12\n  id    arm    ecog    age race  sex     sld response pfs_time pfs_event os_time\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;lgl&gt;       &lt;dbl&gt;\n1 318   Docet… 1        64 WHITE M     152   PD          0.107 TRUE        0.433\n2 1088  Docet… 0        65 WHITE M      36   PD          0.194 TRUE        0.402\n3 345   Docet… 1        75 WHITE M      92   &lt;NA&gt;        0.162 TRUE        0.162\n4 588   Docet… 0        61 WHITE F      36   SD          1.02  TRUE        2.05 \n5 306   MPDL3… 1        53 WHITE F      86   PD          0.118 TRUE        0.498\n6 718   Docet… 1        80 WHITE M      45.7 SD          0.712 TRUE        1.60 \n# ℹ 1 more variable: os_event &lt;lgl&gt;\n\n\nShow the code\nsummary(os_data)\n\n\n       id             arm      ecog         age           race     sex    \n 1000   :  1   Docetaxel:425   0:315   Min.   :33.00   ASIAN:180   F:330  \n 1001   :  1   MPDL3280A:425   1:535   1st Qu.:57.00   OTHER: 72   M:520  \n 1002   :  1                           Median :64.00   WHITE:598          \n 1003   :  1                           Mean   :63.23                      \n 1004   :  1                           3rd Qu.:70.00                      \n 1005   :  1                           Max.   :85.00                      \n (Other):844                                                              \n      sld         response      pfs_time        pfs_event      \n Min.   : 10.00   CR  :  7   Min.   :0.002738   Mode :logical  \n 1st Qu.: 41.00   PR  :108   1st Qu.:0.117728   FALSE:95       \n Median : 66.00   SD  :327   Median :0.240931   TRUE :755      \n Mean   : 76.69   PD  :304   Mean   :0.443174                  \n 3rd Qu.: 99.00   NE  : 24   3rd Qu.:0.580424                  \n Max.   :316.00   NA's: 80   Max.   :2.242300                  \n NA's   :1                                                     \n    os_time          os_event      \n Min.   :0.002738   Mode :logical  \n 1st Qu.:0.364134   FALSE:281      \n Median :0.803559   TRUE :569      \n Mean   :0.949841                  \n 3rd Qu.:1.620808                  \n Max.   :2.253251                  \n                                   \n\n\nNow we would like to join the two data sets, i.e. know which tumor growth data belongs to which baseline covariates and overall survival time and event indicator. Unfortunately we cannot use the id variable as it is not consistent between the two data sets, because both were anonymized. Instead, we will use the sld variable in combination with the treatment arm. We can also double check with the best overall response variable, because that is closely related to the tumor growth.\nFirst, we will summarize the patient information int he tumor data set. We will calculate the baseline SLD, determine the time of the last tumor measurement and approximate the best overall response by looking at the change from baseline:\n\n\nShow the code\nget_baseline &lt;- function(sld, year) {\n    which_base &lt;- tail(which(year &lt;= 0), 1L)\n    if (length(which_base) == 0) {\n      which_base &lt;- which.min(year)\n    }\n    sld[which_base]\n}\n\nget_contig_below_thresh &lt;- function(sld, year, bsld, thresh) {\n    rle_res &lt;- with(\n        rle(((sld[year &gt; 0] - bsld) / bsld) &lt; 0.2), \n        lengths[values]\n    )\n    if (length(rle_res) == 0) {\n      0\n    } else {\n      max(rle_res)\n    }\n}\n\ntumor_data_summary &lt;- tumor_data |&gt; \n    group_by(id) |&gt; \n    arrange(year) |&gt;\n    summarize(\n      arm = arm[1L],\n      bsld = get_baseline(sld, year),\n      last_year = tail(year, 1L),\n      nadir = min(sld[year &gt;= 0], na.rm = TRUE),\n      max_cfn = max((sld[year &gt;= 0] - nadir) / nadir, na.rm = TRUE),\n      min_cfb = min((sld[year &gt;= 0] - bsld) / bsld, na.rm = TRUE),\n      contig_below_0.2 = get_contig_below_thresh(sld, year, bsld, thresh = 0.2),\n      approx_response = case_when(\n        min_cfb &lt;= -0.3 ~ \"PR\",\n        contig_below_0.2 &gt;= 2 ~ \"SD\",\n        max_cfn &gt;= 0.2 ~ \"PD\",        \n        .default = \"NE\"\n      )   \n    )\nhead(tumor_data_summary)\n\n\n# A tibble: 6 × 9\n  id                arm    bsld last_year nadir max_cfn min_cfb contig_below_0.2\n  &lt;fct&gt;             &lt;fct&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n1 -100096848245159… Doce…    53     0.356    55  0       0.0377                3\n2 -102777882269110… MPDL…    11     0.901     2  2      -0.818                 7\n3 -105049343897269… MPDL…    20     0.345    21  1.62    0.05                  2\n4 -106934201929572… Doce…    15     1.72     11  0.273  -0.267                12\n5 -108949367808374… Doce…    33     0.709    31  0.0968 -0.0606                6\n6 -110624773747042… MPDL…    24     1.32     16  0.562  -0.333                 9\n# ℹ 1 more variable: approx_response &lt;chr&gt;\n\n\nNow let’s try to fuzzy join this with the OS data set, based on the baseline SLD and the treatment arm. In addition, we know that the last time point in the tumor data needs to be before the OS time point. We will also check that the best overall response is the same in both data sets. There will be some errors here: For example, we don’t know whether there were new lesions leading to a progression assessment, or whether the patient had later better results. So we need to expect that the matching will not be perfect.\n\n\nShow the code\nos_data_keys &lt;- os_data |&gt; \n    select(id, arm, sld, pfs_time, os_time, response)\nhead(os_data_keys)\n\n\n# A tibble: 6 × 6\n  id    arm         sld pfs_time os_time response\n  &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n1 318   Docetaxel 152      0.107   0.433 PD      \n2 1088  Docetaxel  36      0.194   0.402 PD      \n3 345   Docetaxel  92      0.162   0.162 &lt;NA&gt;    \n4 588   Docetaxel  36      1.02    2.05  SD      \n5 306   MPDL3280A  86      0.118   0.498 PD      \n6 718   Docetaxel  45.7    0.712   1.60  SD      \n\n\nShow the code\ndist_match &lt;- function(v1, v2) {\n    dist &lt;- abs(v1 - v2)\n    data.frame(include = (dist &lt;= 0.05))\n}\n\nless_match &lt;- function(lower, upper) {\n    data.frame(include = lower &lt;= upper)\n}\n\ntumor_os_data_joined &lt;- tumor_data_summary |&gt; \n    fuzzy_left_join(\n        os_data_keys,\n        by = c(\n          \"arm\" = \"arm\", \n          \"bsld\" = \"sld\",          \n          \"last_year\" = \"os_time\",\n          \"approx_response\" = \"response\"\n        ),\n        match_fun = list(\n          `==`, \n          dist_match, \n          less_match,\n          `==`\n        )\n    )\nnrow(tumor_os_data_joined)\n\n\n[1] 954\n\n\nWe see that we cannot match the two data sets exactly:\n\nWe have less patients to start with in the tumor size data set.\nWe have patients in the tumor size data set which match multiple patients in the OS data set based on the treatment arm and the baseline SLD, and also the last measurement time point cannot disambiguate them.\nWe have patients in the tumor size data set which do not match any patient in the OS data set.\n\nHowever, for the purpose of this training we don’t need to be able to recover the true full data set. Therefore we will just subset the matched data set to a reasonable combination of the two data sets:\n\n\nShow the code\ntumor_os_data_joined_subset &lt;- tumor_os_data_joined |&gt; \n    na.omit() |&gt; \n    filter(!duplicated(id.y)) |&gt;\n    filter(!duplicated(id.x))\nnrow(tumor_os_data_joined_subset)\n\n\n[1] 203\n\n\nNow let’s save the joining information:\n\n\nShow the code\ntgi_os_join_keys &lt;- tumor_os_data_joined_subset |&gt; \n    select(id.x, id.y, arm.x) |&gt; \n    rename(\n        id_tgi = id.x,\n        id_os = id.y,\n        arm = arm.x\n    )\ntable(tgi_os_join_keys$arm)\n\n\n\nDocetaxel MPDL3280A \n       96       107 \n\n\nSo we have about 100 patients in each arm.\nWe subset the two data sets accordingly:\n\n\nShow the code\ntumor_data &lt;- tumor_data |&gt; \n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_tgi\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id) |&gt; \n    rename(id = id_os)\n\nos_data &lt;- os_data |&gt;\n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_os\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id_tgi)",
    "crumbs": [
      "Session 3: TGI-OS",
      "0. Setup and data preparation"
    ]
  }
]