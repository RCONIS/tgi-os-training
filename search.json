[
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html",
    "href": "session-tgi/1_tgi_sf_brms.html",
    "title": "1. TGI model minimal workflow with brms",
    "section": "",
    "text": "The purpose of this document is to show a minimal workflow for fitting a TGI model using the brms package.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#setup-and-load-data",
    "href": "session-tgi/1_tgi_sf_brms.html#setup-and-load-data",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#stein-fojo-model",
    "href": "session-tgi/1_tgi_sf_brms.html#stein-fojo-model",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Stein-Fojo model",
    "text": "Stein-Fojo model\nWe start from the Stein-Fojo model as shown in the slides:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\{\\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) + \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) - 1\\}\n\\]\n\nMean\nWe will make one more tweak here. If the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\{\\exp(-\\psi_{k_{s}i} \\cdot t_{ij}) + \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) - 1\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\n\n\nLikelihood\nFor the likelihood given the mean SLD \\(y^{*}\\), we will assume a normal distribution with a constant coefficient of variation \\(\\tau\\):\n\\[\ny(t_{ij}) \\sim \\text{N}(y^{*}(t_{ij}), y^{*}(t_{ij})\\tau)\n\\]\nNote that for consistency with the brms and Stan convention, here we denote the standard deviation as the second parameter of the normal distribution. So in this case, the variance would be \\((y^{*}(t_{ij})\\tau)^2\\).\nThis can also be written as:\n\\[\ny(t_{ij}) = (1 + \\epsilon_{ij}) \\cdot y^{*}(t_{ij})\n\\]\nwhere \\(\\epsilon_{ij} \\sim \\text{N}(0, \\tau)\\).\nNote that also the additive model is a possible choice, where\n\\[\ny(t_{ij}) = y^{*}(t_{ij}) + \\epsilon_{ij}\n\\]\nsuch that the error does not depend on the scale of the SLD any longer.\n\n\nRandom effects\nNext, we define the distributions of the random effects \\(\\psi_{b_{0}i}\\), \\(\\psi_{k_{s}i}\\), \\(\\psi_{k_{g}i}\\), for \\(i = 1, \\dotsc, n\\):\n\\[\n\\begin{align*}\n\\psi_{b_{0}i} &\\sim \\text{LogNormal}(\\mu_{b_{0}}, \\omega_{0}) \\\\\n\\psi_{k_{s}i} &\\sim \\text{LogNormal}(\\mu_{k_{s}}, \\omega_{s}) \\\\\n\\psi_{k_{g}i} &\\sim \\text{LogNormal}(\\mu_{k_{g}}, \\omega_{g})\n\\end{align*}\n\\]\nThis can be rewritten as:\n\\[\n\\begin{align*}\n\\psi_{b_{0}i} &= \\exp(\\mu_{b_{0}} + \\omega_{0} \\cdot \\eta_{b_{0}i}) \\\\\n\\psi_{k_{s}i} &= \\exp(\\mu_{k_{s}} + \\omega_{s} \\cdot \\eta_{k_{s}i}) \\\\\n\\psi_{k_{g}i} &= \\exp(\\mu_{k_{g}} + \\omega_{g} \\cdot \\eta_{k_{g}i})\n\\end{align*}\n\\]\nwhere \\(\\eta_{b_{0}i}\\), \\(\\eta_{k_{s}i}\\), \\(\\eta_{k_{g}i}\\) are the standard normal distributed random effects.\nThis is important for two reasons:\n\nThis parametrization can help the sampler to converge faster. See here for more information.\nThis shows a bit more explicitly that the population mean of the random effects is not equal to the \\(\\mu\\) parameter, but to \\(\\theta = \\exp(\\mu + \\omega^2 / 2)\\), because they are log-normally distributed (see e.g. Wikipedia for the formulas). This is important for the interpretation of the parameter estimates.\n\n\n\nPriors\nFinally, we need to define the priors for the hyperparameters.\nThere are different principles we could use to define these priors:\n\nNon-informative priors: We could use priors that are as non-informative as possible. This is especially useful if we do not have any prior knowledge about the parameters. For example, we could use normal priors with a large standard deviation for the population means of the random effects.\nInformative priors: If we have some prior knowledge about the parameters, we can use this to define the priors. For example, if we have literature data about the \\(k_g\\) parameter estimate, we could use this to define the prior for the population mean of the growth rate. (Here we just need to be careful to consider the time scale and the log-normal distribution, as mentioned above)\n\nHere we use relatively informative priors for the log-normal location parameters, motivated by prior analyses of the same study:\n\\[\n\\begin{align*}\n\\mu_{b_{0}} &\\sim \\text{Normal}(\\log(65), 1) \\\\\n\\mu_{k_{s}} &\\sim \\text{Normal}(\\log(0.52), 0.1) \\\\\n\\mu_{k_{g}} &\\sim \\text{Normal}(\\log(1.04), 1)\n\\end{align*}\n\\]\nFor all standard deviations we use truncated normal priors:\n\\[\n\\begin{align*}\n\\omega_{0} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\omega_{s} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\omega_{g} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\tau &\\sim \\text{PositiveNormal}(0, 3)\n\\end{align*}\n\\]\nwhere \\(\\text{PositiveNormal}(0, 3)\\) denotes a truncated normal distribution with mean \\(0\\) and standard deviation \\(3\\), truncated to the positive real numbers.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#fit-model",
    "href": "session-tgi/1_tgi_sf_brms.html#fit-model",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood:\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (exp(-ks * year) + exp(kg * year) -  1)) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # Define the standard deviation (called sigma in brms) as a \n  # coefficient tau times the mean ystar.\n  # sigma is modelled on the log scale though, therefore:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  # This line is needed to declare tau as a model parameter:\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + (1 | id)) +\n  lf(lkg ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\"),\n  prior(normal(log(1.04), 1), nlpar = \"lkg\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lks = array(-1.25),\n  b_lkg = array(-1.33),\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/fit9.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsummary(fit)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.00      689     1248\nsd(lks_Intercept)     1.40      0.08     1.25     1.55 1.00      702     1586\nsd(lkg_Intercept)     0.96      0.04     0.88     1.05 1.01      902     1750\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.16      0.00     0.16     0.17 1.00     2250     2892\nlb0_Intercept     3.62      0.02     3.57     3.66 1.01      420      723\nlks_Intercept    -0.86      0.08    -1.01    -0.72 1.00     1661     2415\nlkg_Intercept    -1.20      0.07    -1.34    -1.08 1.01      676     1419\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote that it is crucial to use here int_step() to properly define the two pieces of the linear predictor for negative and non-negative time values: If you used step() like I did for a few days, then you will have the wrong model! This is because in Stan, step(false) = step(0) = 1 and not 0 as you would expect. Only int_step(0) = 0 as we need it here.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#parameter-estimates",
    "href": "session-tgi/1_tgi_sf_brms.html#parameter-estimates",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nHere we extract all parameter estimates using the as_draws_df method for brmsfit objects. Note that we could also just extract a subset of parameters, see ?as_draws.brmsfit for more information.\nAs mentioned above, we use the expectation of the log-normal distribution to get the population level estimates (called \\(\\theta\\) with the corresponding subscript) for the \\(b_0\\), \\(k_s\\), and \\(k_g\\) parameters. We also calculate the coefficient of variation for each parameter.\n\n\nShow the code\npost_df &lt;- as_draws_df(fit)\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"r_id__lb0[1,Intercept]\" \"r_id__lb0[2,Intercept]\"\n[10] \"r_id__lb0[3,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nFor a graphical check of the convergence, we can use the mcmc_trace and mcmc_pairs functions from the bayesplot package:\n\n\nShow the code\nsf_pop_params &lt;- c(\"theta_b0\", \"theta_ks\", \"theta_kg\", \"omega_0\", \"omega_s\", \"omega_g\", \"sigma\")\n\nmcmc_trace(post_df, pars = sf_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = sf_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nNext, we can create a nice summary table of the parameter estimates, using summarize_draws from the posterior package in combination with functions from the gt package:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  select(theta_b0, theta_ks, theta_kg, omega_0, omega_s, omega_g, cv_0, cv_s, cv_g, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.0\n44.0\n1.03\n1.01\n42.3\n45.7\n1.00\n423\n726\n\n\ntheta_ks\n1.13\n1.12\n0.104\n0.101\n0.972\n1.32\n1.00\n445\n915\n\n\ntheta_kg\n0.476\n0.475\n0.0311\n0.0318\n0.426\n0.528\n1.00\n941\n1,320\n\n\nomega_0\n0.579\n0.579\n0.0162\n0.0166\n0.552\n0.605\n1.00\n676\n1,220\n\n\nomega_s\n1.40\n1.40\n0.0753\n0.0745\n1.28\n1.52\n1.00\n699\n1,560\n\n\nomega_g\n0.958\n0.956\n0.0446\n0.0446\n0.888\n1.04\n1.00\n882\n1,720\n\n\ncv_0\n0.631\n0.631\n0.0208\n0.0212\n0.597\n0.665\n1.00\n676\n1,220\n\n\ncv_s\n2.49\n2.46\n0.311\n0.295\n2.02\n3.04\n1.00\n699\n1,560\n\n\ncv_g\n1.23\n1.22\n0.0883\n0.0869\n1.10\n1.39\n1.00\n882\n1,720\n\n\nsigma\n0.161\n0.161\n0.00230\n0.00229\n0.157\n0.165\n1.00\n2,190\n2,860\n\n\n\n\n\n\n\nWe can also look at parameters for individual patients. This is simplified by using the spread_draws() function:\n\n\nShow the code\n# Understand the names of the random effects:\nhead(get_variables(fit), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"r_id__lb0[1,Intercept]\" \"r_id__lb0[2,Intercept]\"\n[10] \"r_id__lb0[3,Intercept]\"\n\n\nShow the code\npost_ind &lt;- fit |&gt; \n  spread_draws(\n    b_lb0_Intercept,\n    r_id__lb0[id, ],\n    b_lks_Intercept,\n    r_id__lks[id, ],\n    b_lkg_Intercept,\n    r_id__lkg[id, ]\n  ) |&gt; \n  mutate(\n    b0 = exp(b_lb0_Intercept + r_id__lb0),\n    ks = exp(b_lks_Intercept + r_id__lks),\n    kg = exp(b_lkg_Intercept + r_id__lkg)\n  ) |&gt; \n  select(\n    .chain, .iteration, .draw,\n    id, b0, ks, kg    \n  )\n\n\nNote that here we do not need to use the log-normal expectation formula, because we just calculate the individual random effects here, in contrast to the population level parameters above.\nWith this we can e.g. report the estimates for the first patient:\n\n\nShow the code\npost_sum_id1 &lt;- post_ind |&gt;\n  filter(id == \"1\") |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_sum_id1\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\n1\n\n\nb0\n43.3\n42.8\n4.80\n4.35\n36.5\n52.2\n1.00\n9,080\n2,890\n\n\nks\n0.486\n0.313\n0.529\n0.309\n0.0392\n1.50\n1.00\n6,300\n3,140\n\n\nkg\n0.395\n0.294\n0.338\n0.241\n0.0620\n1.04\n1.00\n7,690\n3,080",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#observation-vs-model-fit",
    "href": "session-tgi/1_tgi_sf_brms.html#observation-vs-model-fit",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again.\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_linpred_draws(fit) |&gt; \n  median_qi() |&gt; \n  mutate(sld_pred = .linpred)\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = sld_pred, ymin = .lower, ymax = .upper),\n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = sld_pred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"SF model fit\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#summary-statistics",
    "href": "session-tgi/1_tgi_sf_brms.html#summary-statistics",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Summary statistics",
    "text": "Summary statistics\nThe time-to-growth formula is:\n\\[\n\\max \\left(\n    \\frac{\n        \\log(k_s) - \\log(k_g)\n    }{\n        k_s + k_g\n    },\n    0\n\\right)\n\\]\nSimilary, we can look at the tumor-ratio at time \\(t\\):\n\\[\n\\frac{y^{*}(t)}{y^{*}(0)} = \\exp(-k_s \\cdot t) + \\exp(k_g \\cdot t) - 1\n\\]\nSo with the posterior samples from above, we can calculate these statistics, separate for each patient, with the tumor-ratio e.g. for 12 weeks (and being careful with the year time scale we used in this model):\n\n\nShow the code\npost_ind_stat &lt;- post_ind |&gt; \n  mutate(\n    ttg = pmax((log(ks) - log(kg)) / (ks + kg), 0),\n    tr12 = exp(-ks * 12/52) + exp(kg * 12/52) - 1\n  )\n\n\nThen we can look at e.g. the first 3 patients parameter estimates:\n\n\nShow the code\npost_ind_stat_sum &lt;- post_ind_stat |&gt;\n  filter(id %in% c(\"1\", \"2\", \"3\")) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_ind_stat_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\n1\n\n\nb0\n43.3\n42.8\n4.80\n4.35\n36.5\n52.2\n1.00\n9,080\n2,890\n\n\nks\n0.486\n0.313\n0.529\n0.309\n0.0392\n1.50\n1.00\n6,300\n3,140\n\n\nkg\n0.395\n0.294\n0.338\n0.241\n0.0620\n1.04\n1.00\n7,690\n3,080\n\n\nttg\n0.785\n0.106\n1.36\n0.158\n0\n3.47\n1.00\n4,440\n3,180\n\n\ntr12\n0.999\n1.00\n0.115\n0.0834\n0.809\n1.18\n1.00\n5,740\n3,400\n\n\n2\n\n\nb0\n37.9\n37.6\n4.10\n3.79\n31.9\n45.1\n1.00\n7,520\n2,750\n\n\nks\n0.396\n0.246\n0.491\n0.237\n0.0355\n1.21\n1.00\n5,330\n3,380\n\n\nkg\n0.548\n0.416\n0.445\n0.357\n0.0735\n1.42\n1.00\n5,860\n2,870\n\n\nttg\n0.501\n0\n1.21\n0\n0\n2.92\n0.999\n3,500\n2,820\n\n\ntr12\n1.06\n1.04\n0.125\n0.0939\n0.886\n1.29\n1.00\n5,810\n3,220\n\n\n3\n\n\nb0\n21.0\n20.7\n2.49\n2.23\n17.6\n25.6\n1.00\n4,310\n2,900\n\n\nks\n1.00\n0.817\n0.662\n0.541\n0.229\n2.34\n1.00\n2,930\n2,760\n\n\nkg\n0.275\n0.249\n0.165\n0.185\n0.0529\n0.575\n1.00\n3,480\n2,790\n\n\nttg\n1.50\n1.01\n1.39\n0.681\n0.418\n4.27\n1.00\n4,440\n2,740\n\n\ntr12\n0.869\n0.884\n0.0815\n0.0720\n0.709\n0.980\n1.00\n3,170\n2,920",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#adding-covariates",
    "href": "session-tgi/1_tgi_sf_brms.html#adding-covariates",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Adding covariates",
    "text": "Adding covariates\nWith brms it is straightforward to add covariates to the model. In this example, an obvious choice for a covariate is the treatment the patients received in the study:\n\n\nShow the code\ndf |&gt; \n  select(id, arm) |&gt; \n  distinct() |&gt;\n  pull(arm) |&gt; \n  table()\n\n\n\n  1   2 \n325 376 \n\n\nHere it makes sense to assume that only the shrinkage and growth parameters differ systematically between the two arms. For example, the baseline SLD should be the same in expectation, because of the randomization between the treatment arms.\nTherefore we can add this binary arm covariate as follows as a fixed effect for the two population level mean parameters \\(\\mu_{k_s}\\) and \\(\\mu_{k_g}\\) on the log scale:\n\n\nShow the code\nformula_by_arm &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood:\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (exp(-ks * year) + exp(kg * year) -  1)) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # As above we also use these formulas here:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + arm + (1 | id)) +\n  lf(lkg ~ 1 + arm + (1 | id))\n\n\nIt is instructive to see that the Stan code that is generated in the background is actually identical to the previous model, except for the prior specification. This is because brms uses a design matrix to model the fixed effects, and the arm variable is just added to this design matrix, which before only contained the intercept column with 1s.\nHowever, now the coefficient vector is no longer of length 1 but of length 2, with the first element corresponding to the intercept and the second element to the effect of the arm variable (for the level “2”). For the latter, we want to assume a standard normal prior on the log scale.\nSo we need to adjust the prior and initial values accordingly:\n\n\nShow the code\npriors_by_arm &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  # Note the changes here:\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\", coef = \"Intercept\"),\n  prior(normal(0, 1), nlpar = \"lks\", coef = \"arm2\"), \n  prior(normal(log(1.04), 1), nlpar = \"lkg\", coef = \"Intercept\"),\n  prior(normal(0, 1), nlpar = \"lkg\", coef = \"arm2\"),\n  # Same as before:\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\ninits_by_arm &lt;- list(\n  b_lb0 = array(3.61),\n  # Note the changes here:\n  b_lks = array(c(-1.25, 0)),\n  b_lkg = array(c(-1.33, 0)),\n  # Same as before:\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n\nNow we can fit the model as before:\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/fit10.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit_by_arm &lt;- brm(\n    formula = formula_by_arm,\n    data = df,\n    prior = priors_by_arm,\n    family = gaussian(),\n    init = rep(list(inits_by_arm), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit_by_arm, file = save_file)\n}\nsummary(fit_by_arm)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + arm + (1 | id)\n         lkg ~ 1 + arm + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.01      356      987\nsd(lks_Intercept)     1.48      0.09     1.31     1.66 1.01      402      917\nsd(lkg_Intercept)     0.97      0.05     0.88     1.07 1.01      323      789\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.16      0.00     0.16     0.17 1.00     1725     2755\nlb0_Intercept     3.61      0.02     3.57     3.66 1.00      199      519\nlks_Intercept    -0.78      0.09    -0.95    -0.61 1.00     1204     2303\nlks_arm2         -0.40      0.16    -0.73    -0.09 1.01      383      823\nlkg_Intercept    -1.26      0.11    -1.48    -1.05 1.01      494      853\nlkg_arm2          0.03      0.13    -0.22     0.28 1.01      460      975\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s have a look at the parameter estimates then:\n\n\nShow the code\npost_df_by_arm &lt;- as_draws_df(fit_by_arm)\nhead(names(post_df_by_arm), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lks_arm2\"             \"b_lkg_Intercept\"        \"b_lkg_arm2\"            \n [7] \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"   \"sd_id__lkg_Intercept\"  \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df_by_arm &lt;- post_df_by_arm |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks_arm1 = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_ks_arm2 = exp(b_lks_Intercept + b_lks_arm2 + sd_id__lks_Intercept^2 / 2),\n    theta_kg_arm1 = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_kg_arm2 = exp(b_lkg_Intercept + b_lkg_arm2 + sd_id__lkg_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\npost_sum_by_arm &lt;- post_df_by_arm |&gt;\n  select(\n    theta_b0, theta_ks_arm1, theta_ks_arm2, theta_kg_arm1, theta_kg_arm2, \n    omega_0, omega_s, omega_g, cv_0, cv_s, cv_g, sigma\n  ) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum_by_arm\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.9\n43.9\n1.10\n1.11\n42.1\n45.8\n1.00\n195\n515\n\n\ntheta_ks_arm1\n1.39\n1.36\n0.193\n0.178\n1.11\n1.74\n1.01\n396\n841\n\n\ntheta_ks_arm2\n0.925\n0.918\n0.112\n0.108\n0.755\n1.12\n1.00\n306\n814\n\n\ntheta_kg_arm1\n0.456\n0.454\n0.0461\n0.0456\n0.382\n0.535\n1.01\n575\n1,000\n\n\ntheta_kg_arm2\n0.469\n0.468\n0.0425\n0.0423\n0.402\n0.544\n1.00\n496\n923\n\n\nomega_0\n0.578\n0.578\n0.0161\n0.0160\n0.552\n0.605\n1.00\n345\n979\n\n\nomega_s\n1.48\n1.48\n0.0909\n0.0899\n1.33\n1.63\n1.01\n397\n900\n\n\nomega_g\n0.968\n0.965\n0.0485\n0.0474\n0.892\n1.05\n1.00\n343\n808\n\n\ncv_0\n0.630\n0.630\n0.0206\n0.0205\n0.597\n0.665\n1.00\n345\n979\n\n\ncv_s\n2.84\n2.79\n0.445\n0.416\n2.21\n3.65\n1.01\n397\n900\n\n\ncv_g\n1.25\n1.24\n0.0976\n0.0929\n1.10\n1.43\n1.00\n343\n808\n\n\nsigma\n0.161\n0.161\n0.00229\n0.00230\n0.157\n0.165\n1.00\n1,700\n2,710\n\n\n\n\n\n\n\nSo we see that the shrinkage is stronger in arm 1 compared to arm 2, while the growth rates are similar. We could also calculate the posterior probability that the growth rate is different between the two arms, for example:\n\n\nShow the code\npost_df_by_arm |&gt;\n  mutate(diff_pos = theta_ks_arm1 - theta_ks_arm2 &gt; 0) |&gt;\n  summarize(diff_prob = mean(diff_pos))\n\n\n# A tibble: 1 × 1\n  diff_prob\n      &lt;dbl&gt;\n1     0.996\n\n\nSo we see that the posterior probability that the shrinkage rate is higher in arm 1 compared to arm 2 is quite high.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#model-comparison-with-loo",
    "href": "session-tgi/1_tgi_sf_brms.html#model-comparison-with-loo",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Model comparison with LOO",
    "text": "Model comparison with LOO\nThe LOO criterion is a widely used method for comparing models. It is based on the idea of leave-one-out cross-validation, but is more efficient to compute.\nWith brms, it is easy to compute the LOO criterion:\n\n\nShow the code\nloo(fit)\n\n\nWarning: Found 374 observations with a pareto_k &gt; 0.7 in model 'fit'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\n\nComputed from 4000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12990.0  97.3\np_loo      1185.2  40.4\nlooic     25980.0 194.7\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.1, 2.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     3725  90.9%   56      \n   (0.7, 1]   (bad)       303   7.4%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   71   1.7%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nA helpful glossary explaining the LOO statistics can be found here.\nDifferent criteria are available, for example the elpd_loo is the expected log pointwise predictive density, and the looic = -2 * elpd_loo is the LOO information criterion. For comparing models, the looic is often used, where smaller numbers are better. it is kind of an equivalent to the AIC, but based on the LOO criterion.\nLet’s e.g. compare the two models we fitted above, one for the whole dataset and one with the treatment arm as a covariate for the shrinkage and growth rates:\n\n\nShow the code\nfit &lt;- add_criterion(fit, \"loo\")\n\n\nWarning: Found 374 observations with a pareto_k &gt; 0.7 in model 'fit'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nShow the code\nfit_by_arm &lt;- add_criterion(fit_by_arm, \"loo\")\n\n\nWarning: Found 359 observations with a pareto_k &gt; 0.7 in model 'fit_by_arm'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nShow the code\nloo_compare(fit, fit_by_arm)\n\n\n           elpd_diff se_diff\nfit         0.0       0.0   \nfit_by_arm -0.2       5.9   \n\n\nSo the model without treatment arm seems to be very slightly preferred here by the LOO criterion. Nevertheless, the model with treatment arm might answer exactly the question we need to answer, so it is important to consider the context of the analysis.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#tips-and-tricks",
    "href": "session-tgi/1_tgi_sf_brms.html#tips-and-tricks",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Tips and tricks",
    "text": "Tips and tricks\n\nWhen the model breaks down completely, that is a sign that likely the model specification is wrong. For example, in above code I first did not understand how to model the sigma parameter correctly, which led to a model where each chain was completely stuck at its initial values.\nIn that case and in general if you are not sure whether the brms model specification is correct, you can check the Stan code that is generated by brms:\n\n\nShow the code\n# Good to check the stan code:\n# (This also helps to find the names of the parameters\n# for which to define the initial values below)\nstancode(formula, prior = priors, data = df, family = gaussian())\n\n\n// generated with brms 2.22.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=1&gt; K_tau;  // number of population-level effects\n  matrix[N, K_tau] X_tau;  // population-level design matrix\n  int&lt;lower=1&gt; K_lb0;  // number of population-level effects\n  matrix[N, K_lb0] X_lb0;  // population-level design matrix\n  int&lt;lower=1&gt; K_lks;  // number of population-level effects\n  matrix[N, K_lks] X_lks;  // population-level design matrix\n  int&lt;lower=1&gt; K_lkg;  // number of population-level effects\n  matrix[N, K_lkg] X_lkg;  // population-level design matrix\n  // covariates for non-linear functions\n  vector[N] C_ystar_1;\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_lb0_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_lks_1;\n  // data for group-level effects of ID 3\n  int&lt;lower=1&gt; N_3;  // number of grouping levels\n  int&lt;lower=1&gt; M_3;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_3;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_3_lkg_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector&lt;lower=0&gt;[K_tau] b_tau;  // regression coefficients\n  vector[K_lb0] b_lb0;  // regression coefficients\n  vector[K_lks] b_lks;  // regression coefficients\n  vector[K_lkg] b_lkg;  // regression coefficients\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_3] sd_3;  // group-level standard deviations\n  array[M_3] vector[N_3] z_3;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_lb0_1;  // actual group-level effects\n  vector[N_2] r_2_lks_1;  // actual group-level effects\n  vector[N_3] r_3_lkg_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_lb0_1 = (sd_1[1] * (z_1[1]));\n  r_2_lks_1 = (sd_2[1] * (z_2[1]));\n  r_3_lkg_1 = (sd_3[1] * (z_3[1]));\n  lprior += normal_lpdf(b_tau | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(b_lb0 | log(65), 1);\n  lprior += normal_lpdf(b_lks | log(0.52), 0.1);\n  lprior += normal_lpdf(b_lkg | log(1.04), 1);\n  lprior += normal_lpdf(sd_1 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(sd_2 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(sd_3 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] nlp_tau = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lb0 = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lks = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lkg = rep_vector(0.0, N);\n    // initialize non-linear predictor term\n    vector[N] nlp_b0;\n    // initialize non-linear predictor term\n    vector[N] nlp_ks;\n    // initialize non-linear predictor term\n    vector[N] nlp_kg;\n    // initialize non-linear predictor term\n    vector[N] nlp_ystar;\n    // initialize non-linear predictor term\n    vector[N] mu;\n    // initialize non-linear predictor term\n    vector[N] sigma;\n    nlp_tau += X_tau * b_tau;\n    nlp_lb0 += X_lb0 * b_lb0;\n    nlp_lks += X_lks * b_lks;\n    nlp_lkg += X_lkg * b_lkg;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lb0[n] += r_1_lb0_1[J_1[n]] * Z_1_lb0_1[n];\n    }\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lks[n] += r_2_lks_1[J_2[n]] * Z_2_lks_1[n];\n    }\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lkg[n] += r_3_lkg_1[J_3[n]] * Z_3_lkg_1[n];\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_b0[n] = (exp(nlp_lb0[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_ks[n] = (exp(nlp_lks[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_kg[n] = (exp(nlp_lkg[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_ystar[n] = (int_step(C_ystar_1[n] &gt; 0) * (nlp_b0[n] * (exp( - nlp_ks[n] * C_ystar_1[n]) + exp(nlp_kg[n] * C_ystar_1[n]) - 1)) + int_step(C_ystar_1[n] &lt;= 0) * (nlp_b0[n] * exp(nlp_kg[n] * C_ystar_1[n])));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      mu[n] = (nlp_ystar[n]);\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      sigma[n] = exp(log(nlp_tau[n]) + log(nlp_ystar[n]));\n    }\n    target += normal_lpdf(Y | mu, sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n  target += std_normal_lpdf(z_3[1]);\n}\ngenerated quantities {\n}\n\n\nHere e.g. it is important to see that the sigma parameter is modelled on the log scale.\nWe can also extract the actual data that is passed to the Stan program:\n\n\nShow the code\n# Extract the data that is passed to the Stan program\nstan_data &lt;- standata(formula, prior = priors, data = df, family = gaussian())\n\n# Check that the time variable is correct\nall(stan_data$C_eta_1 == df$year)\n\n\n[1] TRUE\n\n\nThis can be useful to check if the data is passed correctly to the Stan program.\nIt is important to take divergence warnings seriously. For the model parameters where Rhat is larger than 1.05 or so, you can just have a look at the traceplot. For example, in an earlier model fit the \\(\\log(\\mu_{\\phi})\\) population parameter had this traceplot:\n\n\nShow the code\nfit_save &lt;- fit\nload(here(\"session-tgi/fit.RData\"))\nplot(fit, pars = \"b_tphi\")\n\n\nWarning: Argument 'pars' is deprecated. Please use 'variable' instead.\n\n\n\n\n\n\n\n\n\nShow the code\nfit &lt;- fit_save\n\n\nWe see that two chains led to a \\(\\log(\\mu_{\\phi})\\) value below 0, while two chains hovered around much larger values between 5 and 15, which corresponds to \\(\\mu_{\\phi} = 1\\). This shows that the model is rather overparametrized. By restricting the range of the prior for \\(\\log(\\mu_{\\phi})\\) to be below 0, we could avoid this problem.\nProviding manually realistic initial values for the model parameters can help to speed up the convergence of the Markov chains. This is especially important for the nonlinear parameters. You can get the names of the parameters from the Stan code above - note that you cannot use the names of the brms R code.\nSometimes it is not easy to get the dimensions of the inital values right. Here it can be helpful to change the backend to rstan, which provides more detailed error messages. You can do this by setting backend = \"rstan\" in the brm call.\nWhen the Stan compiler gives you an error, that can be very informative: Just open the Stan code file that is mentioned in the error message and look at the line number that is mentioned. This can give you a good idea of what went wrong: Maybe a typo in the prior definition, or a missing semicolon, or a wrong dimension in the data block. With VScode e.g. this is very easy: Just hold Ctrl and click on the file name and number, and you will be taken to the right line in the Stan code.\nIn order to quickly get results for a model, e.g. for obtaining useful starting values or prior distributions, it can be worth trying the “Pathfinder” algorithm in brms. Pathfinder is a variational method for approximately sampling from differentiable log densities. You can use it by setting algorithm = \"pathfinder\" in the brm call. In this example it takes only a minute, compared to more than an hour for the full model fit. However, the results are still quite different, so it is likely only useful as a first approximation. Nevertheless, the individual model fits look quite encouraging, in the sense that they have found good parameter values - but they are still lacking any uncertainty, so could not be used for confidence or prediction intervals:\n\n\nShow the code\nfit_fast_file &lt;- here(\"session-tgi/fit_fast.RData\")\nif (file.exists(fit_fast_file)) {\n  load(fit_fast_file)\n} else {\n  fit_fast &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH,\n    algorithm = \"pathfinder\"\n  )\n  save(fit_fast, file = fit_fast_file)\n}\n\nsummary(fit_fast)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ ystar \n         ystar ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(ystar)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 1 chains, each with iter = 1000; warmup = 0; thin = 1;\n         total post-warmup draws = 1000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     3.37      0.00     3.37     3.37   NA       NA       NA\nsd(lks_Intercept)     9.16      0.00     9.16     9.16   NA       NA       NA\nsd(lkg_Intercept)     6.90      0.00     6.90     6.90   NA       NA       NA\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.13      0.00     0.13     0.13   NA       NA       NA\nlb0_Intercept     3.62      0.00     3.62     3.62   NA       NA       NA\nlks_Intercept    -0.49      0.00    -0.49    -0.49   NA       NA       NA\nlkg_Intercept    -0.66      0.00    -0.66    -0.66   NA       NA       NA\n\nDraws were sampled using pathfinder(). \n\n\nShow the code\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_linpred_draws(fit_fast) |&gt; \n  median_qi() |&gt; \n  mutate(sld_pred = .linpred)\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = sld_pred, ymin = .lower, ymax = .upper),\n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = sld_pred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"SF model fit (pathfinder approximation)\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html",
    "href": "session-tgi/2_tgi_sf_jmpost.html",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "",
    "text": "Let’s try to fit the same model now with jmpost. We will use the same data as in the previous notebook.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#setup-and-load-data",
    "href": "session-tgi/2_tgi_sf_jmpost.html#setup-and-load-data",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#data-preparation",
    "href": "session-tgi/2_tgi_sf_jmpost.html#data-preparation",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Data preparation",
    "text": "Data preparation\nWe start with the subject level data set. For the beginning, we want to treat all observations as if they come from a single arm and single study for now. Therefore we insert constant study and arm values here.\n\n\nShow the code\nsubj_df &lt;- data.frame(\n  id = unique(df$id),\n  arm = \"arm\",\n  study = \"study\"\n)\nsubj_data &lt;- DataSubject(\n  data = subj_df,\n  subject = \"id\",\n  arm = \"arm\",\n  study = \"study\"\n)\n\n\nNext we prepare the longitudinal data object.\n\n\nShow the code\nlong_df &lt;- df |&gt;\n  select(id, year, sld)\nlong_data &lt;- DataLongitudinal(\n  data = long_df,\n  formula = sld ~ year\n)\n\n\nNow we can create the JointData object:\n\n\nShow the code\njoint_data &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data\n)",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#model-specification",
    "href": "session-tgi/2_tgi_sf_jmpost.html#model-specification",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Model specification",
    "text": "Model specification\nThe statistical model is specified in the jmpost vignette here.\nHere we just want to fit the longitudinal data, therefore:\n\n\nShow the code\ntgi_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    )\n)\n\n\nNote that the priors on the standard deviations, omega_* and sigma, are truncated to the positive domain. So we used here truncated normal priors.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#fit-model",
    "href": "session-tgi/2_tgi_sf_jmpost.html#fit-model",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using jmpost.\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/jm5.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  mcmc_results &lt;- sampleStanModel(\n      tgi_mod,\n      data = joint_data,\n      iter_sampling = ITER,\n      iter_warmup = WARMUP,\n      chains = CHAINS,\n      parallel_chains = CHAINS,\n      thin = CHAINS,\n      seed = BAYES.SEED,\n      refresh = REFRESH\n  )\n  save(mcmc_results, file = save_file)\n}\n\n\nLet’s check the convergence of the population parameters:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nsave_overall_file &lt;- here(\"session-tgi/jm5_more.RData\")\nif (file.exists(save_overall_file)) {\n  load(save_overall_file)\n} else {\n  mcmc_res_cmdstan &lt;- cmdstanr::as.CmdStanMCMC(mcmc_results)\n  mcmc_res_sum &lt;- mcmc_res_cmdstan$summary(vars)\n  vars_draws &lt;- mcmc_res_cmdstan$draws(vars)\n  loo_res &lt;- mcmc_res_cmdstan$loo(r_eff = FALSE)\n  save(mcmc_res_sum, vars_draws, loo_res, file = save_overall_file)\n}\nmcmc_res_sum\n\n\n# A tibble: 7 × 10\n  variable     mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lm_sf_mu_…  3.61   3.61  0.0236  0.0232   3.57   3.65   1.02     142.     188.\n2 lm_sf_mu_… -1.27  -1.27  0.160   0.152   -1.54  -1.03   1.01     352.     500.\n3 lm_sf_mu_… -1.35  -1.35  0.0828  0.0789  -1.49  -1.22   1.00     230.     456.\n4 lm_sf_sig…  0.161  0.161 0.00233 0.00244  0.158  0.165  1.00     513.     875.\n5 lm_sf_ome…  0.579  0.578 0.0156  0.0153   0.555  0.608  1.00     341.     576.\n6 lm_sf_ome…  1.62   1.62  0.117   0.112    1.45   1.82   1.01     321.     509.\n7 lm_sf_ome…  0.997  0.993 0.0505  0.0503   0.920  1.08   1.00     390.     595.\n\n\nThis looks good, let’s check the traceplots:\n\n\nShow the code\n# vars_draws &lt;- mcmc_res_cmdstan$draws(vars)\nmcmc_trace(vars_draws)\n\n\n\n\n\n\n\n\n\nThey also look ok, all chains are mixing well in the same range of parameter values.\nAlso here we could look at the pairs plot:\n\n\nShow the code\nmcmc_pairs(\n  vars_draws,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#observation-vs.-model-fit",
    "href": "session-tgi/2_tgi_sf_jmpost.html#observation-vs.-model-fit",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Observation vs. model fit",
    "text": "Observation vs. model fit\nLet’s check the fit of the model to the data:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\n\nsave_fit_file &lt;- here(\"session-tgi/jm5_fit.RData\")\nif (file.exists(save_fit_file)) {\n  load(save_fit_file)\n} else {\n  fit_subset &lt;- LongitudinalQuantities(\n    mcmc_results, \n    grid = GridObserved(subjects = pt_subset)\n  )\n  save(fit_subset, file = save_fit_file)\n}\n\nautoplot(fit_subset)+\n  labs(x = \"Time (years)\", y = \"SLD (mm)\")\n\n\n\n\n\n\n\n\n\nSo this works very nicely.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#prior-vs.-posterior",
    "href": "session-tgi/2_tgi_sf_jmpost.html#prior-vs.-posterior",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Prior vs. posterior",
    "text": "Prior vs. posterior\nLet’s check the prior vs. posterior for the parameters:\n\n\nShow the code\npost_samples &lt;- as_draws_df(vars_draws) |&gt; \n  rename(\n    mu_bsld = \"lm_sf_mu_bsld[1]\",\n    mu_ks = \"lm_sf_mu_ks[1]\",\n    mu_kg = \"lm_sf_mu_kg[1]\",\n    omega_bsld = \"lm_sf_omega_bsld[1]\",\n    omega_ks = \"lm_sf_omega_ks[1]\",\n    omega_kg = \"lm_sf_omega_kg[1]\",\n    sigma = lm_sf_sigma\n  ) |&gt; \n  mutate(type = \"posterior\") |&gt; \n  select(mu_bsld, mu_ks, mu_kg, omega_bsld, omega_ks, omega_kg, sigma, type)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\nn_prior_samples &lt;- nrow(post_samples)\nprior_samples &lt;- data.frame(\n    mu_bsld = rnorm(n_prior_samples, log(65), 1),\n    mu_ks = rnorm(n_prior_samples, log(0.52), 1),\n    mu_kg = rnorm(n_prior_samples, log(1.04), 1),\n    omega_bsld = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    omega_ks = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    omega_kg = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    sigma = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3)\n  ) |&gt; \n  mutate(type = \"prior\")\n\n# Combine the two\ncombined_samples &lt;- rbind(post_samples, prior_samples) |&gt; \n  pivot_longer(cols = -type, names_to = \"parameter\", values_to = \"value\")\n\nggplot(combined_samples, aes(x = value, fill = type)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~parameter, scales = \"free\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis looks good, because the priors are covering the range of the posterior samples and are not too informative.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#parameter-estimates",
    "href": "session-tgi/2_tgi_sf_jmpost.html#parameter-estimates",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nHere we need again to be careful: We are interested in the posterior mean estimates of the baseline, shrinkage and growth population rates on the original scale. Because we model them on the log scale as normal distributed, we need to use the mean of the log-normal distribution to get the mean on the original scale.\n\n\nShow the code\npost_sum &lt;- post_samples |&gt;\n  mutate(\n    theta_b0 = exp(mu_bsld + omega_bsld^2 / 2), \n    theta_ks = exp(mu_ks + omega_ks^2 / 2), \n    theta_kg = exp(mu_kg + omega_kg^2 / 2),\n    cv_0 = sqrt(exp(omega_bsld^2) - 1),\n    cv_s = sqrt(exp(omega_ks^2) - 1),\n    cv_g = sqrt(exp(omega_kg^2) - 1)\n  ) |&gt;\n  select(theta_b0, theta_ks, theta_kg, omega_bsld, omega_ks, omega_kg, cv_0, cv_s, cv_g, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.7\n43.8\n1.10\n1.12\n42.0\n45.5\n1.00\n151\n201\n\n\ntheta_ks\n1.06\n1.05\n0.129\n0.120\n0.871\n1.30\n1.00\n226\n408\n\n\ntheta_kg\n0.428\n0.427\n0.0311\n0.0306\n0.378\n0.479\n1.00\n318\n472\n\n\nomega_bsld\n0.579\n0.578\n0.0156\n0.0153\n0.555\n0.608\n1.00\n346\n545\n\n\nomega_ks\n1.62\n1.62\n0.117\n0.112\n1.45\n1.82\n1.00\n318\n494\n\n\nomega_kg\n0.997\n0.993\n0.0505\n0.0503\n0.920\n1.08\n1.00\n390\n576\n\n\ncv_0\n0.631\n0.630\n0.0200\n0.0196\n0.600\n0.668\n1.00\n346\n545\n\n\ncv_s\n3.70\n3.57\n0.818\n0.685\n2.66\n5.16\n1.00\n318\n494\n\n\ncv_g\n1.31\n1.30\n0.106\n0.103\n1.15\n1.49\n1.00\n390\n576\n\n\nsigma\n0.161\n0.161\n0.00233\n0.00244\n0.158\n0.165\n1.00\n495\n844\n\n\n\n\n\n\n\nWe can see that these are consistent with the estimates from the brms model earlier.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#separate-arm-estimates",
    "href": "session-tgi/2_tgi_sf_jmpost.html#separate-arm-estimates",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Separate arm estimates",
    "text": "Separate arm estimates\nWhile there is no general covariates support in jmpost for the longitudinal models as of now, we can obtain separate estimates for the longitudinal model parameters: As detailed in the model specification here, as soon as we have the arm defined, then separate estimates for the arm-specific shrinkage and growth parameters will be obtained: Both the population means and standard deviation parameters are here arm-specific. (Note that this is slightly different from brms where we assumed earlier the same standard deviation independent of the treatment arm.)\nSo we need to define the subject data accordingly now with the arm information:\n\n\nShow the code\nsubj_df_by_arm &lt;- df |&gt;\n  select(id, arm) |&gt;\n  distinct() |&gt; \n  mutate(study = \"study\")\n\nsubj_data_by_arm &lt;- DataSubject(\n  data = subj_df_by_arm,\n  subject = \"id\",\n  arm = \"arm\",\n  study = \"study\"\n)\n\n\nWe redefine the JointData object and can then fit the model, because the prior specification does not need to change: We assume iid priors on the arm-specific parameters here.\n\n\nShow the code\njoint_data_by_arm &lt;- DataJoint(\n    subject = subj_data_by_arm,\n    longitudinal = long_data\n)\n\n\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/jm6.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  mcmc_results_by_arm &lt;- sampleStanModel(\n      tgi_mod,\n      data = joint_data_by_arm,\n      iter_sampling = ITER,\n      iter_warmup = WARMUP,\n      chains = CHAINS,\n      parallel_chains = CHAINS,\n      thin = CHAINS,\n      seed = BAYES.SEED,\n      refresh = REFRESH\n  )\n  save(mcmc_results_by_arm, file = save_file)\n}\n\n\nLet’s again check the convergence:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nsave_arm_file &lt;- here(\"session-tgi/jm6_more.RData\")\nif (file.exists(save_arm_file)) {\n  load(save_arm_file)\n} else {\n  mcmc_res_cmdstan_by_arm &lt;- cmdstanr::as.CmdStanMCMC(mcmc_results_by_arm)\n  mcmc_res_sum_by_arm &lt;- mcmc_res_cmdstan_by_arm$summary(vars)\n  vars_draws_by_arm &lt;- mcmc_res_cmdstan_by_arm$draws(vars)\n  loo_by_arm &lt;- mcmc_res_cmdstan_by_arm$loo(r_eff = FALSE)\n  save(mcmc_res_sum_by_arm, vars_draws_by_arm, loo_by_arm, file = save_arm_file)\n}\nmcmc_res_sum_by_arm\n\n\n# A tibble: 11 × 10\n   variable    mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu…  3.61   3.61  0.0211  0.0217   3.58   3.65  1.00      244.     502.\n 2 lm_sf_mu… -0.752 -0.730 0.222   0.216   -1.14  -0.398 1.01      592.     673.\n 3 lm_sf_mu… -1.52  -1.51  0.222   0.217   -1.90  -1.17  1.00      706.     850.\n 4 lm_sf_mu… -1.11  -1.11  0.138   0.139   -1.35  -0.900 1.00      594.     814.\n 5 lm_sf_mu… -1.35  -1.35  0.103   0.104   -1.52  -1.19  1.00      560.     776.\n 6 lm_sf_si…  0.161  0.161 0.00224 0.00228  0.157  0.165 1.00      892.     941.\n 7 lm_sf_om…  0.577  0.577 0.0159  0.0153   0.551  0.603 1.00      511.     672.\n 8 lm_sf_om…  1.34   1.33  0.150   0.152    1.11   1.60  1.00      630.     844.\n 9 lm_sf_om…  1.76   1.76  0.161   0.150    1.51   2.05  1.00      662.     990.\n10 lm_sf_om…  0.765  0.758 0.0710  0.0685   0.662  0.891 0.999     809.     951.\n11 lm_sf_om…  1.10   1.10  0.0705  0.0684   0.991  1.22  0.999     669.     736.\n\n\nLet’s again tabulate the parameter estimates:\n\n\nShow the code\npost_samples_by_arm &lt;- as_draws_df(vars_draws_by_arm) |&gt; \n  rename(\n    mu_bsld = \"lm_sf_mu_bsld[1]\",\n    mu_ks1 = \"lm_sf_mu_ks[1]\",\n    mu_ks2 = \"lm_sf_mu_ks[2]\",\n    mu_kg1 = \"lm_sf_mu_kg[1]\",\n    mu_kg2 = \"lm_sf_mu_kg[2]\",\n    omega_bsld = \"lm_sf_omega_bsld[1]\",\n    omega_ks1 = \"lm_sf_omega_ks[1]\",\n    omega_ks2 = \"lm_sf_omega_ks[2]\",\n    omega_kg1 = \"lm_sf_omega_kg[1]\",\n    omega_kg2 = \"lm_sf_omega_kg[2]\",\n    sigma = lm_sf_sigma\n  ) |&gt; \n  mutate(\n    theta_b0 = exp(mu_bsld + omega_bsld^2 / 2), \n    theta_ks1 = exp(mu_ks1 + omega_ks1^2 / 2), \n    theta_ks2 = exp(mu_ks2 + omega_ks2^2 / 2),\n    theta_kg1 = exp(mu_kg1 + omega_kg1^2 / 2),\n    theta_kg2 = exp(mu_kg2 + omega_kg2^2 / 2),\n    cv_0 = sqrt(exp(omega_bsld^2) - 1),\n    cv_s1 = sqrt(exp(omega_ks1^2) - 1),\n    cv_s2 = sqrt(exp(omega_ks2^2) - 1),\n    cv_g1 = sqrt(exp(omega_kg1^2) - 1),\n    cv_g2 = sqrt(exp(omega_kg2^2) - 1)\n  ) \n  \npost_sum_by_arm &lt;- post_samples_by_arm |&gt;\n  select(\n    theta_b0, theta_ks1, theta_ks2, theta_kg1, theta_kg2, \n    omega_bsld, omega_ks1, omega_ks2, omega_kg1, omega_kg2, \n    cv_0, cv_s1, cv_s2, cv_g1, cv_g2, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum_by_arm\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.7\n43.7\n0.999\n1.05\n42.1\n45.4\n1.00\n227\n558\n\n\ntheta_ks1\n1.18\n1.17\n0.152\n0.146\n0.961\n1.44\n1.00\n530\n696\n\n\ntheta_ks2\n1.07\n1.04\n0.209\n0.183\n0.779\n1.42\n1.00\n344\n443\n\n\ntheta_kg1\n0.444\n0.443\n0.0490\n0.0489\n0.365\n0.527\n1.00\n539\n712\n\n\ntheta_kg2\n0.479\n0.478\n0.0487\n0.0479\n0.399\n0.563\n1.00\n735\n908\n\n\nomega_bsld\n0.577\n0.577\n0.0159\n0.0153\n0.551\n0.603\n1.00\n504\n659\n\n\nomega_ks1\n1.34\n1.33\n0.150\n0.152\n1.11\n1.60\n1.00\n620\n838\n\n\nomega_ks2\n1.76\n1.76\n0.161\n0.150\n1.51\n2.05\n1.00\n651\n972\n\n\nomega_kg1\n0.765\n0.758\n0.0710\n0.0685\n0.662\n0.891\n0.999\n799\n943\n\n\nomega_kg2\n1.10\n1.10\n0.0705\n0.0684\n0.991\n1.22\n1.00\n649\n693\n\n\ncv_0\n0.629\n0.628\n0.0205\n0.0196\n0.596\n0.662\n1.00\n504\n659\n\n\ncv_s1\n2.33\n2.22\n0.601\n0.547\n1.55\n3.46\n1.00\n620\n838\n\n\ncv_s2\n4.90\n4.58\n1.64\n1.25\n2.96\n8.11\n1.00\n651\n972\n\n\ncv_g1\n0.897\n0.880\n0.112\n0.105\n0.742\n1.10\n0.999\n799\n943\n\n\ncv_g2\n1.55\n1.54\n0.175\n0.163\n1.29\n1.85\n1.00\n649\n693\n\n\nsigma\n0.161\n0.161\n0.00224\n0.00228\n0.157\n0.165\n1.00\n885\n881\n\n\n\n\n\n\n\nHere again the shrinkage rate in the treatment arm 1 seems higher than in the treatment arm 2. However, the difference is not as pronounced as in the brms model before with the same standard deviation for both arms. We can again calculate the posterior probability that the shrinkage rate in arm 1 is higher than in arm 2:\n\n\nShow the code\nprob_ks1_greater_ks2 &lt;- mean(post_samples_by_arm$theta_ks1 &gt; post_samples_by_arm$theta_ks2)\nprob_ks1_greater_ks2\n\n\n[1] 0.706\n\n\nSo the posterior probability is now only around 71%.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#model-comparison-with-loo",
    "href": "session-tgi/2_tgi_sf_jmpost.html#model-comparison-with-loo",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Model comparison with LOO",
    "text": "Model comparison with LOO\nAs we have seen for brms, also for jmpost we can easily compute the LOO criterion:\n\n\nShow the code\n# loo_res &lt;- mcmc_res_cmdstan$loo(r_eff = FALSE)\nloo_res\n\n\n\nComputed from 1000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12957.3  95.8\np_loo      1137.1  38.4\nlooic     25914.5 191.6\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nPareto k diagnostic values:\n                          Count Pct.    Min. ESS\n(-Inf, 0.67]   (good)     3665  89.4%   26      \n   (0.67, 1]   (bad)       363   8.9%   &lt;NA&gt;    \n    (1, Inf)   (very bad)   71   1.7%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nUnderneath this is using the $loo() method from cmdstanr.\nAnd we can compare this to the LOO of the model with separate arm estimates:\n\n\nShow the code\n# loo_by_arm &lt;- mcmc_res_cmdstan_by_arm$loo(r_eff = FALSE)\nloo_by_arm\n\n\n\nComputed from 1000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12931.8  95.6\np_loo      1121.5  36.8\nlooic     25863.5 191.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nPareto k diagnostic values:\n                          Count Pct.    Min. ESS\n(-Inf, 0.67]   (good)     3652  89.1%   48      \n   (0.67, 1]   (bad)       374   9.1%   &lt;NA&gt;    \n    (1, Inf)   (very bad)   73   1.8%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nSo the model by treatment arm performs here better than the model without treatment arm specific growth and shrinkage parameters.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#tipps-and-tricks",
    "href": "session-tgi/2_tgi_sf_jmpost.html#tipps-and-tricks",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Tipps and tricks",
    "text": "Tipps and tricks\n\nAlso here it is possible to look at the underlying Stan code:\n\n\nShow the code\ntmp &lt;- tempfile()\nwrite_stan(tgi_mod, destination = tmp)\nfile.edit(tmp) # opens the Stan file in the default editor\n\n\nIt is not trivial to transport saved models from one computer to another. This is because cmdstanr only loads the results it currently needs from disk into memory, and thus into the R session. If you want to transport the model to another computer, you need to save the Stan code and the data, and then re-run the model on the other computer. This is because the model object in R is only a reference to the model on disk, not the model itself. Note that there is the $save_object() method, see here, however this leads to very large files (here about 300 MB for one fit) and can thus not be uploaded to typical git repositories. Therefore above we saved interim result objects separately as needed.\nIt is important to explicitly define the truncation boundaries for the truncated normal priors, because otherwise the MCMC results will not be correct.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html",
    "href": "session-tgi/3_tgi_gsf_brms.html",
    "title": "3. Generalized Stein-Fojo model",
    "section": "",
    "text": "This appendix shows how the generalized Stein-Fojo model can be implemented in a Bayesian framework using the brms package in R.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#setup-and-load-data",
    "href": "session-tgi/3_tgi_gsf_brms.html#setup-and-load-data",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#generalized-stein-fojo-model",
    "href": "session-tgi/3_tgi_gsf_brms.html#generalized-stein-fojo-model",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Generalized Stein-Fojo model",
    "text": "Generalized Stein-Fojo model\nHere we have an additional parameter \\(\\phi\\), which is the weight for the shrinkage in the double exponential model. The model is then:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\{\n  \\psi_{\\phi i} \\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) +\n  (1 - \\psi_{\\phi i}) \\exp(\\psi_{k_{g}i} \\cdot t_{ij})\n\\}\n\\]\nfor positive times \\(t_{ij}\\). Again, if the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. That is, we have then \\(\\phi_i = 0\\). Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\{\n  \\psi_{\\phi i} \\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) +\n  (1 - \\psi_{\\phi i}) \\exp(\\psi_{k_{g}i} \\cdot t_{ij})\n\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\nIn terms of likelihood and priors, we can use the same assumptions as in the previous model. The only difference is that we have to model the \\(\\phi\\) parameter. We can use a logit-normal distribution for this parameter. This is a normal distribution on the logit scale, which is then transformed to the unit interval. \\[\n\\psi_{\\phi i} \\sim \\text{LogitNormal}(\\text{logit}(0.5) = 0, 0.5)\n\\]",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#fit-model",
    "href": "session-tgi/3_tgi_gsf_brms.html#fit-model",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (phi * exp(-ks * year) + (1 - phi) * exp(kg * year))) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # As before:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(phi ~ inv_logit(tphi)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(tphi ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + (1 | id)) +\n  lf(lkg ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\"),\n  prior(normal(log(1.04), 1), nlpar = \"lkg\"),\n  prior(normal(0, 0.5), nlpar = \"tphi\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(student_t(3, 0, 22.2), lb = 0, nlpar = \"tphi\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lks = array(-1.25),\n  b_lkg = array(-1.33),\n  b_tphi = array(0),\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  sd_4 = array(0.1),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients),\n  z_4 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/gsf1.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = WARMUP + ITER, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsummary(fit)\n\n\nWarning: There were 41 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (phi * exp(-ks * year) + (1 - phi) * exp(kg * year))) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         phi ~ inv_logit(tphi)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         tphi ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)      0.58      0.02     0.55     0.61 1.01      382      606\nsd(tphi_Intercept)     2.12      0.18     1.78     2.49 1.01      611     1216\nsd(lks_Intercept)      2.16      0.14     1.90     2.46 1.00      520     1282\nsd(lkg_Intercept)      1.18      0.09     1.01     1.36 1.00      899     1766\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept      0.15      0.00     0.14     0.15 1.00     1611     2351\nlb0_Intercept      3.63      0.02     3.59     3.68 1.03      221      547\ntphi_Intercept    -0.14      0.22    -0.57     0.27 1.00      424      839\nlks_Intercept     -0.62      0.10    -0.81    -0.43 1.00     1170     1868\nlkg_Intercept     -1.15      0.14    -1.43    -0.89 1.00      699     1488\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn total this took 76 minutes on my laptop.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#parameter-estimates",
    "href": "session-tgi/3_tgi_gsf_brms.html#parameter-estimates",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\n\nShow the code\npost_df &lt;- as_draws_df(fit)\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_tphi_Intercept\"      \n [4] \"b_lks_Intercept\"        \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"  \n [7] \"sd_id__tphi_Intercept\"  \"sd_id__lks_Intercept\"   \"sd_id__lkg_Intercept\"  \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_phi = plogis(b_tphi_Intercept),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    omega_phi = sd_id__tphi_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nLet’s first look at the population level parameters:\n\n\nShow the code\ngsf_pop_params &lt;- c(\"theta_b0\", \"theta_ks\", \"theta_kg\", \"theta_phi\", \"sigma\")\n\nmcmc_trace(post_df, pars = gsf_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = gsf_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nThe trace plots look good. The chains seem to have converged and the pairs plot shows no strong correlations between the parameters. Let’s check the table:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  select(theta_b0, theta_ks, theta_kg, theta_phi, omega_0, omega_s, omega_g, omega_phi, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.766\n44.746\n1.057\n1.073\n43.074\n46.528\n1.007\n179.361\n378.906\n\n\ntheta_ks\n5.857\n5.444\n1.924\n1.582\n3.589\n9.560\n1.003\n384.622\n905.388\n\n\ntheta_kg\n0.640\n0.637\n0.061\n0.062\n0.541\n0.745\n1.000\n685.379\n1,885.264\n\n\ntheta_phi\n0.467\n0.466\n0.055\n0.058\n0.376\n0.555\n1.003\n419.761\n831.991\n\n\nomega_0\n0.578\n0.578\n0.016\n0.017\n0.553\n0.606\n1.001\n376.483\n628.807\n\n\nomega_s\n2.159\n2.155\n0.145\n0.145\n1.929\n2.412\n1.002\n521.866\n1,265.639\n\n\nomega_g\n1.177\n1.174\n0.090\n0.090\n1.037\n1.333\n1.002\n868.075\n1,746.640\n\n\nomega_phi\n2.117\n2.108\n0.182\n0.184\n1.833\n2.423\n1.003\n603.806\n1,215.066\n\n\nsigma\n0.147\n0.147\n0.002\n0.002\n0.143\n0.150\n1.000\n1,575.916\n2,168.010\n\n\n\n\n\n\n\nSo \\(\\theta_{\\phi}\\) is estimated around 0.5. The other parameters are similar to the previous Stein-Fojo model, but we see a larger \\(\\theta_{k_s}\\) e.g.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#observation-vs-model-fit",
    "href": "session-tgi/3_tgi_gsf_brms.html#observation-vs-model-fit",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_epred_draws(fit) |&gt;\n  median_qi()\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = .epred, ymin = .lower, ymax = .upper), \n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = .epred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"GSF model fit\")\n\n\n\n\n\n\n\n\n\nThis also looks good. The model seems to capture the data well.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#with-jmpost",
    "href": "session-tgi/3_tgi_gsf_brms.html#with-jmpost",
    "title": "3. Generalized Stein-Fojo model",
    "section": "With jmpost",
    "text": "With jmpost\nThis model can also be fit with the jmpost package. The corresponding function is LongitudinalGSF. The statistical model is specified in the vignette here.\nHomework: Implement the generalized Stein-Fojo model with jmpost and compare the results with the brms implementation.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Index",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n2025-03-10\n\n\n0. Setup\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n2025-03-10\n\n\n1. TGI model minimal workflow with brms\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-03-10\n\n\n2. TGI model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-03-10\n\n\n3. Generalized Stein-Fojo model\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-03-10\n\n\n4. Claret-Bruno model\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Index"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Index",
    "section": "License",
    "text": "License\nThe training material is licensed under a Creative Commons Attribution 4.0 International License. If you wish to reuse any part of this material, please ensure proper attribution is given to the original authors as specified by the Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Index"
    ]
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Index",
    "section": "Copyright",
    "text": "Copyright\n© 2025 Genentech Inc. All rights reserved.",
    "crumbs": [
      "Index"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html",
    "href": "session-tgi/4_tgi_cb_brms.html",
    "title": "4. Claret-Bruno model",
    "section": "",
    "text": "This appendix shows how the Claret-Bruno model can be implemented in a Bayesian framework using the brms package in R.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#setup-and-load-data",
    "href": "session-tgi/4_tgi_cb_brms.html#setup-and-load-data",
    "title": "4. Claret-Bruno model",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#claret-bruno-model",
    "href": "session-tgi/4_tgi_cb_brms.html#claret-bruno-model",
    "title": "4. Claret-Bruno model",
    "section": "Claret-Bruno model",
    "text": "Claret-Bruno model\nIn the Claret-Bruno model we have again the baseline SLD and the growth rate as in the Stein-Fojo model. Then in addition we have the inhibition response rate \\(\\psi_{p}\\) and the treatment resistance rate \\(\\psi_{c}\\). The model is then:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\exp \\left\\{\n  \\psi_{k_{g}i} t_{ij} -\n  \\frac{\\psi_{pi}}{\\psi_{ci}} (1 - \\exp(-\\psi_{ci} t_{ij}))\n\\right\\}\n\\]\nfor positive times \\(t_{ij}\\). Again, if the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. That is, we have then \\(\\psi_{pi} = 0\\). Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\exp \\{\n  \\psi_{k_{g}i} t_{ij} -\n  \\frac{\\psi_{pi}}{\\psi_{ci}} (1 - \\exp(-\\psi_{ci} t_{ij}))\n\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\nFor the new model parameters we can again use log-normal prior distributions.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#fit-model",
    "href": "session-tgi/4_tgi_cb_brms.html#fit-model",
    "title": "4. Claret-Bruno model",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * exp(kg * year - (p / c) * (1 - exp(-c * year)))) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # Define the standard deviation (called sigma in brms) as a \n  # coefficient tau times the mean.\n  # sigma = tau * ystar is modelled on the log scale though, therefore:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations\n  nlf(b0 ~ exp(lb0)) +\n  nlf(kg ~ exp(lkg)) +\n  nlf(p ~ exp(lp)) +\n  nlf(c ~ exp(lc)) +\n  # Define random effect structure\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lkg ~ 1 + (1 | id)) +\n  lf(lp ~ 1 + (1 | id)) + \n  lf(lc ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.5), 0.1), nlpar = \"lkg\"),\n  prior(normal(0, 1), nlpar = \"lp\"),\n  prior(normal(log(0.5), 0.1), nlpar = \"lc\"),\n  prior(normal(2, 1), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(1, 1), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 0.5), lb = 0, nlpar = \"lp\", class = \"sd\"),\n  prior(normal(0, 0.5), lb = 0, nlpar = \"lc\", class = \"sd\"),\n  prior(normal(0, 1), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lkg = array(-0.69),\n  b_lp = array(0),\n  b_lc = array(-0.69),\n  sd_1 = array(0.5),\n  sd_2 = array(0.5),\n  sd_3 = array(0.1),\n  sd_4 = array(0.1),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients),\n  z_4 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/cb3.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else if (interactive()) {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = WARMUP + ITER, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH,\n    adapt_delta = 0.9,\n    max_treedepth = 15\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsave_fit_sum_file &lt;- here(\"session-tgi/cb3_fit_sum.RData\")\nif (file.exists(save_fit_sum_file)) {\n  load(save_fit_sum_file)\n} else {\n  fit_sum &lt;- summary(fit)\n  save(fit_sum, file = save_fit_sum_file)\n}\nfit_sum\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ ystar \n         ystar ~ int_step(year &gt; 0) * (b0 * exp(kg * year - (p/c) * (1 - exp(-c * year)))) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(ystar)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         kg ~ exp(lkg)\n         p ~ exp(lp)\n         c ~ exp(lc)\n         lb0 ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n         lp ~ 1 + (1 | id)\n         lc ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.01      419     1044\nsd(lkg_Intercept)     1.04      0.06     0.92     1.17 1.01      696     1181\nsd(lp_Intercept)      1.58      0.11     1.39     1.80 1.01      443     1110\nsd(lc_Intercept)      1.57      0.15     1.28     1.87 1.01      657      935\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.15      0.00     0.15     0.16 1.00     1345     3193\nlb0_Intercept     3.62      0.02     3.57     3.66 1.01      200      330\nlkg_Intercept    -1.00      0.07    -1.14    -0.86 1.01     1020     1906\nlp_Intercept     -0.82      0.14    -1.10    -0.57 1.01      643     1594\nlc_Intercept     -0.13      0.10    -0.34     0.07 1.00     1335     2478\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe did obtain here a warning about divergent transitions, see stan documentation for details:\nWarning message:\nThere were 4489 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup \nHowever, the effective sample size is high, i.e. the Rhat values are close to 1. This indicates that the chains have converged. We can proceed with the post-processing.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#parameter-estimates",
    "href": "session-tgi/4_tgi_cb_brms.html#parameter-estimates",
    "title": "4. Claret-Bruno model",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\n\nShow the code\npost_df_file &lt;- here(\"session-tgi/cb3_post_df.RData\")\nif (file.exists(post_df_file)) {\n  load(post_df_file)\n} else {\n  post_df &lt;- as_draws_df(fit) |&gt; \n    subset_draws(iteration = (1:1000) * 2)\n  save(post_df, file = post_df_file)\n}\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lkg_Intercept\"       \n [4] \"b_lp_Intercept\"         \"b_lc_Intercept\"         \"sd_id__lb0_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"sd_id__lp_Intercept\"    \"sd_id__lc_Intercept\"   \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_p = exp(b_lp_Intercept + sd_id__lp_Intercept^2 / 2),\n    theta_c = exp(b_lc_Intercept + sd_id__lc_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    omega_p = sd_id__lp_Intercept,\n    omega_c = sd_id__lc_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    cv_p = sqrt(exp(sd_id__lp_Intercept^2) - 1),\n    cv_c = sqrt(exp(sd_id__lc_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nLet’s first look at the population level parameters:\n\n\nShow the code\ncb_pop_params &lt;- c(\"theta_b0\", \"theta_kg\", \"theta_p\", \"theta_c\", \"sigma\")\n\nmcmc_trace(post_df, pars = cb_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_dens_overlay(post_df, pars = cb_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = cb_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nThe trace plots look good. The chains seem to have converged and the pairs plot shows no strong correlations between the parameters. Let’s check the table:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  dplyr::select(theta_b0, theta_kg, theta_p, theta_c, omega_0, omega_g, omega_p, omega_c,\n  cv_0, cv_g, cv_p, cv_c,\n  sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.142\n44.118\n1.093\n1.090\n42.411\n45.949\n1.003\n198.908\n390.883\n\n\ntheta_kg\n0.636\n0.634\n0.042\n0.041\n0.570\n0.711\n1.000\n1,093.102\n1,749.378\n\n\ntheta_p\n1.539\n1.515\n0.183\n0.179\n1.284\n1.871\n1.002\n361.463\n395.589\n\n\ntheta_c\n3.098\n2.984\n0.678\n0.607\n2.234\n4.366\n1.001\n552.128\n669.150\n\n\nomega_0\n0.581\n0.581\n0.016\n0.016\n0.556\n0.608\n1.003\n429.108\n1,002.446\n\n\nomega_g\n1.043\n1.040\n0.063\n0.062\n0.943\n1.149\n1.004\n664.633\n1,063.643\n\n\nomega_p\n1.576\n1.570\n0.105\n0.104\n1.413\n1.757\n1.001\n434.576\n976.113\n\n\nomega_c\n1.569\n1.565\n0.150\n0.153\n1.326\n1.822\n1.001\n638.934\n941.209\n\n\ncv_0\n0.634\n0.634\n0.020\n0.021\n0.602\n0.669\n1.003\n429.108\n1,002.446\n\n\ncv_g\n1.409\n1.396\n0.140\n0.136\n1.197\n1.655\n1.004\n664.633\n1,063.643\n\n\ncv_p\n3.383\n3.278\n0.645\n0.578\n2.521\n4.577\n1.001\n434.576\n976.113\n\n\ncv_c\n3.415\n3.254\n0.930\n0.838\n2.192\n5.165\n1.001\n638.934\n941.209\n\n\nsigma\n0.154\n0.154\n0.002\n0.002\n0.150\n0.158\n1.000\n1,258.184\n2,273.384\n\n\n\n\n\n\n\nWe see similar estimated values as before for \\(\\theta_{b_{0}}\\), \\(\\theta_{k_{g}}\\) and \\(\\sigma\\).",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#observation-vs-model-fit",
    "href": "session-tgi/4_tgi_cb_brms.html#observation-vs-model-fit",
    "title": "4. Claret-Bruno model",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim_save_file &lt;- here(\"session-tgi/cb3_sim_df.RData\")\nif (file.exists(df_sim_save_file)) {\n  load(df_sim_save_file)\n} else {\n  df_sim &lt;- df_subset |&gt; \n    data_grid(\n      id = pt_subset, \n      year = seq_range(year, 101)\n    ) |&gt;\n    add_epred_draws(fit) |&gt;\n    median_qi()\n  save(df_sim, file = df_sim_save_file)\n}\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = .epred, ymin = .lower, ymax = .upper), \n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = .epred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"CB model fit\")\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThis also looks good. The model seems to capture the data well.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#with-jmpost",
    "href": "session-tgi/4_tgi_cb_brms.html#with-jmpost",
    "title": "4. Claret-Bruno model",
    "section": "With jmpost",
    "text": "With jmpost\nThis model can also be fit with the jmpost package. The corresponding function is LongitudinalClaretBruno. The statistical model is specified in the vignette here.\nHomework: Implement the generalized Claret-Bruno model with jmpost and compare the results with the brms implementation.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html",
    "href": "session-tgi/0_setup.html",
    "title": "0. Setup",
    "section": "",
    "text": "This is a repository with training material for Tumor Growth Inhibition (TGI) and joint TGI-OS (Overall Survival) modeling.\nHere is an overview of the required setup steps, which are described in more detail below:\n\nInstall RTools (if you are on Windows)\nInstall necessary R packages\nInstall cmdstanr (optional but highly recommended)\nClone the repository from GitHub (https://github.com/RCONIS/tgi-os-training)\nOpen the folder in RStudio or VSCode\n\n\n\nIf you are running the examples on a Windows PC, you will need to install RTools, see\nhere.\nYou need to use the version of RTools that matches your R version. You can check your R version by running R.Version()$version.string.\nYou can then afterwards check the installation of RTools with:\nif(!require(pkgbuild)) install.packages(\"pkgbuild\")\npkgbuild::has_build_tools()\n\n\n\nThe following code will install the packages that are required to run the examples in this repository.\ninstall_if_not_available &lt;- function(pkg, min_version = NULL) {\n    is_installed &lt;- suppressWarnings(suppressPackageStartupMessages(\n      require(pkg, character.only = TRUE)\n    ))\n    if (is_installed & !is.null(min_version)) {\n      version_ok &lt;- packageVersion(pkg) &gt;= min_version\n    }    \n    if (!is_installed | !version_ok) {\n        install.packages(pkg)\n    }\n  }\npackages &lt;- c(\n  \"bayesplot\", \n  \"brms\", \n  \"ggplot2\",\n  \"gt\",\n  \"here\", \n  \"janitor\",\n  \"modelr\",\n  \"posterior\",\n  \"readxl\",\n  \"rstan\",\n  \"tidybayes\", \n  \"tidyverse\",   \n  \"truncnorm\"\n)\nsapply(packages, install_if_not_available)\nremotes::install_github(\"genentech/jmpost\")\n\n\n\nOptionally, you can use cmdstanr as the backend of brms for fitting the models.\nThere are a few advantages of using cmdstanr over the default rstan:\n\nWith cmdstanr, you can cache the compiled model, therefore you don’t need to recompile the model as long as the same formula is used, even after restarting R session (or re-opening RStudio).\ncmdstanr is more up-to-date and more actively maintained compared to rstan.\n\nA detailed installation guide for cmdstanr is available here. Here is a brief summary:\n\nInstall cmdstanr with:\n# Typically, you install cmdstanr from the R-universe as follows:\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\n\n# If the above does not work, e.g. because you in a company network or VPN, you can install cmdstanr from GitHub instead:\nremotes::install_github(\"stan-dev/cmdstanr@*release\")    \nCheck the compiler toolchain with:\ncmdstanr::check_cmdstan_toolchain()\n\nThis should not be a problem on Mac and Linux (including RStudio Cloud instances), but might be a problem on Windows.\nIf you have problems, please check the installation guide for Windows.\n\nRTools is the easiest way.\nEven if you have RTools, you might still see an error like \"Rtools44 installation found but the toolchain was not installed.\".\nIn this case, you can run cmdstanr::check_cmdstan_toolchain(fix = TRUE) and this will likely resolve the issue.\n\n\nInstall the CmdStan backend with:\ncmdstanr::install_cmdstan(cores = 2)\nTesting the cmdstanr installation\n\nIf the above installation was successful, you should now be able to run the following simple model.\n\nlibrary(cmdstanr)\nfile &lt;- file.path(\n  cmdstan_path(), \n  \"examples\", \"bernoulli\", \"bernoulli.stan\"\n)\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500\n)",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#install-rtools",
    "href": "session-tgi/0_setup.html#install-rtools",
    "title": "0. Setup",
    "section": "",
    "text": "If you are running the examples on a Windows PC, you will need to install RTools, see\nhere.\nYou need to use the version of RTools that matches your R version. You can check your R version by running R.Version()$version.string.\nYou can then afterwards check the installation of RTools with:\nif(!require(pkgbuild)) install.packages(\"pkgbuild\")\npkgbuild::has_build_tools()",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#install-necessary-r-packages",
    "href": "session-tgi/0_setup.html#install-necessary-r-packages",
    "title": "0. Setup",
    "section": "",
    "text": "The following code will install the packages that are required to run the examples in this repository.\ninstall_if_not_available &lt;- function(pkg, min_version = NULL) {\n    is_installed &lt;- suppressWarnings(suppressPackageStartupMessages(\n      require(pkg, character.only = TRUE)\n    ))\n    if (is_installed & !is.null(min_version)) {\n      version_ok &lt;- packageVersion(pkg) &gt;= min_version\n    }    \n    if (!is_installed | !version_ok) {\n        install.packages(pkg)\n    }\n  }\npackages &lt;- c(\n  \"bayesplot\", \n  \"brms\", \n  \"ggplot2\",\n  \"gt\",\n  \"here\", \n  \"janitor\",\n  \"modelr\",\n  \"posterior\",\n  \"readxl\",\n  \"rstan\",\n  \"tidybayes\", \n  \"tidyverse\",   \n  \"truncnorm\"\n)\nsapply(packages, install_if_not_available)\nremotes::install_github(\"genentech/jmpost\")",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#optional-install-cmdstanr",
    "href": "session-tgi/0_setup.html#optional-install-cmdstanr",
    "title": "0. Setup",
    "section": "",
    "text": "Optionally, you can use cmdstanr as the backend of brms for fitting the models.\nThere are a few advantages of using cmdstanr over the default rstan:\n\nWith cmdstanr, you can cache the compiled model, therefore you don’t need to recompile the model as long as the same formula is used, even after restarting R session (or re-opening RStudio).\ncmdstanr is more up-to-date and more actively maintained compared to rstan.\n\nA detailed installation guide for cmdstanr is available here. Here is a brief summary:\n\nInstall cmdstanr with:\n# Typically, you install cmdstanr from the R-universe as follows:\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\n\n# If the above does not work, e.g. because you in a company network or VPN, you can install cmdstanr from GitHub instead:\nremotes::install_github(\"stan-dev/cmdstanr@*release\")    \nCheck the compiler toolchain with:\ncmdstanr::check_cmdstan_toolchain()\n\nThis should not be a problem on Mac and Linux (including RStudio Cloud instances), but might be a problem on Windows.\nIf you have problems, please check the installation guide for Windows.\n\nRTools is the easiest way.\nEven if you have RTools, you might still see an error like \"Rtools44 installation found but the toolchain was not installed.\".\nIn this case, you can run cmdstanr::check_cmdstan_toolchain(fix = TRUE) and this will likely resolve the issue.\n\n\nInstall the CmdStan backend with:\ncmdstanr::install_cmdstan(cores = 2)\nTesting the cmdstanr installation\n\nIf the above installation was successful, you should now be able to run the following simple model.\n\nlibrary(cmdstanr)\nfile &lt;- file.path(\n  cmdstan_path(), \n  \"examples\", \"bernoulli\", \"bernoulli.stan\"\n)\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500\n)",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  }
]