[
  {
    "objectID": "session-bhm/0_setup.html",
    "href": "session-bhm/0_setup.html",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "First we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 5: BHM",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-bhm/0_setup.html#setup",
    "href": "session-bhm/0_setup.html#setup",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "First we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 5: BHM",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-bhm/0_setup.html#data-preparation",
    "href": "session-bhm/0_setup.html#data-preparation",
    "title": "0. Setup and data preparation",
    "section": "Data preparation",
    "text": "Data preparation\nWe prepare now the same dataset as before in the OS session. The only difference now is that we split the data into a historical data set and a current data set, in order to imitate application scenario.\nWe will again use the OAK study data.\nFor the tumor growth data, we are using the S1 data set from here. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\ntumor_data &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\") |&gt;\n    read_excel(sheet = \"Study4\") |&gt;\n    clean_names() |&gt;\n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm)),\n        arm = fct_recode(arm, \"Docetaxel\" = \"1\", \"MPDL3280A\" = \"2\")\n    ) |&gt;\n    select(id, year, sld, arm)\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 4\n  id                       year   sld arm      \n  &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 -594245265511412736  -0.00821  42   Docetaxel\n2 -594245265511412736   0.0986   44   Docetaxel\n3 -594245265511412736   0.222    44   Docetaxel\n4 -4078394139718744064 -0.0110   65.8 Docetaxel\n5 -4078394139718744064  0.832    66   Docetaxel\n6 -4078394139718744064  1.07     68   Docetaxel\n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        \n 5394902984416364544 :  16   Min.   :-0.1068   Min.   :  2.00  \n 7160320731596789760 :  16   1st Qu.: 0.1123   1st Qu.: 19.05  \n -4159496062492130816:  15   Median : 0.2519   Median : 32.00  \n -7900338178541499392:  15   Mean   : 0.3995   Mean   : 38.16  \n 4878279915140903936 :  15   3rd Qu.: 0.5749   3rd Qu.: 51.00  \n 9202154259772598272 :  15   Max.   : 2.0753   Max.   :228.00  \n (Other)             :4034                     NA's   :27      \n        arm      \n Docetaxel:1658  \n MPDL3280A:2468  \n                 \n                 \n                 \n                 \n                 \n\n\nHere we have 701 patients, and we know from Fig. 1 in the publication that this is the subset of patients with at least 3 tumor size measurements. We have guessed from the second Excel sheet with parameter estimates that arm 1 means docetaxel and arm 2 means MPDL3280A. This also seems reasonable given the larger number of measurements per patient:\n\n\nShow the code\n# make a table\ntumor_data |&gt;\n    group_by(id) |&gt;\n    summarize(arm = arm[1], n = n()) |&gt;\n    group_by(arm) |&gt;\n    summarize(mean_n = mean(n))\n\n\n# A tibble: 2 × 2\n  arm       mean_n\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Docetaxel   5.10\n2 MPDL3280A   6.56\n\n\nFor the OS data, we will be using the Supplementary Table 8 from here. Again, we have copied the data set in this GitHub repository.\nFrom the supplementary material, we know that:\n\nPFS/OS: time in months\nPFS.CNSR/OS.CNSR: 1 for censor, 0 for event\n\nTherefore we will convert the time to years and the censoring to a logical variable:\n\n\nShow the code\nos_data &lt;- here(\"data/41591_2018_134_MOESM3_ESM.xlsx\") |&gt;\n    read_excel(sheet = \"OAK_Clinical_Data\") |&gt;\n    clean_names() |&gt;\n    rename(\n        age = bage,\n        race = race2,\n        ecog = ecoggr,\n        arm = trt01p,\n        response = bcor\n    ) |&gt;\n    mutate(\n        id = factor(as.character(pt_id)),\n        sld = as.numeric(na_if(bl_sld, \".\")),\n        pfs_time = as.numeric(pfs) / 12,\n        pfs_event = as.numeric(pfs_cnsr) == 0,\n        os_time = as.numeric(os) / 12,\n        os_event = as.numeric(os_cnsr) == 0,\n        race = factor(race),\n        ecog = factor(ecog),\n        arm = factor(arm),\n        response = factor(\n            response,\n            levels = c(\"CR\", \"PR\", \"SD\", \"PD\", \"NE\")\n        ),\n        sex = factor(sex)\n    ) |&gt;\n    select(\n        id,\n        arm,\n        ecog,\n        age,\n        race,\n        sex,\n        sld,\n        response,\n        pfs_time,\n        pfs_event,\n        os_time,\n        os_event\n    )\n\nhead(os_data)\n\n\n# A tibble: 6 × 12\n  id    arm    ecog    age race  sex     sld response pfs_time pfs_event os_time\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;lgl&gt;       &lt;dbl&gt;\n1 318   Docet… 1        64 WHITE M     152   PD          0.107 TRUE        0.433\n2 1088  Docet… 0        65 WHITE M      36   PD          0.194 TRUE        0.402\n3 345   Docet… 1        75 WHITE M      92   &lt;NA&gt;        0.162 TRUE        0.162\n4 588   Docet… 0        61 WHITE F      36   SD          1.02  TRUE        2.05 \n5 306   MPDL3… 1        53 WHITE F      86   PD          0.118 TRUE        0.498\n6 718   Docet… 1        80 WHITE M      45.7 SD          0.712 TRUE        1.60 \n# ℹ 1 more variable: os_event &lt;lgl&gt;\n\n\nShow the code\nsummary(os_data)\n\n\n       id             arm      ecog         age           race     sex    \n 1000   :  1   Docetaxel:425   0:315   Min.   :33.00   ASIAN:180   F:330  \n 1001   :  1   MPDL3280A:425   1:535   1st Qu.:57.00   OTHER: 72   M:520  \n 1002   :  1                           Median :64.00   WHITE:598          \n 1003   :  1                           Mean   :63.23                      \n 1004   :  1                           3rd Qu.:70.00                      \n 1005   :  1                           Max.   :85.00                      \n (Other):844                                                              \n      sld         response      pfs_time        pfs_event      \n Min.   : 10.00   CR  :  7   Min.   :0.002738   Mode :logical  \n 1st Qu.: 41.00   PR  :108   1st Qu.:0.117728   FALSE:95       \n Median : 66.00   SD  :327   Median :0.240931   TRUE :755      \n Mean   : 76.69   PD  :304   Mean   :0.443174                  \n 3rd Qu.: 99.00   NE  : 24   3rd Qu.:0.580424                  \n Max.   :316.00   NA's: 80   Max.   :2.242300                  \n NA's   :1                                                     \n    os_time          os_event      \n Min.   :0.002738   Mode :logical  \n 1st Qu.:0.364134   FALSE:281      \n Median :0.803559   TRUE :569      \n Mean   :0.949841                  \n 3rd Qu.:1.620808                  \n Max.   :2.253251                  \n                                   \n\n\nNow we would like to join the two data sets, i.e. know which tumor growth data belongs to which baseline covariates and overall survival time and event indicator. Unfortunately we cannot use the id variable as it is not consistent between the two data sets, because both were anonymized. Instead, we will use the sld variable in combination with the treatment arm. We can also double check with the best overall response variable, because that is closely related to the tumor growth.\nFirst, we will summarize the patient information int he tumor data set. We will calculate the baseline SLD, determine the time of the last tumor measurement and approximate the best overall response by looking at the change from baseline:\n\n\nShow the code\nget_baseline &lt;- function(sld, year) {\n    which_base &lt;- tail(which(year &lt;= 0), 1L)\n    if (length(which_base) == 0) {\n        which_base &lt;- which.min(year)\n    }\n    sld[which_base]\n}\n\nget_contig_below_thresh &lt;- function(sld, year, bsld, thresh) {\n    rle_res &lt;- with(\n        rle(((sld[year &gt; 0] - bsld) / bsld) &lt; 0.2),\n        lengths[values]\n    )\n    if (length(rle_res) == 0) {\n        0\n    } else {\n        max(rle_res)\n    }\n}\n\ntumor_data_summary &lt;- tumor_data |&gt;\n    group_by(id) |&gt;\n    arrange(year) |&gt;\n    summarize(\n        arm = arm[1L],\n        bsld = get_baseline(sld, year),\n        last_year = tail(year, 1L),\n        nadir = min(sld[year &gt;= 0], na.rm = TRUE),\n        max_cfn = max((sld[year &gt;= 0] - nadir) / nadir, na.rm = TRUE),\n        min_cfb = min((sld[year &gt;= 0] - bsld) / bsld, na.rm = TRUE),\n        contig_below_0.2 = get_contig_below_thresh(sld, year, bsld, thresh = 0.2),\n        approx_response = case_when(\n            min_cfb &lt;= -0.3 ~ \"PR\",\n            contig_below_0.2 &gt;= 2 ~ \"SD\",\n            max_cfn &gt;= 0.2 ~ \"PD\",\n            .default = \"NE\"\n        )\n    )\nhead(tumor_data_summary)\n\n\n# A tibble: 6 × 9\n  id                arm    bsld last_year nadir max_cfn min_cfb contig_below_0.2\n  &lt;fct&gt;             &lt;fct&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n1 -100096848245159… Doce…    53     0.356    55  0       0.0377                3\n2 -102777882269110… MPDL…    11     0.901     2  2      -0.818                 7\n3 -105049343897269… MPDL…    20     0.345    21  1.62    0.05                  2\n4 -106934201929572… Doce…    15     1.72     11  0.273  -0.267                12\n5 -108949367808374… Doce…    33     0.709    31  0.0968 -0.0606                6\n6 -110624773747042… MPDL…    24     1.32     16  0.562  -0.333                 9\n# ℹ 1 more variable: approx_response &lt;chr&gt;\n\n\nNow let’s try to fuzzy join this with the OS data set, based on the baseline SLD and the treatment arm. In addition, we know that the last time point in the tumor data needs to be before the OS time point. We will also check that the best overall response is the same in both data sets. There will be some errors here: For example, we don’t know whether there were new lesions leading to a progression assessment, or whether the patient had later better results. So we need to expect that the matching will not be perfect.\n\n\nShow the code\nos_data_keys &lt;- os_data |&gt;\n    select(id, arm, sld, pfs_time, os_time, response)\nhead(os_data_keys)\n\n\n# A tibble: 6 × 6\n  id    arm         sld pfs_time os_time response\n  &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n1 318   Docetaxel 152      0.107   0.433 PD      \n2 1088  Docetaxel  36      0.194   0.402 PD      \n3 345   Docetaxel  92      0.162   0.162 &lt;NA&gt;    \n4 588   Docetaxel  36      1.02    2.05  SD      \n5 306   MPDL3280A  86      0.118   0.498 PD      \n6 718   Docetaxel  45.7    0.712   1.60  SD      \n\n\nShow the code\ndist_match &lt;- function(v1, v2) {\n    dist &lt;- abs(v1 - v2)\n    data.frame(include = (dist &lt;= 0.05))\n}\n\nless_match &lt;- function(lower, upper) {\n    data.frame(include = lower &lt;= upper)\n}\n\ntumor_os_data_joined &lt;- tumor_data_summary |&gt;\n    fuzzy_left_join(\n        os_data_keys,\n        by = c(\n            \"arm\" = \"arm\",\n            \"bsld\" = \"sld\",\n            \"last_year\" = \"os_time\",\n            \"approx_response\" = \"response\"\n        ),\n        match_fun = list(\n            `==`,\n            dist_match,\n            less_match,\n            `==`\n        )\n    )\nnrow(tumor_os_data_joined)\n\n\n[1] 954\n\n\nWe see that we cannot match the two data sets exactly:\n\nWe have less patients to start with in the tumor size data set.\nWe have patients in the tumor size data set which match multiple patients in the OS data set based on the treatment arm and the baseline SLD, and also the last measurement time point cannot disambiguate them.\nWe have patients in the tumor size data set which do not match any patient in the OS data set.\n\nHowever, for the purpose of this training we don’t need to be able to recover the true full data set. Therefore we will just subset the matched data set to a reasonable combination of the two data sets:\n\n\nShow the code\ntumor_os_data_joined_subset &lt;- tumor_os_data_joined |&gt;\n    na.omit() |&gt;\n    filter(!duplicated(id.y)) |&gt;\n    filter(!duplicated(id.x))\nnrow(tumor_os_data_joined_subset)\n\n\n[1] 203\n\n\nNow let’s save the joining information:\n\n\nShow the code\ntgi_os_join_keys &lt;- tumor_os_data_joined_subset |&gt;\n    select(id.x, id.y, arm.x) |&gt;\n    rename(\n        id_tgi = id.x,\n        id_os = id.y,\n        arm = arm.x\n    )\ntable(tgi_os_join_keys$arm)\n\n\n\nDocetaxel MPDL3280A \n       96       107 \n\n\nSo we have about 100 patients in each arm.\nWe subset the two data sets accordingly:\n\n\nShow the code\ntumor_data &lt;- tumor_data |&gt;\n    inner_join(\n        tgi_os_join_keys,\n        by = c(\"id\" = \"id_tgi\", \"arm\" = \"arm\")\n    ) |&gt;\n    select(-id) |&gt;\n    rename(id = id_os)\n\nos_data &lt;- os_data |&gt;\n    inner_join(\n        tgi_os_join_keys,\n        by = c(\"id\" = \"id_os\", \"arm\" = \"arm\")\n    ) |&gt;\n    select(-id_tgi)",
    "crumbs": [
      "Session 5: BHM",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-bhm/0_setup.html#split-into-historical-and-current-data-sets",
    "href": "session-bhm/0_setup.html#split-into-historical-and-current-data-sets",
    "title": "0. Setup and data preparation",
    "section": "Split into historical and current data sets",
    "text": "Split into historical and current data sets\nFor the purpose of the Bayesian Hierarchical Modeling session, we will split the data sets into historical and current data sets.\n\n\nShow the code\nhistorical_proportion &lt;- 0.5\nn_total &lt;- nrow(os_data)\nn_historical &lt;- floor(n_total * historical_proportion)\n\nset.seed(8469)\nhistorical_ids &lt;- os_data |&gt;\n    pull(id) |&gt;\n    sample(n_historical)\ncurrent_ids &lt;- setdiff(os_data$id, historical_ids)\n\nhistorical_os_data &lt;- os_data |&gt;\n    filter(id %in% historical_ids) |&gt;\n    mutate(study = \"historical\")\ncurrent_os_data &lt;- os_data |&gt;\n    filter(id %in% current_ids) |&gt;\n    mutate(study = \"current\")\n\nhistorical_tumor_data &lt;- tumor_data |&gt;\n    filter(id %in% historical_ids)\ncurrent_tumor_data &lt;- tumor_data |&gt;\n    filter(id %in% current_ids)\n\n\nIf we want to make this even more realistic, we can also censor the overall survival times in the current data set at a certain cutoff time point, say 1 year:\n\n\nShow the code\ncurrent_os_cutoff &lt;- 1 # year\n\ncurrent_os_data &lt;- current_os_data |&gt;\n    mutate(\n        os_event = ifelse(os_time &gt; current_os_cutoff, FALSE, os_event),\n        os_time = pmin(os_time, current_os_cutoff)\n    )\n\n\nThis simulates immature OS data in the current study, which has only been followed for 1 year. We can see this in the number of events:\n\n\nShow the code\nsum(historical_os_data$os_event)\n\n\n[1] 51\n\n\nShow the code\nsum(current_os_data$os_event)\n\n\n[1] 32",
    "crumbs": [
      "Session 5: BHM",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html",
    "href": "session-tgi/4_tgi_cb_brms.html",
    "title": "4. Claret-Bruno model",
    "section": "",
    "text": "This appendix shows how the Claret-Bruno model can be implemented in a Bayesian framework using the brms package in R.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#setup-and-load-data",
    "href": "session-tgi/4_tgi_cb_brms.html#setup-and-load-data",
    "title": "4. Claret-Bruno model",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#claret-bruno-model",
    "href": "session-tgi/4_tgi_cb_brms.html#claret-bruno-model",
    "title": "4. Claret-Bruno model",
    "section": "Claret-Bruno model",
    "text": "Claret-Bruno model\nIn the Claret-Bruno model we have again the baseline SLD and the growth rate as in the Stein-Fojo model. Then in addition we have the inhibition response rate \\(\\psi_{p}\\) and the treatment resistance rate \\(\\psi_{c}\\). The model is then:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\exp \\left\\{\n  \\psi_{k_{g}i} t_{ij} -\n  \\frac{\\psi_{pi}}{\\psi_{ci}} (1 - \\exp(-\\psi_{ci} t_{ij}))\n\\right\\}\n\\]\nfor positive times \\(t_{ij}\\). Again, if the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. That is, we have then \\(\\psi_{pi} = 0\\). Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\exp \\{\n  \\psi_{k_{g}i} t_{ij} -\n  \\frac{\\psi_{pi}}{\\psi_{ci}} (1 - \\exp(-\\psi_{ci} t_{ij}))\n\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\nFor the new model parameters we can again use log-normal prior distributions.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#fit-model",
    "href": "session-tgi/4_tgi_cb_brms.html#fit-model",
    "title": "4. Claret-Bruno model",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * exp(kg * year - (p / c) * (1 - exp(-c * year)))) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # Define the standard deviation (called sigma in brms) as a \n  # coefficient tau times the mean.\n  # sigma = tau * ystar is modelled on the log scale though, therefore:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations\n  nlf(b0 ~ exp(lb0)) +\n  nlf(kg ~ exp(lkg)) +\n  nlf(p ~ exp(lp)) +\n  nlf(c ~ exp(lc)) +\n  # Define random effect structure\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lkg ~ 1 + (1 | id)) +\n  lf(lp ~ 1 + (1 | id)) + \n  lf(lc ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.5), 0.1), nlpar = \"lkg\"),\n  prior(normal(0, 1), nlpar = \"lp\"),\n  prior(normal(log(0.5), 0.1), nlpar = \"lc\"),\n  prior(normal(2, 1), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(1, 1), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 0.5), lb = 0, nlpar = \"lp\", class = \"sd\"),\n  prior(normal(0, 0.5), lb = 0, nlpar = \"lc\", class = \"sd\"),\n  prior(normal(0, 1), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lkg = array(-0.69),\n  b_lp = array(0),\n  b_lc = array(-0.69),\n  sd_1 = array(0.5),\n  sd_2 = array(0.5),\n  sd_3 = array(0.1),\n  sd_4 = array(0.1),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients),\n  z_4 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/cb3.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else if (interactive()) {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = WARMUP + ITER, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH,\n    adapt_delta = 0.9,\n    max_treedepth = 15\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsave_fit_sum_file &lt;- here(\"session-tgi/cb3_fit_sum.RData\")\nif (file.exists(save_fit_sum_file)) {\n  load(save_fit_sum_file)\n} else {\n  fit_sum &lt;- summary(fit)\n  save(fit_sum, file = save_fit_sum_file)\n}\nfit_sum\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ ystar \n         ystar ~ int_step(year &gt; 0) * (b0 * exp(kg * year - (p/c) * (1 - exp(-c * year)))) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(ystar)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         kg ~ exp(lkg)\n         p ~ exp(lp)\n         c ~ exp(lc)\n         lb0 ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n         lp ~ 1 + (1 | id)\n         lc ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.01      419     1044\nsd(lkg_Intercept)     1.04      0.06     0.92     1.17 1.01      696     1181\nsd(lp_Intercept)      1.58      0.11     1.39     1.80 1.01      443     1110\nsd(lc_Intercept)      1.57      0.15     1.28     1.87 1.01      657      935\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.15      0.00     0.15     0.16 1.00     1345     3193\nlb0_Intercept     3.62      0.02     3.57     3.66 1.01      200      330\nlkg_Intercept    -1.00      0.07    -1.14    -0.86 1.01     1020     1906\nlp_Intercept     -0.82      0.14    -1.10    -0.57 1.01      643     1594\nlc_Intercept     -0.13      0.10    -0.34     0.07 1.00     1335     2478\n\nDraws were sampled using sample(hmc). Overall Rhat and ESS estimates\nare not informative for brm_multiple models and are hence not displayed.\nPlease see ?brm_multiple for how to assess convergence of such models.\n\n\nWe did obtain here a warning about divergent transitions, see stan documentation for details:\nWarning message:\nThere were 4489 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup \nHowever, the effective sample size is high, i.e. the Rhat values are close to 1. This indicates that the chains have converged. We can proceed with the post-processing.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#parameter-estimates",
    "href": "session-tgi/4_tgi_cb_brms.html#parameter-estimates",
    "title": "4. Claret-Bruno model",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\n\nShow the code\npost_df_file &lt;- here(\"session-tgi/cb3_post_df.RData\")\nif (file.exists(post_df_file)) {\n  load(post_df_file)\n} else {\n  post_df &lt;- as_draws_df(fit) |&gt; \n    subset_draws(iteration = (1:1000) * 2)\n  save(post_df, file = post_df_file)\n}\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lkg_Intercept\"       \n [4] \"b_lp_Intercept\"         \"b_lc_Intercept\"         \"sd_id__lb0_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"sd_id__lp_Intercept\"    \"sd_id__lc_Intercept\"   \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_p = exp(b_lp_Intercept + sd_id__lp_Intercept^2 / 2),\n    theta_c = exp(b_lc_Intercept + sd_id__lc_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    omega_p = sd_id__lp_Intercept,\n    omega_c = sd_id__lc_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    cv_p = sqrt(exp(sd_id__lp_Intercept^2) - 1),\n    cv_c = sqrt(exp(sd_id__lc_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nLet’s first look at the population level parameters:\n\n\nShow the code\ncb_pop_params &lt;- c(\"theta_b0\", \"theta_kg\", \"theta_p\", \"theta_c\", \"sigma\")\n\nmcmc_trace(post_df, pars = cb_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_dens_overlay(post_df, pars = cb_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = cb_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nThe trace plots look good. The chains seem to have converged and the pairs plot shows no strong correlations between the parameters. Let’s check the table:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  dplyr::select(theta_b0, theta_kg, theta_p, theta_c, omega_0, omega_g, omega_p, omega_c,\n  cv_0, cv_g, cv_p, cv_c,\n  sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.142\n44.118\n1.093\n1.090\n42.411\n45.949\n1.003\n198.908\n390.883\n\n\ntheta_kg\n0.636\n0.634\n0.042\n0.041\n0.570\n0.711\n1.000\n1,093.102\n1,749.378\n\n\ntheta_p\n1.539\n1.515\n0.183\n0.179\n1.284\n1.871\n1.002\n361.463\n395.589\n\n\ntheta_c\n3.098\n2.984\n0.678\n0.607\n2.234\n4.366\n1.001\n552.128\n669.150\n\n\nomega_0\n0.581\n0.581\n0.016\n0.016\n0.556\n0.608\n1.003\n429.108\n1,002.446\n\n\nomega_g\n1.043\n1.040\n0.063\n0.062\n0.943\n1.149\n1.004\n664.633\n1,063.643\n\n\nomega_p\n1.576\n1.570\n0.105\n0.104\n1.413\n1.757\n1.001\n434.576\n976.113\n\n\nomega_c\n1.569\n1.565\n0.150\n0.153\n1.326\n1.822\n1.001\n638.934\n941.209\n\n\ncv_0\n0.634\n0.634\n0.020\n0.021\n0.602\n0.669\n1.003\n429.108\n1,002.446\n\n\ncv_g\n1.409\n1.396\n0.140\n0.136\n1.197\n1.655\n1.004\n664.633\n1,063.643\n\n\ncv_p\n3.383\n3.278\n0.645\n0.578\n2.521\n4.577\n1.001\n434.576\n976.113\n\n\ncv_c\n3.415\n3.254\n0.930\n0.838\n2.192\n5.165\n1.001\n638.934\n941.209\n\n\nsigma\n0.154\n0.154\n0.002\n0.002\n0.150\n0.158\n1.000\n1,258.184\n2,273.384\n\n\n\n\n\n\n\nWe see similar estimated values as before for \\(\\theta_{b_{0}}\\), \\(\\theta_{k_{g}}\\) and \\(\\sigma\\).",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#observation-vs-model-fit",
    "href": "session-tgi/4_tgi_cb_brms.html#observation-vs-model-fit",
    "title": "4. Claret-Bruno model",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim_save_file &lt;- here(\"session-tgi/cb3_sim_df.RData\")\nif (file.exists(df_sim_save_file)) {\n  load(df_sim_save_file)\n} else {\n  df_sim &lt;- df_subset |&gt; \n    data_grid(\n      id = pt_subset, \n      year = seq_range(year, 101)\n    ) |&gt;\n    add_epred_draws(fit) |&gt;\n    median_qi()\n  save(df_sim, file = df_sim_save_file)\n}\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = .epred, ymin = .lower, ymax = .upper), \n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = .epred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"CB model fit\")\n\n\nWarning: Removed 26 rows containing missing values or values outside the scale range\n(`geom_ribbon()`).\n\n\nWarning: Removed 26 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThis also looks good. The model seems to capture the data well.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#with-jmpost",
    "href": "session-tgi/4_tgi_cb_brms.html#with-jmpost",
    "title": "4. Claret-Bruno model",
    "section": "With jmpost",
    "text": "With jmpost\nThis model can also be fit with the jmpost package. The corresponding function is LongitudinalClaretBruno. The statistical model is specified in the vignette here.\nHomework: Implement the generalized Claret-Bruno model with jmpost and compare the results with the brms implementation.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html",
    "href": "session-tgi/1_tgi_sf_brms.html",
    "title": "1. TGI model minimal workflow with brms",
    "section": "",
    "text": "The purpose of this document is to show a minimal workflow for fitting a TGI model using the brms package.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#setup-and-load-data",
    "href": "session-tgi/1_tgi_sf_brms.html#setup-and-load-data",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#stein-fojo-model",
    "href": "session-tgi/1_tgi_sf_brms.html#stein-fojo-model",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Stein-Fojo model",
    "text": "Stein-Fojo model\nWe start from the Stein-Fojo model as shown in the slides:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\{\\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) + \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) - 1\\}\n\\]\n\nMean\nWe will make one more tweak here. If the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\{\\exp(-\\psi_{k_{s}i} \\cdot t_{ij}) + \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) - 1\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\n\n\nLikelihood\nFor the likelihood given the mean SLD \\(y^{*}\\), we will assume a normal distribution with a constant coefficient of variation \\(\\tau\\):\n\\[\ny(t_{ij}) \\sim \\text{N}(y^{*}(t_{ij}), y^{*}(t_{ij})\\tau)\n\\]\nNote that for consistency with the brms and Stan convention, here we denote the standard deviation as the second parameter of the normal distribution. So in this case, the variance would be \\((y^{*}(t_{ij})\\tau)^2\\).\nThis can also be written as:\n\\[\ny(t_{ij}) = (1 + \\epsilon_{ij}) \\cdot y^{*}(t_{ij})\n\\]\nwhere \\(\\epsilon_{ij} \\sim \\text{N}(0, \\tau)\\).\nNote that also the additive model is a possible choice, where\n\\[\ny(t_{ij}) = y^{*}(t_{ij}) + \\epsilon_{ij}\n\\]\nsuch that the error does not depend on the scale of the SLD any longer.\n\n\nRandom effects\nNext, we define the distributions of the random effects \\(\\psi_{b_{0}i}\\), \\(\\psi_{k_{s}i}\\), \\(\\psi_{k_{g}i}\\), for \\(i = 1, \\dotsc, n\\):\n\\[\n\\begin{align*}\n\\psi_{b_{0}i} &\\sim \\text{LogNormal}(\\mu_{b_{0}}, \\omega_{0}) \\\\\n\\psi_{k_{s}i} &\\sim \\text{LogNormal}(\\mu_{k_{s}}, \\omega_{s}) \\\\\n\\psi_{k_{g}i} &\\sim \\text{LogNormal}(\\mu_{k_{g}}, \\omega_{g})\n\\end{align*}\n\\]\nThis can be rewritten as:\n\\[\n\\begin{align*}\n\\psi_{b_{0}i} &= \\exp(\\mu_{b_{0}} + \\omega_{0} \\cdot \\eta_{b_{0}i}) \\\\\n\\psi_{k_{s}i} &= \\exp(\\mu_{k_{s}} + \\omega_{s} \\cdot \\eta_{k_{s}i}) \\\\\n\\psi_{k_{g}i} &= \\exp(\\mu_{k_{g}} + \\omega_{g} \\cdot \\eta_{k_{g}i})\n\\end{align*}\n\\]\nwhere \\(\\eta_{b_{0}i}\\), \\(\\eta_{k_{s}i}\\), \\(\\eta_{k_{g}i}\\) are the standard normal distributed random effects.\nThis is important for two reasons:\n\nThis parametrization can help the sampler to converge faster. See here for more information.\nThis shows a bit more explicitly that the population mean of the random effects is not equal to the \\(\\mu\\) parameter, but to \\(\\theta = \\exp(\\mu + \\omega^2 / 2)\\), because they are log-normally distributed (see e.g. Wikipedia for the formulas). This is important for the interpretation of the parameter estimates.\n\n\n\nPriors\nFinally, we need to define the priors for the hyperparameters.\nThere are different principles we could use to define these priors:\n\nNon-informative priors: We could use priors that are as non-informative as possible. This is especially useful if we do not have any prior knowledge about the parameters. For example, we could use normal priors with a large standard deviation for the population means of the random effects.\nInformative priors: If we have some prior knowledge about the parameters, we can use this to define the priors. For example, if we have literature data about the \\(k_g\\) parameter estimate, we could use this to define the prior for the population mean of the growth rate. (Here we just need to be careful to consider the time scale and the log-normal distribution, as mentioned above)\n\nHere we use relatively informative priors for the log-normal location parameters, motivated by prior analyses of the same study:\n\\[\n\\begin{align*}\n\\mu_{b_{0}} &\\sim \\text{Normal}(\\log(65), 1) \\\\\n\\mu_{k_{s}} &\\sim \\text{Normal}(\\log(0.52), 0.1) \\\\\n\\mu_{k_{g}} &\\sim \\text{Normal}(\\log(1.04), 1)\n\\end{align*}\n\\]\nFor all standard deviations we use truncated normal priors:\n\\[\n\\begin{align*}\n\\omega_{0} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\omega_{s} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\omega_{g} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\tau &\\sim \\text{PositiveNormal}(0, 3)\n\\end{align*}\n\\]\nwhere \\(\\text{PositiveNormal}(0, 3)\\) denotes a truncated normal distribution with mean \\(0\\) and standard deviation \\(3\\), truncated to the positive real numbers.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#fit-model",
    "href": "session-tgi/1_tgi_sf_brms.html#fit-model",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood:\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (exp(-ks * year) + exp(kg * year) -  1)) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # Define the standard deviation (called sigma in brms) as a \n  # coefficient tau times the mean ystar.\n  # sigma is modelled on the log scale though, therefore:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  # This line is needed to declare tau as a model parameter:\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + (1 | id)) +\n  lf(lkg ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\"),\n  prior(normal(log(1.04), 1), nlpar = \"lkg\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lks = array(-1.25),\n  b_lkg = array(-1.33),\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/fit9.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsummary(fit)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.00      689     1248\nsd(lks_Intercept)     1.40      0.08     1.25     1.55 1.00      702     1586\nsd(lkg_Intercept)     0.96      0.04     0.88     1.05 1.01      902     1750\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.16      0.00     0.16     0.17 1.00     2250     2892\nlb0_Intercept     3.62      0.02     3.57     3.66 1.01      420      723\nlks_Intercept    -0.86      0.08    -1.01    -0.72 1.00     1661     2415\nlkg_Intercept    -1.20      0.07    -1.34    -1.08 1.01      676     1419\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote that it is crucial to use here int_step() to properly define the two pieces of the linear predictor for negative and non-negative time values: If you used step() like I did for a few days, then you will have the wrong model! This is because in Stan, step(false) = step(0) = 1 and not 0 as you would expect. Only int_step(0) = 0 as we need it here.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#parameter-estimates",
    "href": "session-tgi/1_tgi_sf_brms.html#parameter-estimates",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nHere we extract all parameter estimates using the as_draws_df method for brmsfit objects. Note that we could also just extract a subset of parameters, see ?as_draws.brmsfit for more information.\nAs mentioned above, we use the expectation of the log-normal distribution to get the population level estimates (called \\(\\theta\\) with the corresponding subscript) for the \\(b_0\\), \\(k_s\\), and \\(k_g\\) parameters. We also calculate the coefficient of variation for each parameter.\n\n\nShow the code\npost_df &lt;- as_draws_df(fit)\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"r_id__lb0[1,Intercept]\" \"r_id__lb0[2,Intercept]\"\n[10] \"r_id__lb0[3,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nFor a graphical check of the convergence, we can use the mcmc_trace and mcmc_pairs functions from the bayesplot package:\n\n\nShow the code\nsf_pop_params &lt;- c(\"theta_b0\", \"theta_ks\", \"theta_kg\", \"omega_0\", \"omega_s\", \"omega_g\", \"sigma\")\n\nmcmc_trace(post_df, pars = sf_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = sf_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nNext, we can create a nice summary table of the parameter estimates, using summarize_draws from the posterior package in combination with functions from the gt package:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  select(theta_b0, theta_ks, theta_kg, omega_0, omega_s, omega_g, cv_0, cv_s, cv_g, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.0\n44.0\n1.03\n1.01\n42.3\n45.7\n1.00\n423\n726\n\n\ntheta_ks\n1.13\n1.12\n0.104\n0.101\n0.972\n1.32\n1.00\n445\n915\n\n\ntheta_kg\n0.476\n0.475\n0.0311\n0.0318\n0.426\n0.528\n1.00\n941\n1,320\n\n\nomega_0\n0.579\n0.579\n0.0162\n0.0166\n0.552\n0.605\n1.00\n676\n1,220\n\n\nomega_s\n1.40\n1.40\n0.0753\n0.0745\n1.28\n1.52\n1.00\n699\n1,560\n\n\nomega_g\n0.958\n0.956\n0.0446\n0.0446\n0.888\n1.04\n1.00\n882\n1,720\n\n\ncv_0\n0.631\n0.631\n0.0208\n0.0212\n0.597\n0.665\n1.00\n676\n1,220\n\n\ncv_s\n2.49\n2.46\n0.311\n0.295\n2.02\n3.04\n1.00\n699\n1,560\n\n\ncv_g\n1.23\n1.22\n0.0883\n0.0869\n1.10\n1.39\n1.00\n882\n1,720\n\n\nsigma\n0.161\n0.161\n0.00230\n0.00229\n0.157\n0.165\n1.00\n2,190\n2,860\n\n\n\n\n\n\n\nWe can also look at parameters for individual patients. This is simplified by using the spread_draws() function:\n\n\nShow the code\n# Understand the names of the random effects:\nhead(get_variables(fit), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"r_id__lb0[1,Intercept]\" \"r_id__lb0[2,Intercept]\"\n[10] \"r_id__lb0[3,Intercept]\"\n\n\nShow the code\npost_ind &lt;- fit |&gt; \n  spread_draws(\n    b_lb0_Intercept,\n    r_id__lb0[id, ],\n    b_lks_Intercept,\n    r_id__lks[id, ],\n    b_lkg_Intercept,\n    r_id__lkg[id, ]\n  ) |&gt; \n  mutate(\n    b0 = exp(b_lb0_Intercept + r_id__lb0),\n    ks = exp(b_lks_Intercept + r_id__lks),\n    kg = exp(b_lkg_Intercept + r_id__lkg)\n  ) |&gt; \n  select(\n    .chain, .iteration, .draw,\n    id, b0, ks, kg    \n  )\n\n\nNote that here we do not need to use the log-normal expectation formula, because we just calculate the individual random effects here, in contrast to the population level parameters above.\nWith this we can e.g. report the estimates for the first patient:\n\n\nShow the code\npost_sum_id1 &lt;- post_ind |&gt;\n  filter(id == \"1\") |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_sum_id1\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\n1\n\n\nb0\n43.3\n42.8\n4.80\n4.35\n36.5\n52.2\n1.00\n9,080\n2,890\n\n\nks\n0.486\n0.313\n0.529\n0.309\n0.0392\n1.50\n1.00\n6,300\n3,140\n\n\nkg\n0.395\n0.294\n0.338\n0.241\n0.0620\n1.04\n1.00\n7,690\n3,080",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#observation-vs-model-fit",
    "href": "session-tgi/1_tgi_sf_brms.html#observation-vs-model-fit",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again.\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_linpred_draws(fit) |&gt; \n  median_qi() |&gt; \n  mutate(sld_pred = .linpred)\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = sld_pred, ymin = .lower, ymax = .upper),\n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = sld_pred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"SF model fit\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#summary-statistics",
    "href": "session-tgi/1_tgi_sf_brms.html#summary-statistics",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Summary statistics",
    "text": "Summary statistics\nThe time-to-growth formula is:\n\\[\n\\max \\left(\n    \\frac{\n        \\log(k_s) - \\log(k_g)\n    }{\n        k_s + k_g\n    },\n    0\n\\right)\n\\]\nSimilary, we can look at the tumor-ratio at time \\(t\\):\n\\[\n\\frac{y^{*}(t)}{y^{*}(0)} = \\exp(-k_s \\cdot t) + \\exp(k_g \\cdot t) - 1\n\\]\nSo with the posterior samples from above, we can calculate these statistics, separate for each patient, with the tumor-ratio e.g. for 12 weeks (and being careful with the year time scale we used in this model):\n\n\nShow the code\npost_ind_stat &lt;- post_ind |&gt; \n  mutate(\n    ttg = pmax((log(ks) - log(kg)) / (ks + kg), 0),\n    tr12 = exp(-ks * 12/52) + exp(kg * 12/52) - 1\n  )\n\n\nThen we can look at e.g. the first 3 patients parameter estimates:\n\n\nShow the code\npost_ind_stat_sum &lt;- post_ind_stat |&gt;\n  filter(id %in% c(\"1\", \"2\", \"3\")) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_ind_stat_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\n1\n\n\nb0\n43.3\n42.8\n4.80\n4.35\n36.5\n52.2\n1.00\n9,080\n2,890\n\n\nks\n0.486\n0.313\n0.529\n0.309\n0.0392\n1.50\n1.00\n6,300\n3,140\n\n\nkg\n0.395\n0.294\n0.338\n0.241\n0.0620\n1.04\n1.00\n7,690\n3,080\n\n\nttg\n0.785\n0.106\n1.36\n0.158\n0\n3.47\n1.00\n4,440\n3,180\n\n\ntr12\n0.999\n1.00\n0.115\n0.0834\n0.809\n1.18\n1.00\n5,740\n3,400\n\n\n2\n\n\nb0\n37.9\n37.6\n4.10\n3.79\n31.9\n45.1\n1.00\n7,520\n2,750\n\n\nks\n0.396\n0.246\n0.491\n0.237\n0.0355\n1.21\n1.00\n5,330\n3,380\n\n\nkg\n0.548\n0.416\n0.445\n0.357\n0.0735\n1.42\n1.00\n5,860\n2,870\n\n\nttg\n0.501\n0\n1.21\n0\n0\n2.92\n0.999\n3,500\n2,820\n\n\ntr12\n1.06\n1.04\n0.125\n0.0939\n0.886\n1.29\n1.00\n5,810\n3,220\n\n\n3\n\n\nb0\n21.0\n20.7\n2.49\n2.23\n17.6\n25.6\n1.00\n4,310\n2,900\n\n\nks\n1.00\n0.817\n0.662\n0.541\n0.229\n2.34\n1.00\n2,930\n2,760\n\n\nkg\n0.275\n0.249\n0.165\n0.185\n0.0529\n0.575\n1.00\n3,480\n2,790\n\n\nttg\n1.50\n1.01\n1.39\n0.681\n0.418\n4.27\n1.00\n4,440\n2,740\n\n\ntr12\n0.869\n0.884\n0.0815\n0.0720\n0.709\n0.980\n1.00\n3,170\n2,920",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#adding-covariates",
    "href": "session-tgi/1_tgi_sf_brms.html#adding-covariates",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Adding covariates",
    "text": "Adding covariates\nWith brms it is straightforward to add covariates to the model. In this example, an obvious choice for a covariate is the treatment the patients received in the study:\n\n\nShow the code\ndf |&gt; \n  select(id, arm) |&gt; \n  distinct() |&gt;\n  pull(arm) |&gt; \n  table()\n\n\n\n  1   2 \n325 376 \n\n\nHere it makes sense to assume that only the shrinkage and growth parameters differ systematically between the two arms. For example, the baseline SLD should be the same in expectation, because of the randomization between the treatment arms.\nTherefore we can add this binary arm covariate as follows as a fixed effect for the two population level mean parameters \\(\\mu_{k_s}\\) and \\(\\mu_{k_g}\\) on the log scale:\n\n\nShow the code\nformula_by_arm &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood:\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (exp(-ks * year) + exp(kg * year) -  1)) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # As above we also use these formulas here:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + arm + (1 | id)) +\n  lf(lkg ~ 1 + arm + (1 | id))\n\n\nIt is instructive to see that the Stan code that is generated in the background is actually identical to the previous model, except for the prior specification. This is because brms uses a design matrix to model the fixed effects, and the arm variable is just added to this design matrix, which before only contained the intercept column with 1s.\nHowever, now the coefficient vector is no longer of length 1 but of length 2, with the first element corresponding to the intercept and the second element to the effect of the arm variable (for the level “2”). For the latter, we want to assume a standard normal prior on the log scale.\nSo we need to adjust the prior and initial values accordingly:\n\n\nShow the code\npriors_by_arm &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  # Note the changes here:\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\", coef = \"Intercept\"),\n  prior(normal(0, 1), nlpar = \"lks\", coef = \"arm2\"), \n  prior(normal(log(1.04), 1), nlpar = \"lkg\", coef = \"Intercept\"),\n  prior(normal(0, 1), nlpar = \"lkg\", coef = \"arm2\"),\n  # Same as before:\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\ninits_by_arm &lt;- list(\n  b_lb0 = array(3.61),\n  # Note the changes here:\n  b_lks = array(c(-1.25, 0)),\n  b_lkg = array(c(-1.33, 0)),\n  # Same as before:\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n\nNow we can fit the model as before:\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/fit10.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit_by_arm &lt;- brm(\n    formula = formula_by_arm,\n    data = df,\n    prior = priors_by_arm,\n    family = gaussian(),\n    init = rep(list(inits_by_arm), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit_by_arm, file = save_file)\n}\nsummary(fit_by_arm)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + arm + (1 | id)\n         lkg ~ 1 + arm + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.01      356      987\nsd(lks_Intercept)     1.48      0.09     1.31     1.66 1.01      402      917\nsd(lkg_Intercept)     0.97      0.05     0.88     1.07 1.01      323      789\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.16      0.00     0.16     0.17 1.00     1725     2755\nlb0_Intercept     3.61      0.02     3.57     3.66 1.00      199      519\nlks_Intercept    -0.78      0.09    -0.95    -0.61 1.00     1204     2303\nlks_arm2         -0.40      0.16    -0.73    -0.09 1.01      383      823\nlkg_Intercept    -1.26      0.11    -1.48    -1.05 1.01      494      853\nlkg_arm2          0.03      0.13    -0.22     0.28 1.01      460      975\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s have a look at the parameter estimates then:\n\n\nShow the code\npost_df_by_arm &lt;- as_draws_df(fit_by_arm)\nhead(names(post_df_by_arm), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lks_arm2\"             \"b_lkg_Intercept\"        \"b_lkg_arm2\"            \n [7] \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"   \"sd_id__lkg_Intercept\"  \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df_by_arm &lt;- post_df_by_arm |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks_arm1 = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_ks_arm2 = exp(b_lks_Intercept + b_lks_arm2 + sd_id__lks_Intercept^2 / 2),\n    theta_kg_arm1 = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_kg_arm2 = exp(b_lkg_Intercept + b_lkg_arm2 + sd_id__lkg_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\npost_sum_by_arm &lt;- post_df_by_arm |&gt;\n  select(\n    theta_b0, theta_ks_arm1, theta_ks_arm2, theta_kg_arm1, theta_kg_arm2, \n    omega_0, omega_s, omega_g, cv_0, cv_s, cv_g, sigma\n  ) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum_by_arm\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.9\n43.9\n1.10\n1.11\n42.1\n45.8\n1.00\n195\n515\n\n\ntheta_ks_arm1\n1.39\n1.36\n0.193\n0.178\n1.11\n1.74\n1.01\n396\n841\n\n\ntheta_ks_arm2\n0.925\n0.918\n0.112\n0.108\n0.755\n1.12\n1.00\n306\n814\n\n\ntheta_kg_arm1\n0.456\n0.454\n0.0461\n0.0456\n0.382\n0.535\n1.01\n575\n1,000\n\n\ntheta_kg_arm2\n0.469\n0.468\n0.0425\n0.0423\n0.402\n0.544\n1.00\n496\n923\n\n\nomega_0\n0.578\n0.578\n0.0161\n0.0160\n0.552\n0.605\n1.00\n345\n979\n\n\nomega_s\n1.48\n1.48\n0.0909\n0.0899\n1.33\n1.63\n1.01\n397\n900\n\n\nomega_g\n0.968\n0.965\n0.0485\n0.0474\n0.892\n1.05\n1.00\n343\n808\n\n\ncv_0\n0.630\n0.630\n0.0206\n0.0205\n0.597\n0.665\n1.00\n345\n979\n\n\ncv_s\n2.84\n2.79\n0.445\n0.416\n2.21\n3.65\n1.01\n397\n900\n\n\ncv_g\n1.25\n1.24\n0.0976\n0.0929\n1.10\n1.43\n1.00\n343\n808\n\n\nsigma\n0.161\n0.161\n0.00229\n0.00230\n0.157\n0.165\n1.00\n1,700\n2,710\n\n\n\n\n\n\n\nSo we see that the shrinkage is stronger in arm 1 compared to arm 2, while the growth rates are similar. We could also calculate the posterior probability that the growth rate is different between the two arms, for example:\n\n\nShow the code\npost_df_by_arm |&gt;\n  mutate(diff_pos = theta_ks_arm1 - theta_ks_arm2 &gt; 0) |&gt;\n  summarize(diff_prob = mean(diff_pos))\n\n\n# A tibble: 1 × 1\n  diff_prob\n      &lt;dbl&gt;\n1     0.996\n\n\nSo we see that the posterior probability that the shrinkage rate is higher in arm 1 compared to arm 2 is quite high.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#model-comparison-with-loo",
    "href": "session-tgi/1_tgi_sf_brms.html#model-comparison-with-loo",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Model comparison with LOO",
    "text": "Model comparison with LOO\nThe LOO criterion is a widely used method for comparing models. It is based on the idea of leave-one-out cross-validation, but is more efficient to compute.\nWith brms, it is easy to compute the LOO criterion:\n\n\nShow the code\nloo(fit)\n\n\nWarning: Found 374 observations with a pareto_k &gt; 0.7 in model 'fit'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\n\nComputed from 4000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12990.0  97.3\np_loo      1185.2  40.4\nlooic     25980.0 194.7\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.1, 2.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     3725  90.9%   56      \n   (0.7, 1]   (bad)       303   7.4%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   71   1.7%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nA helpful glossary explaining the LOO statistics can be found here.\nDifferent criteria are available, for example the elpd_loo is the expected log pointwise predictive density, and the looic = -2 * elpd_loo is the LOO information criterion. For comparing models, the looic is often used, where smaller numbers are better. it is kind of an equivalent to the AIC, but based on the LOO criterion.\nLet’s e.g. compare the two models we fitted above, one for the whole dataset and one with the treatment arm as a covariate for the shrinkage and growth rates:\n\n\nShow the code\nfit &lt;- add_criterion(fit, \"loo\")\n\n\nWarning: Found 374 observations with a pareto_k &gt; 0.7 in model 'fit'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nShow the code\nfit_by_arm &lt;- add_criterion(fit_by_arm, \"loo\")\n\n\nWarning: Found 359 observations with a pareto_k &gt; 0.7 in model 'fit_by_arm'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nShow the code\nloo_compare(fit, fit_by_arm)\n\n\n           elpd_diff se_diff\nfit         0.0       0.0   \nfit_by_arm -0.2       5.9   \n\n\nSo the model without treatment arm seems to be very slightly preferred here by the LOO criterion. Nevertheless, the model with treatment arm might answer exactly the question we need to answer, so it is important to consider the context of the analysis.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#tips-and-tricks",
    "href": "session-tgi/1_tgi_sf_brms.html#tips-and-tricks",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Tips and tricks",
    "text": "Tips and tricks\n\nWhen the model breaks down completely, that is a sign that likely the model specification is wrong. For example, in above code I first did not understand how to model the sigma parameter correctly, which led to a model where each chain was completely stuck at its initial values.\nIn that case and in general if you are not sure whether the brms model specification is correct, you can check the Stan code that is generated by brms:\n\n\nShow the code\n# Good to check the stan code:\n# (This also helps to find the names of the parameters\n# for which to define the initial values below)\nstancode(formula, prior = priors, data = df, family = gaussian())\n\n\n// generated with brms 2.23.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=1&gt; K_tau;  // number of population-level effects\n  matrix[N, K_tau] X_tau;  // population-level design matrix\n  int&lt;lower=1&gt; K_lb0;  // number of population-level effects\n  matrix[N, K_lb0] X_lb0;  // population-level design matrix\n  int&lt;lower=1&gt; K_lks;  // number of population-level effects\n  matrix[N, K_lks] X_lks;  // population-level design matrix\n  int&lt;lower=1&gt; K_lkg;  // number of population-level effects\n  matrix[N, K_lkg] X_lkg;  // population-level design matrix\n  // covariates for non-linear functions\n  vector[N] C_ystar_1;\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_lb0_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_lks_1;\n  // data for group-level effects of ID 3\n  int&lt;lower=1&gt; N_3;  // number of grouping levels\n  int&lt;lower=1&gt; M_3;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_3;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_3_lkg_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector&lt;lower=0&gt;[K_tau] b_tau;  // regression coefficients\n  vector[K_lb0] b_lb0;  // regression coefficients\n  vector[K_lks] b_lks;  // regression coefficients\n  vector[K_lkg] b_lkg;  // regression coefficients\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_3] sd_3;  // group-level standard deviations\n  array[M_3] vector[N_3] z_3;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_lb0_1;  // actual group-level effects\n  vector[N_2] r_2_lks_1;  // actual group-level effects\n  vector[N_3] r_3_lkg_1;  // actual group-level effects\n  // prior contributions to the log posterior\n  real lprior = 0;\n  r_1_lb0_1 = (sd_1[1] * (z_1[1]));\n  r_2_lks_1 = (sd_2[1] * (z_2[1]));\n  r_3_lkg_1 = (sd_3[1] * (z_3[1]));\n  lprior += normal_lpdf(b_tau | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(b_lb0 | log(65), 1);\n  lprior += normal_lpdf(b_lks | log(0.52), 0.1);\n  lprior += normal_lpdf(b_lkg | log(1.04), 1);\n  lprior += normal_lpdf(sd_1 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(sd_2 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(sd_3 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] nlp_tau = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lb0 = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lks = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lkg = rep_vector(0.0, N);\n    // initialize non-linear predictor term\n    vector[N] nlp_b0;\n    // initialize non-linear predictor term\n    vector[N] nlp_ks;\n    // initialize non-linear predictor term\n    vector[N] nlp_kg;\n    // initialize non-linear predictor term\n    vector[N] nlp_ystar;\n    // initialize non-linear predictor term\n    vector[N] mu;\n    // initialize non-linear predictor term\n    vector[N] sigma;\n    nlp_tau += X_tau * b_tau;\n    nlp_lb0 += X_lb0 * b_lb0;\n    nlp_lks += X_lks * b_lks;\n    nlp_lkg += X_lkg * b_lkg;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lb0[n] += r_1_lb0_1[J_1[n]] * Z_1_lb0_1[n];\n    }\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lks[n] += r_2_lks_1[J_2[n]] * Z_2_lks_1[n];\n    }\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lkg[n] += r_3_lkg_1[J_3[n]] * Z_3_lkg_1[n];\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_b0[n] = (exp(nlp_lb0[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_ks[n] = (exp(nlp_lks[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_kg[n] = (exp(nlp_lkg[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_ystar[n] = (int_step(C_ystar_1[n] &gt; 0) * (nlp_b0[n] * (exp( - nlp_ks[n] * C_ystar_1[n]) + exp(nlp_kg[n] * C_ystar_1[n]) - 1)) + int_step(C_ystar_1[n] &lt;= 0) * (nlp_b0[n] * exp(nlp_kg[n] * C_ystar_1[n])));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      mu[n] = (nlp_ystar[n]);\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      sigma[n] = exp(log(nlp_tau[n]) + log(nlp_ystar[n]));\n    }\n    target += normal_lpdf(Y | mu, sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n  target += std_normal_lpdf(z_3[1]);\n}\ngenerated quantities {\n}\n\n\nHere e.g. it is important to see that the sigma parameter is modelled on the log scale.\nWe can also extract the actual data that is passed to the Stan program:\n\n\nShow the code\n# Extract the data that is passed to the Stan program\nstan_data &lt;- standata(formula, prior = priors, data = df, family = gaussian())\n\n# Check that the time variable is correct\nall(stan_data$C_eta_1 == df$year)\n\n\n[1] TRUE\n\n\nThis can be useful to check if the data is passed correctly to the Stan program.\nIt is important to take divergence warnings seriously. For the model parameters where Rhat is larger than 1.05 or so, you can just have a look at the traceplot. For example, in an earlier model fit the \\(\\log(\\mu_{\\phi})\\) population parameter had this traceplot:\n\n\nShow the code\nfit_save &lt;- fit\nload(here(\"session-tgi/fit.RData\"))\nplot(fit, pars = \"b_tphi\")\n\n\nWarning: Argument 'pars' is deprecated. Please use 'variable' instead.\n\n\n\n\n\n\n\n\n\nShow the code\nfit &lt;- fit_save\n\n\nWe see that two chains led to a \\(\\log(\\mu_{\\phi})\\) value below 0, while two chains hovered around much larger values between 5 and 15, which corresponds to \\(\\mu_{\\phi} = 1\\). This shows that the model is rather overparametrized. By restricting the range of the prior for \\(\\log(\\mu_{\\phi})\\) to be below 0, we could avoid this problem.\nProviding manually realistic initial values for the model parameters can help to speed up the convergence of the Markov chains. This is especially important for the nonlinear parameters. You can get the names of the parameters from the Stan code above - note that you cannot use the names of the brms R code.\nSometimes it is not easy to get the dimensions of the inital values right. Here it can be helpful to change the backend to rstan, which provides more detailed error messages. You can do this by setting backend = \"rstan\" in the brm call.\nWhen the Stan compiler gives you an error, that can be very informative: Just open the Stan code file that is mentioned in the error message and look at the line number that is mentioned. This can give you a good idea of what went wrong: Maybe a typo in the prior definition, or a missing semicolon, or a wrong dimension in the data block. With VScode e.g. this is very easy: Just hold Ctrl and click on the file name and number, and you will be taken to the right line in the Stan code.\nIn order to quickly get results for a model, e.g. for obtaining useful starting values or prior distributions, it can be worth trying the “Pathfinder” algorithm in brms. Pathfinder is a variational method for approximately sampling from differentiable log densities. You can use it by setting algorithm = \"pathfinder\" in the brm call. In this example it takes only a minute, compared to more than an hour for the full model fit. However, the results are still quite different, so it is likely only useful as a first approximation. Nevertheless, the individual model fits look quite encouraging, in the sense that they have found good parameter values - but they are still lacking any uncertainty, so could not be used for confidence or prediction intervals:\n\n\nShow the code\nfit_fast_file &lt;- here(\"session-tgi/fit_fast.RData\")\nif (file.exists(fit_fast_file)) {\n  load(fit_fast_file)\n} else {\n  fit_fast &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH,\n    algorithm = \"pathfinder\"\n  )\n  save(fit_fast, file = fit_fast_file)\n}\n\nsummary(fit_fast)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ ystar \n         ystar ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(ystar)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 1 chains, each with iter = 1000; warmup = 0; thin = 1;\n         total post-warmup draws = 1000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     3.37      0.00     3.37     3.37   NA       NA       NA\nsd(lks_Intercept)     9.16      0.00     9.16     9.16   NA       NA       NA\nsd(lkg_Intercept)     6.90      0.00     6.90     6.90   NA       NA       NA\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.13      0.00     0.13     0.13   NA       NA       NA\nlb0_Intercept     3.62      0.00     3.62     3.62   NA       NA       NA\nlks_Intercept    -0.49      0.00    -0.49    -0.49   NA       NA       NA\nlkg_Intercept    -0.66      0.00    -0.66    -0.66   NA       NA       NA\n\nDraws were sampled using pathfinder(). \n\n\nShow the code\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_linpred_draws(fit_fast) |&gt; \n  median_qi() |&gt; \n  mutate(sld_pred = .linpred)\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = sld_pred, ymin = .lower, ymax = .upper),\n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = sld_pred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"SF model fit (pathfinder approximation)\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html",
    "href": "session-tgi/2_tgi_sf_jmpost.html",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "",
    "text": "Let’s try to fit the same model now with jmpost. We will use the same data as in the previous notebook.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#setup-and-load-data",
    "href": "session-tgi/2_tgi_sf_jmpost.html#setup-and-load-data",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#data-preparation",
    "href": "session-tgi/2_tgi_sf_jmpost.html#data-preparation",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Data preparation",
    "text": "Data preparation\nWe start with the subject level data set. For the beginning, we want to treat all observations as if they come from a single arm and single study for now. Therefore we insert constant study and arm values here.\n\n\nShow the code\nsubj_df &lt;- data.frame(\n  id = unique(df$id),\n  arm = \"arm\",\n  study = \"study\"\n)\nsubj_data &lt;- DataSubject(\n  data = subj_df,\n  subject = \"id\",\n  arm = \"arm\",\n  study = \"study\"\n)\n\n\nNext we prepare the longitudinal data object.\n\n\nShow the code\nlong_df &lt;- df |&gt;\n  select(id, year, sld)\nlong_data &lt;- DataLongitudinal(\n  data = long_df,\n  formula = sld ~ year\n)\n\n\nNow we can create the JointData object:\n\n\nShow the code\njoint_data &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data\n)",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#model-specification",
    "href": "session-tgi/2_tgi_sf_jmpost.html#model-specification",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Model specification",
    "text": "Model specification\nThe statistical model is specified in the jmpost vignette here.\nHere we just want to fit the longitudinal data, therefore:\n\n\nShow the code\ntgi_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    )\n)\n\n\nNote that the priors on the standard deviations, omega_* and sigma, are truncated to the positive domain. So we used here truncated normal priors.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#fit-model",
    "href": "session-tgi/2_tgi_sf_jmpost.html#fit-model",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using jmpost.\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/jm5.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  mcmc_results &lt;- sampleStanModel(\n      tgi_mod,\n      data = joint_data,\n      iter_sampling = ITER,\n      iter_warmup = WARMUP,\n      chains = CHAINS,\n      parallel_chains = CHAINS,\n      thin = CHAINS,\n      seed = BAYES.SEED,\n      refresh = REFRESH\n  )\n  save(mcmc_results, file = save_file)\n}\n\n\nLet’s check the convergence of the population parameters:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nsave_overall_file &lt;- here(\"session-tgi/jm5_more.RData\")\nif (file.exists(save_overall_file)) {\n  load(save_overall_file)\n} else {\n  mcmc_res_cmdstan &lt;- cmdstanr::as.CmdStanMCMC(mcmc_results)\n  mcmc_res_sum &lt;- mcmc_res_cmdstan$summary(vars)\n  vars_draws &lt;- mcmc_res_cmdstan$draws(vars)\n  loo_res &lt;- mcmc_res_cmdstan$loo(r_eff = FALSE)\n  save(mcmc_res_sum, vars_draws, loo_res, file = save_overall_file)\n}\nmcmc_res_sum\n\n\n# A tibble: 7 × 10\n  variable     mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lm_sf_mu_…  3.61   3.61  0.0236  0.0232   3.57   3.65   1.02     142.     188.\n2 lm_sf_mu_… -1.27  -1.27  0.160   0.152   -1.54  -1.03   1.01     352.     500.\n3 lm_sf_mu_… -1.35  -1.35  0.0828  0.0789  -1.49  -1.22   1.00     230.     456.\n4 lm_sf_sig…  0.161  0.161 0.00233 0.00244  0.158  0.165  1.00     513.     875.\n5 lm_sf_ome…  0.579  0.578 0.0156  0.0153   0.555  0.608  1.00     341.     576.\n6 lm_sf_ome…  1.62   1.62  0.117   0.112    1.45   1.82   1.01     321.     509.\n7 lm_sf_ome…  0.997  0.993 0.0505  0.0503   0.920  1.08   1.00     390.     595.\n\n\nThis looks good, let’s check the traceplots:\n\n\nShow the code\n# vars_draws &lt;- mcmc_res_cmdstan$draws(vars)\nmcmc_trace(vars_draws)\n\n\n\n\n\n\n\n\n\nThey also look ok, all chains are mixing well in the same range of parameter values.\nAlso here we could look at the pairs plot:\n\n\nShow the code\nmcmc_pairs(\n  vars_draws,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#observation-vs.-model-fit",
    "href": "session-tgi/2_tgi_sf_jmpost.html#observation-vs.-model-fit",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Observation vs. model fit",
    "text": "Observation vs. model fit\nLet’s check the fit of the model to the data:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\n\nsave_fit_file &lt;- here(\"session-tgi/jm5_fit.RData\")\nif (file.exists(save_fit_file)) {\n  load(save_fit_file)\n} else {\n  fit_subset &lt;- LongitudinalQuantities(\n    mcmc_results, \n    grid = GridObserved(subjects = pt_subset)\n  )\n  save(fit_subset, file = save_fit_file)\n}\n\nautoplot(fit_subset)+\n  labs(x = \"Time (years)\", y = \"SLD (mm)\")\n\n\n\n\n\n\n\n\n\nSo this works very nicely.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#prior-vs.-posterior",
    "href": "session-tgi/2_tgi_sf_jmpost.html#prior-vs.-posterior",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Prior vs. posterior",
    "text": "Prior vs. posterior\nLet’s check the prior vs. posterior for the parameters:\n\n\nShow the code\npost_samples &lt;- as_draws_df(vars_draws) |&gt; \n  rename(\n    mu_bsld = \"lm_sf_mu_bsld[1]\",\n    mu_ks = \"lm_sf_mu_ks[1]\",\n    mu_kg = \"lm_sf_mu_kg[1]\",\n    omega_bsld = \"lm_sf_omega_bsld[1]\",\n    omega_ks = \"lm_sf_omega_ks[1]\",\n    omega_kg = \"lm_sf_omega_kg[1]\",\n    sigma = lm_sf_sigma\n  ) |&gt; \n  mutate(type = \"posterior\") |&gt; \n  select(mu_bsld, mu_ks, mu_kg, omega_bsld, omega_ks, omega_kg, sigma, type)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\nn_prior_samples &lt;- nrow(post_samples)\nprior_samples &lt;- data.frame(\n    mu_bsld = rnorm(n_prior_samples, log(65), 1),\n    mu_ks = rnorm(n_prior_samples, log(0.52), 1),\n    mu_kg = rnorm(n_prior_samples, log(1.04), 1),\n    omega_bsld = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    omega_ks = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    omega_kg = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    sigma = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3)\n  ) |&gt; \n  mutate(type = \"prior\")\n\n# Combine the two\ncombined_samples &lt;- rbind(post_samples, prior_samples) |&gt; \n  pivot_longer(cols = -type, names_to = \"parameter\", values_to = \"value\")\n\nggplot(combined_samples, aes(x = value, fill = type)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~parameter, scales = \"free\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis looks good, because the priors are covering the range of the posterior samples and are not too informative.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#parameter-estimates",
    "href": "session-tgi/2_tgi_sf_jmpost.html#parameter-estimates",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nHere we need again to be careful: We are interested in the posterior mean estimates of the baseline, shrinkage and growth population rates on the original scale. Because we model them on the log scale as normal distributed, we need to use the mean of the log-normal distribution to get the mean on the original scale.\n\n\nShow the code\npost_sum &lt;- post_samples |&gt;\n  mutate(\n    theta_b0 = exp(mu_bsld + omega_bsld^2 / 2), \n    theta_ks = exp(mu_ks + omega_ks^2 / 2), \n    theta_kg = exp(mu_kg + omega_kg^2 / 2),\n    cv_0 = sqrt(exp(omega_bsld^2) - 1),\n    cv_s = sqrt(exp(omega_ks^2) - 1),\n    cv_g = sqrt(exp(omega_kg^2) - 1)\n  ) |&gt;\n  select(theta_b0, theta_ks, theta_kg, omega_bsld, omega_ks, omega_kg, cv_0, cv_s, cv_g, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.7\n43.8\n1.10\n1.12\n42.0\n45.5\n1.00\n151\n201\n\n\ntheta_ks\n1.06\n1.05\n0.129\n0.120\n0.871\n1.30\n1.00\n226\n408\n\n\ntheta_kg\n0.428\n0.427\n0.0311\n0.0306\n0.378\n0.479\n1.00\n318\n472\n\n\nomega_bsld\n0.579\n0.578\n0.0156\n0.0153\n0.555\n0.608\n1.00\n346\n545\n\n\nomega_ks\n1.62\n1.62\n0.117\n0.112\n1.45\n1.82\n1.00\n318\n494\n\n\nomega_kg\n0.997\n0.993\n0.0505\n0.0503\n0.920\n1.08\n1.00\n390\n576\n\n\ncv_0\n0.631\n0.630\n0.0200\n0.0196\n0.600\n0.668\n1.00\n346\n545\n\n\ncv_s\n3.70\n3.57\n0.818\n0.685\n2.66\n5.16\n1.00\n318\n494\n\n\ncv_g\n1.31\n1.30\n0.106\n0.103\n1.15\n1.49\n1.00\n390\n576\n\n\nsigma\n0.161\n0.161\n0.00233\n0.00244\n0.158\n0.165\n1.00\n495\n844\n\n\n\n\n\n\n\nWe can see that these are consistent with the estimates from the brms model earlier.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#separate-arm-estimates",
    "href": "session-tgi/2_tgi_sf_jmpost.html#separate-arm-estimates",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Separate arm estimates",
    "text": "Separate arm estimates\nWhile there is no general covariates support in jmpost for the longitudinal models as of now, we can obtain separate estimates for the longitudinal model parameters: As detailed in the model specification here, as soon as we have the arm defined, then separate estimates for the arm-specific shrinkage and growth parameters will be obtained: Both the population means and standard deviation parameters are here arm-specific. (Note that this is slightly different from brms where we assumed earlier the same standard deviation independent of the treatment arm.)\nSo we need to define the subject data accordingly now with the arm information:\n\n\nShow the code\nsubj_df_by_arm &lt;- df |&gt;\n  select(id, arm) |&gt;\n  distinct() |&gt; \n  mutate(study = \"study\")\n\nsubj_data_by_arm &lt;- DataSubject(\n  data = subj_df_by_arm,\n  subject = \"id\",\n  arm = \"arm\",\n  study = \"study\"\n)\n\n\nWe redefine the JointData object and can then fit the model, because the prior specification does not need to change: We assume iid priors on the arm-specific parameters here.\n\n\nShow the code\njoint_data_by_arm &lt;- DataJoint(\n    subject = subj_data_by_arm,\n    longitudinal = long_data\n)\n\n\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/jm6.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  mcmc_results_by_arm &lt;- sampleStanModel(\n      tgi_mod,\n      data = joint_data_by_arm,\n      iter_sampling = ITER,\n      iter_warmup = WARMUP,\n      chains = CHAINS,\n      parallel_chains = CHAINS,\n      thin = CHAINS,\n      seed = BAYES.SEED,\n      refresh = REFRESH\n  )\n  save(mcmc_results_by_arm, file = save_file)\n}\n\n\nLet’s again check the convergence:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nsave_arm_file &lt;- here(\"session-tgi/jm6_more.RData\")\nif (file.exists(save_arm_file)) {\n  load(save_arm_file)\n} else {\n  mcmc_res_cmdstan_by_arm &lt;- cmdstanr::as.CmdStanMCMC(mcmc_results_by_arm)\n  mcmc_res_sum_by_arm &lt;- mcmc_res_cmdstan_by_arm$summary(vars)\n  vars_draws_by_arm &lt;- mcmc_res_cmdstan_by_arm$draws(vars)\n  loo_by_arm &lt;- mcmc_res_cmdstan_by_arm$loo(r_eff = FALSE)\n  save(mcmc_res_sum_by_arm, vars_draws_by_arm, loo_by_arm, file = save_arm_file)\n}\nmcmc_res_sum_by_arm\n\n\n# A tibble: 11 × 10\n   variable    mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu…  3.61   3.61  0.0211  0.0217   3.58   3.65  1.00      244.     502.\n 2 lm_sf_mu… -0.752 -0.730 0.222   0.216   -1.14  -0.398 1.01      592.     673.\n 3 lm_sf_mu… -1.52  -1.51  0.222   0.217   -1.90  -1.17  1.00      706.     850.\n 4 lm_sf_mu… -1.11  -1.11  0.138   0.139   -1.35  -0.900 1.00      594.     814.\n 5 lm_sf_mu… -1.35  -1.35  0.103   0.104   -1.52  -1.19  1.00      560.     776.\n 6 lm_sf_si…  0.161  0.161 0.00224 0.00228  0.157  0.165 1.00      892.     941.\n 7 lm_sf_om…  0.577  0.577 0.0159  0.0153   0.551  0.603 1.00      511.     672.\n 8 lm_sf_om…  1.34   1.33  0.150   0.152    1.11   1.60  1.00      630.     844.\n 9 lm_sf_om…  1.76   1.76  0.161   0.150    1.51   2.05  1.00      662.     990.\n10 lm_sf_om…  0.765  0.758 0.0710  0.0685   0.662  0.891 0.999     809.     951.\n11 lm_sf_om…  1.10   1.10  0.0705  0.0684   0.991  1.22  0.999     669.     736.\n\n\nLet’s again tabulate the parameter estimates:\n\n\nShow the code\npost_samples_by_arm &lt;- as_draws_df(vars_draws_by_arm) |&gt; \n  rename(\n    mu_bsld = \"lm_sf_mu_bsld[1]\",\n    mu_ks1 = \"lm_sf_mu_ks[1]\",\n    mu_ks2 = \"lm_sf_mu_ks[2]\",\n    mu_kg1 = \"lm_sf_mu_kg[1]\",\n    mu_kg2 = \"lm_sf_mu_kg[2]\",\n    omega_bsld = \"lm_sf_omega_bsld[1]\",\n    omega_ks1 = \"lm_sf_omega_ks[1]\",\n    omega_ks2 = \"lm_sf_omega_ks[2]\",\n    omega_kg1 = \"lm_sf_omega_kg[1]\",\n    omega_kg2 = \"lm_sf_omega_kg[2]\",\n    sigma = lm_sf_sigma\n  ) |&gt; \n  mutate(\n    theta_b0 = exp(mu_bsld + omega_bsld^2 / 2), \n    theta_ks1 = exp(mu_ks1 + omega_ks1^2 / 2), \n    theta_ks2 = exp(mu_ks2 + omega_ks2^2 / 2),\n    theta_kg1 = exp(mu_kg1 + omega_kg1^2 / 2),\n    theta_kg2 = exp(mu_kg2 + omega_kg2^2 / 2),\n    cv_0 = sqrt(exp(omega_bsld^2) - 1),\n    cv_s1 = sqrt(exp(omega_ks1^2) - 1),\n    cv_s2 = sqrt(exp(omega_ks2^2) - 1),\n    cv_g1 = sqrt(exp(omega_kg1^2) - 1),\n    cv_g2 = sqrt(exp(omega_kg2^2) - 1)\n  ) \n  \npost_sum_by_arm &lt;- post_samples_by_arm |&gt;\n  select(\n    theta_b0, theta_ks1, theta_ks2, theta_kg1, theta_kg2, \n    omega_bsld, omega_ks1, omega_ks2, omega_kg1, omega_kg2, \n    cv_0, cv_s1, cv_s2, cv_g1, cv_g2, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum_by_arm\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.7\n43.7\n0.999\n1.05\n42.1\n45.4\n1.00\n227\n558\n\n\ntheta_ks1\n1.18\n1.17\n0.152\n0.146\n0.961\n1.44\n1.00\n530\n696\n\n\ntheta_ks2\n1.07\n1.04\n0.209\n0.183\n0.779\n1.42\n1.00\n344\n443\n\n\ntheta_kg1\n0.444\n0.443\n0.0490\n0.0489\n0.365\n0.527\n1.00\n539\n712\n\n\ntheta_kg2\n0.479\n0.478\n0.0487\n0.0479\n0.399\n0.563\n1.00\n735\n908\n\n\nomega_bsld\n0.577\n0.577\n0.0159\n0.0153\n0.551\n0.603\n1.00\n504\n659\n\n\nomega_ks1\n1.34\n1.33\n0.150\n0.152\n1.11\n1.60\n1.00\n620\n838\n\n\nomega_ks2\n1.76\n1.76\n0.161\n0.150\n1.51\n2.05\n1.00\n651\n972\n\n\nomega_kg1\n0.765\n0.758\n0.0710\n0.0685\n0.662\n0.891\n0.999\n799\n943\n\n\nomega_kg2\n1.10\n1.10\n0.0705\n0.0684\n0.991\n1.22\n1.00\n649\n693\n\n\ncv_0\n0.629\n0.628\n0.0205\n0.0196\n0.596\n0.662\n1.00\n504\n659\n\n\ncv_s1\n2.33\n2.22\n0.601\n0.547\n1.55\n3.46\n1.00\n620\n838\n\n\ncv_s2\n4.90\n4.58\n1.64\n1.25\n2.96\n8.11\n1.00\n651\n972\n\n\ncv_g1\n0.897\n0.880\n0.112\n0.105\n0.742\n1.10\n0.999\n799\n943\n\n\ncv_g2\n1.55\n1.54\n0.175\n0.163\n1.29\n1.85\n1.00\n649\n693\n\n\nsigma\n0.161\n0.161\n0.00224\n0.00228\n0.157\n0.165\n1.00\n885\n881\n\n\n\n\n\n\n\nHere again the shrinkage rate in the treatment arm 1 seems higher than in the treatment arm 2. However, the difference is not as pronounced as in the brms model before with the same standard deviation for both arms. We can again calculate the posterior probability that the shrinkage rate in arm 1 is higher than in arm 2:\n\n\nShow the code\nprob_ks1_greater_ks2 &lt;- mean(post_samples_by_arm$theta_ks1 &gt; post_samples_by_arm$theta_ks2)\nprob_ks1_greater_ks2\n\n\n[1] 0.706\n\n\nSo the posterior probability is now only around 71%.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#model-comparison-with-loo",
    "href": "session-tgi/2_tgi_sf_jmpost.html#model-comparison-with-loo",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Model comparison with LOO",
    "text": "Model comparison with LOO\nAs we have seen for brms, also for jmpost we can easily compute the LOO criterion:\n\n\nShow the code\n# loo_res &lt;- mcmc_res_cmdstan$loo(r_eff = FALSE)\nloo_res\n\n\n\nComputed from 1000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12957.3  95.8\np_loo      1137.1  38.4\nlooic     25914.5 191.6\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nPareto k diagnostic values:\n                          Count Pct.    Min. ESS\n(-Inf, 0.67]   (good)     3665  89.4%   26      \n   (0.67, 1]   (bad)       363   8.9%   &lt;NA&gt;    \n    (1, Inf)   (very bad)   71   1.7%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nUnderneath this is using the $loo() method from cmdstanr.\nAnd we can compare this to the LOO of the model with separate arm estimates:\n\n\nShow the code\n# loo_by_arm &lt;- mcmc_res_cmdstan_by_arm$loo(r_eff = FALSE)\nloo_by_arm\n\n\n\nComputed from 1000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12931.8  95.6\np_loo      1121.5  36.8\nlooic     25863.5 191.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nPareto k diagnostic values:\n                          Count Pct.    Min. ESS\n(-Inf, 0.67]   (good)     3652  89.1%   48      \n   (0.67, 1]   (bad)       374   9.1%   &lt;NA&gt;    \n    (1, Inf)   (very bad)   73   1.8%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nSo the model by treatment arm performs here better than the model without treatment arm specific growth and shrinkage parameters.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#tipps-and-tricks",
    "href": "session-tgi/2_tgi_sf_jmpost.html#tipps-and-tricks",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Tipps and tricks",
    "text": "Tipps and tricks\n\nAlso here it is possible to look at the underlying Stan code:\n\n\nShow the code\ntmp &lt;- tempfile(fileext = \".stan\") # file extension for syntax highlighting\nwrite_stan(tgi_mod, destination = tmp)\nfile.edit(tmp) # opens the Stan file in the default editor\n\n\nIt is not trivial to transport saved models from one computer to another. This is because cmdstanr only loads the results it currently needs from disk into memory, and thus into the R session. If you want to transport the model to another computer, you need to save the Stan code and the data, and then re-run the model on the other computer. This is because the model object in R is only a reference to the model on disk, not the model itself. Note that there is the $save_object() method, see here, however this leads to very large files (here about 300 MB for one fit) and can thus not be uploaded to typical git repositories. Therefore above we saved interim result objects separately as needed.\nIt is important to explicitly define the truncation boundaries for the truncated normal priors, because otherwise the MCMC results will not be correct.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-pts/0_setup.html",
    "href": "session-pts/0_setup.html",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "First we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 4: PTS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-pts/0_setup.html#setup",
    "href": "session-pts/0_setup.html#setup",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "First we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 4: PTS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-pts/0_setup.html#data-preparation",
    "href": "session-pts/0_setup.html#data-preparation",
    "title": "0. Setup and data preparation",
    "section": "Data preparation",
    "text": "Data preparation\nWe prepare now the same dataset as before in the Joint Model session.\nWe will again use the OAK study data.\nFor the tumor growth data, we are using the S1 data set from here. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\ntumor_data &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\") |&gt; \n    read_excel(sheet = \"Study4\") |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm)),\n        arm = fct_recode(arm, \"Docetaxel\" = \"1\", \"MPDL3280A\" = \"2\")\n    ) |&gt; \n    select(id, year, sld, arm)\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 4\n  id                       year   sld arm      \n  &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 -594245265511412736  -0.00821  42   Docetaxel\n2 -594245265511412736   0.0986   44   Docetaxel\n3 -594245265511412736   0.222    44   Docetaxel\n4 -4078394139718744064 -0.0110   65.8 Docetaxel\n5 -4078394139718744064  0.832    66   Docetaxel\n6 -4078394139718744064  1.07     68   Docetaxel\n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        \n 5394902984416364544 :  16   Min.   :-0.1068   Min.   :  2.00  \n 7160320731596789760 :  16   1st Qu.: 0.1123   1st Qu.: 19.05  \n -4159496062492130816:  15   Median : 0.2519   Median : 32.00  \n -7900338178541499392:  15   Mean   : 0.3995   Mean   : 38.16  \n 4878279915140903936 :  15   3rd Qu.: 0.5749   3rd Qu.: 51.00  \n 9202154259772598272 :  15   Max.   : 2.0753   Max.   :228.00  \n (Other)             :4034                     NA's   :27      \n        arm      \n Docetaxel:1658  \n MPDL3280A:2468  \n                 \n                 \n                 \n                 \n                 \n\n\nHere we have 701 patients, and we know from Fig. 1 in the publication that this is the subset of patients with at least 3 tumor size measurements. We have guessed from the second Excel sheet with parameter estimates that arm 1 means docetaxel and arm 2 means MPDL3280A. This also seems reasonable given the larger number of measurements per patient:\n\n\nShow the code\n# make a table\ntumor_data |&gt; \n    group_by(id) |&gt; \n    summarize(arm = arm[1], n = n())  |&gt; \n    group_by(arm) |&gt; \n    summarize(mean_n = mean(n))\n\n\n# A tibble: 2 × 2\n  arm       mean_n\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Docetaxel   5.10\n2 MPDL3280A   6.56\n\n\nFor the OS data, we will be using the Supplementary Table 8 from here. Again, we have copied the data set in this GitHub repository.\nFrom the supplementary material, we know that:\n\nPFS/OS: time in months\nPFS.CNSR/OS.CNSR: 1 for censor, 0 for event\n\nTherefore we will convert the time to years and the censoring to a logical variable:\n\n\nShow the code\nos_data &lt;- here(\"data/41591_2018_134_MOESM3_ESM.xlsx\") |&gt; \n    read_excel(sheet = \"OAK_Clinical_Data\") |&gt; \n    clean_names() |&gt; \n    rename(\n        age = bage,\n        race = race2,\n        ecog = ecoggr,\n        arm = trt01p,\n        response = bcor\n    ) |&gt; \n    mutate(\n        id = factor(as.character(pt_id)),\n        sld = as.numeric(na_if(bl_sld, \".\")),\n        pfs_time = as.numeric(pfs) / 12,\n        pfs_event = as.numeric(pfs_cnsr) == 0,\n        os_time = as.numeric(os) / 12,\n        os_event = as.numeric(os_cnsr) == 0,\n        race = factor(race),\n        ecog = factor(ecog),\n        arm = factor(arm),\n        response = factor(\n          response, \n          levels = c(\"CR\", \"PR\", \"SD\", \"PD\", \"NE\")\n        ),\n        sex = factor(sex)\n    ) |&gt; \n    select(\n      id,\n      arm,\n      ecog,\n      age,\n      race,\n      sex,\n      sld,\n      response,\n      pfs_time,\n      pfs_event,\n      os_time,\n      os_event\n    )\n\nhead(os_data)\n\n\n# A tibble: 6 × 12\n  id    arm    ecog    age race  sex     sld response pfs_time pfs_event os_time\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;lgl&gt;       &lt;dbl&gt;\n1 318   Docet… 1        64 WHITE M     152   PD          0.107 TRUE        0.433\n2 1088  Docet… 0        65 WHITE M      36   PD          0.194 TRUE        0.402\n3 345   Docet… 1        75 WHITE M      92   &lt;NA&gt;        0.162 TRUE        0.162\n4 588   Docet… 0        61 WHITE F      36   SD          1.02  TRUE        2.05 \n5 306   MPDL3… 1        53 WHITE F      86   PD          0.118 TRUE        0.498\n6 718   Docet… 1        80 WHITE M      45.7 SD          0.712 TRUE        1.60 \n# ℹ 1 more variable: os_event &lt;lgl&gt;\n\n\nShow the code\nsummary(os_data)\n\n\n       id             arm      ecog         age           race     sex    \n 1000   :  1   Docetaxel:425   0:315   Min.   :33.00   ASIAN:180   F:330  \n 1001   :  1   MPDL3280A:425   1:535   1st Qu.:57.00   OTHER: 72   M:520  \n 1002   :  1                           Median :64.00   WHITE:598          \n 1003   :  1                           Mean   :63.23                      \n 1004   :  1                           3rd Qu.:70.00                      \n 1005   :  1                           Max.   :85.00                      \n (Other):844                                                              \n      sld         response      pfs_time        pfs_event      \n Min.   : 10.00   CR  :  7   Min.   :0.002738   Mode :logical  \n 1st Qu.: 41.00   PR  :108   1st Qu.:0.117728   FALSE:95       \n Median : 66.00   SD  :327   Median :0.240931   TRUE :755      \n Mean   : 76.69   PD  :304   Mean   :0.443174                  \n 3rd Qu.: 99.00   NE  : 24   3rd Qu.:0.580424                  \n Max.   :316.00   NA's: 80   Max.   :2.242300                  \n NA's   :1                                                     \n    os_time          os_event      \n Min.   :0.002738   Mode :logical  \n 1st Qu.:0.364134   FALSE:281      \n Median :0.803559   TRUE :569      \n Mean   :0.949841                  \n 3rd Qu.:1.620808                  \n Max.   :2.253251                  \n                                   \n\n\nNow we would like to join the two data sets, i.e. know which tumor growth data belongs to which baseline covariates and overall survival time and event indicator. Unfortunately we cannot use the id variable as it is not consistent between the two data sets, because both were anonymized. Instead, we will use the sld variable in combination with the treatment arm. We can also double check with the best overall response variable, because that is closely related to the tumor growth.\nFirst, we will summarize the patient information int he tumor data set. We will calculate the baseline SLD, determine the time of the last tumor measurement and approximate the best overall response by looking at the change from baseline:\n\n\nShow the code\nget_baseline &lt;- function(sld, year) {\n    which_base &lt;- tail(which(year &lt;= 0), 1L)\n    if (length(which_base) == 0) {\n      which_base &lt;- which.min(year)\n    }\n    sld[which_base]\n}\n\nget_contig_below_thresh &lt;- function(sld, year, bsld, thresh) {\n    rle_res &lt;- with(\n        rle(((sld[year &gt; 0] - bsld) / bsld) &lt; 0.2), \n        lengths[values]\n    )\n    if (length(rle_res) == 0) {\n      0\n    } else {\n      max(rle_res)\n    }\n}\n\ntumor_data_summary &lt;- tumor_data |&gt; \n    group_by(id) |&gt; \n    arrange(year) |&gt;\n    summarize(\n      arm = arm[1L],\n      bsld = get_baseline(sld, year),\n      last_year = tail(year, 1L),\n      nadir = min(sld[year &gt;= 0], na.rm = TRUE),\n      max_cfn = max((sld[year &gt;= 0] - nadir) / nadir, na.rm = TRUE),\n      min_cfb = min((sld[year &gt;= 0] - bsld) / bsld, na.rm = TRUE),\n      contig_below_0.2 = get_contig_below_thresh(sld, year, bsld, thresh = 0.2),\n      approx_response = case_when(\n        min_cfb &lt;= -0.3 ~ \"PR\",\n        contig_below_0.2 &gt;= 2 ~ \"SD\",\n        max_cfn &gt;= 0.2 ~ \"PD\",        \n        .default = \"NE\"\n      )   \n    )\nhead(tumor_data_summary)\n\n\n# A tibble: 6 × 9\n  id                arm    bsld last_year nadir max_cfn min_cfb contig_below_0.2\n  &lt;fct&gt;             &lt;fct&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n1 -100096848245159… Doce…    53     0.356    55  0       0.0377                3\n2 -102777882269110… MPDL…    11     0.901     2  2      -0.818                 7\n3 -105049343897269… MPDL…    20     0.345    21  1.62    0.05                  2\n4 -106934201929572… Doce…    15     1.72     11  0.273  -0.267                12\n5 -108949367808374… Doce…    33     0.709    31  0.0968 -0.0606                6\n6 -110624773747042… MPDL…    24     1.32     16  0.562  -0.333                 9\n# ℹ 1 more variable: approx_response &lt;chr&gt;\n\n\nNow let’s try to fuzzy join this with the OS data set, based on the baseline SLD and the treatment arm. In addition, we know that the last time point in the tumor data needs to be before the OS time point. We will also check that the best overall response is the same in both data sets. There will be some errors here: For example, we don’t know whether there were new lesions leading to a progression assessment, or whether the patient had later better results. So we need to expect that the matching will not be perfect.\n\n\nShow the code\nos_data_keys &lt;- os_data |&gt; \n    select(id, arm, sld, pfs_time, os_time, response)\nhead(os_data_keys)\n\n\n# A tibble: 6 × 6\n  id    arm         sld pfs_time os_time response\n  &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n1 318   Docetaxel 152      0.107   0.433 PD      \n2 1088  Docetaxel  36      0.194   0.402 PD      \n3 345   Docetaxel  92      0.162   0.162 &lt;NA&gt;    \n4 588   Docetaxel  36      1.02    2.05  SD      \n5 306   MPDL3280A  86      0.118   0.498 PD      \n6 718   Docetaxel  45.7    0.712   1.60  SD      \n\n\nShow the code\ndist_match &lt;- function(v1, v2) {\n    dist &lt;- abs(v1 - v2)\n    data.frame(include = (dist &lt;= 0.05))\n}\n\nless_match &lt;- function(lower, upper) {\n    data.frame(include = lower &lt;= upper)\n}\n\ntumor_os_data_joined &lt;- tumor_data_summary |&gt; \n    fuzzy_left_join(\n        os_data_keys,\n        by = c(\n          \"arm\" = \"arm\", \n          \"bsld\" = \"sld\",          \n          \"last_year\" = \"os_time\",\n          \"approx_response\" = \"response\"\n        ),\n        match_fun = list(\n          `==`, \n          dist_match, \n          less_match,\n          `==`\n        )\n    )\nnrow(tumor_os_data_joined)\n\n\n[1] 954\n\n\nWe see that we cannot match the two data sets exactly:\n\nWe have less patients to start with in the tumor size data set.\nWe have patients in the tumor size data set which match multiple patients in the OS data set based on the treatment arm and the baseline SLD, and also the last measurement time point cannot disambiguate them.\nWe have patients in the tumor size data set which do not match any patient in the OS data set.\n\nHowever, for the purpose of this training we don’t need to be able to recover the true full data set. Therefore we will just subset the matched data set to a reasonable combination of the two data sets:\n\n\nShow the code\ntumor_os_data_joined_subset &lt;- tumor_os_data_joined |&gt; \n    na.omit() |&gt; \n    filter(!duplicated(id.y)) |&gt;\n    filter(!duplicated(id.x))\nnrow(tumor_os_data_joined_subset)\n\n\n[1] 203\n\n\nNow let’s save the joining information:\n\n\nShow the code\ntgi_os_join_keys &lt;- tumor_os_data_joined_subset |&gt; \n    select(id.x, id.y, arm.x) |&gt; \n    rename(\n        id_tgi = id.x,\n        id_os = id.y,\n        arm = arm.x\n    )\ntable(tgi_os_join_keys$arm)\n\n\n\nDocetaxel MPDL3280A \n       96       107 \n\n\nSo we have about 100 patients in each arm.\nWe subset the two data sets accordingly:\n\n\nShow the code\ntumor_data &lt;- tumor_data |&gt; \n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_tgi\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id) |&gt; \n    rename(id = id_os)\n\nos_data &lt;- os_data |&gt;\n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_os\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id_tgi)",
    "crumbs": [
      "Session 4: PTS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-jm/0_setup.html",
    "href": "session-jm/0_setup.html",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "First we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 3: TGI-OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-jm/0_setup.html#setup",
    "href": "session-jm/0_setup.html#setup",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "First we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 3: TGI-OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-jm/0_setup.html#data-preparation",
    "href": "session-jm/0_setup.html#data-preparation",
    "title": "0. Setup and data preparation",
    "section": "Data preparation",
    "text": "Data preparation\nWe prepare now the same dataset as before in the OS session.\nWe will again use the OAK study data.\nFor the tumor growth data, we are using the S1 data set from here. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\ntumor_data &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\") |&gt; \n    read_excel(sheet = \"Study4\") |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm)),\n        arm = fct_recode(arm, \"Docetaxel\" = \"1\", \"MPDL3280A\" = \"2\")\n    ) |&gt; \n    select(id, year, sld, arm)\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 4\n  id                       year   sld arm      \n  &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 -594245265511412736  -0.00821  42   Docetaxel\n2 -594245265511412736   0.0986   44   Docetaxel\n3 -594245265511412736   0.222    44   Docetaxel\n4 -4078394139718744064 -0.0110   65.8 Docetaxel\n5 -4078394139718744064  0.832    66   Docetaxel\n6 -4078394139718744064  1.07     68   Docetaxel\n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        \n 5394902984416364544 :  16   Min.   :-0.1068   Min.   :  2.00  \n 7160320731596789760 :  16   1st Qu.: 0.1123   1st Qu.: 19.05  \n -4159496062492130816:  15   Median : 0.2519   Median : 32.00  \n -7900338178541499392:  15   Mean   : 0.3995   Mean   : 38.16  \n 4878279915140903936 :  15   3rd Qu.: 0.5749   3rd Qu.: 51.00  \n 9202154259772598272 :  15   Max.   : 2.0753   Max.   :228.00  \n (Other)             :4034                     NA's   :27      \n        arm      \n Docetaxel:1658  \n MPDL3280A:2468  \n                 \n                 \n                 \n                 \n                 \n\n\nHere we have 701 patients, and we know from Fig. 1 in the publication that this is the subset of patients with at least 3 tumor size measurements. We have guessed from the second Excel sheet with parameter estimates that arm 1 means docetaxel and arm 2 means MPDL3280A. This also seems reasonable given the larger number of measurements per patient:\n\n\nShow the code\n# make a table\ntumor_data |&gt; \n    group_by(id) |&gt; \n    summarize(arm = arm[1], n = n())  |&gt; \n    group_by(arm) |&gt; \n    summarize(mean_n = mean(n))\n\n\n# A tibble: 2 × 2\n  arm       mean_n\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Docetaxel   5.10\n2 MPDL3280A   6.56\n\n\nFor the OS data, we will be using the Supplementary Table 8 from here. Again, we have copied the data set in this GitHub repository.\nFrom the supplementary material, we know that:\n\nPFS/OS: time in months\nPFS.CNSR/OS.CNSR: 1 for censor, 0 for event\n\nTherefore we will convert the time to years and the censoring to a logical variable:\n\n\nShow the code\nos_data &lt;- here(\"data/41591_2018_134_MOESM3_ESM.xlsx\") |&gt; \n    read_excel(sheet = \"OAK_Clinical_Data\") |&gt; \n    clean_names() |&gt; \n    rename(\n        age = bage,\n        race = race2,\n        ecog = ecoggr,\n        arm = trt01p,\n        response = bcor\n    ) |&gt; \n    mutate(\n        id = factor(as.character(pt_id)),\n        sld = as.numeric(na_if(bl_sld, \".\")),\n        pfs_time = as.numeric(pfs) / 12,\n        pfs_event = as.numeric(pfs_cnsr) == 0,\n        os_time = as.numeric(os) / 12,\n        os_event = as.numeric(os_cnsr) == 0,\n        race = factor(race),\n        ecog = factor(ecog),\n        arm = factor(arm),\n        response = factor(\n          response, \n          levels = c(\"CR\", \"PR\", \"SD\", \"PD\", \"NE\")\n        ),\n        sex = factor(sex)\n    ) |&gt; \n    select(\n      id,\n      arm,\n      ecog,\n      age,\n      race,\n      sex,\n      sld,\n      response,\n      pfs_time,\n      pfs_event,\n      os_time,\n      os_event\n    )\n\nhead(os_data)\n\n\n# A tibble: 6 × 12\n  id    arm    ecog    age race  sex     sld response pfs_time pfs_event os_time\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;lgl&gt;       &lt;dbl&gt;\n1 318   Docet… 1        64 WHITE M     152   PD          0.107 TRUE        0.433\n2 1088  Docet… 0        65 WHITE M      36   PD          0.194 TRUE        0.402\n3 345   Docet… 1        75 WHITE M      92   &lt;NA&gt;        0.162 TRUE        0.162\n4 588   Docet… 0        61 WHITE F      36   SD          1.02  TRUE        2.05 \n5 306   MPDL3… 1        53 WHITE F      86   PD          0.118 TRUE        0.498\n6 718   Docet… 1        80 WHITE M      45.7 SD          0.712 TRUE        1.60 \n# ℹ 1 more variable: os_event &lt;lgl&gt;\n\n\nShow the code\nsummary(os_data)\n\n\n       id             arm      ecog         age           race     sex    \n 1000   :  1   Docetaxel:425   0:315   Min.   :33.00   ASIAN:180   F:330  \n 1001   :  1   MPDL3280A:425   1:535   1st Qu.:57.00   OTHER: 72   M:520  \n 1002   :  1                           Median :64.00   WHITE:598          \n 1003   :  1                           Mean   :63.23                      \n 1004   :  1                           3rd Qu.:70.00                      \n 1005   :  1                           Max.   :85.00                      \n (Other):844                                                              \n      sld         response      pfs_time        pfs_event      \n Min.   : 10.00   CR  :  7   Min.   :0.002738   Mode :logical  \n 1st Qu.: 41.00   PR  :108   1st Qu.:0.117728   FALSE:95       \n Median : 66.00   SD  :327   Median :0.240931   TRUE :755      \n Mean   : 76.69   PD  :304   Mean   :0.443174                  \n 3rd Qu.: 99.00   NE  : 24   3rd Qu.:0.580424                  \n Max.   :316.00   NA's: 80   Max.   :2.242300                  \n NA's   :1                                                     \n    os_time          os_event      \n Min.   :0.002738   Mode :logical  \n 1st Qu.:0.364134   FALSE:281      \n Median :0.803559   TRUE :569      \n Mean   :0.949841                  \n 3rd Qu.:1.620808                  \n Max.   :2.253251                  \n                                   \n\n\nNow we would like to join the two data sets, i.e. know which tumor growth data belongs to which baseline covariates and overall survival time and event indicator. Unfortunately we cannot use the id variable as it is not consistent between the two data sets, because both were anonymized. Instead, we will use the sld variable in combination with the treatment arm. We can also double check with the best overall response variable, because that is closely related to the tumor growth.\nFirst, we will summarize the patient information int he tumor data set. We will calculate the baseline SLD, determine the time of the last tumor measurement and approximate the best overall response by looking at the change from baseline:\n\n\nShow the code\nget_baseline &lt;- function(sld, year) {\n    which_base &lt;- tail(which(year &lt;= 0), 1L)\n    if (length(which_base) == 0) {\n      which_base &lt;- which.min(year)\n    }\n    sld[which_base]\n}\n\nget_contig_below_thresh &lt;- function(sld, year, bsld, thresh) {\n    rle_res &lt;- with(\n        rle(((sld[year &gt; 0] - bsld) / bsld) &lt; 0.2), \n        lengths[values]\n    )\n    if (length(rle_res) == 0) {\n      0\n    } else {\n      max(rle_res)\n    }\n}\n\ntumor_data_summary &lt;- tumor_data |&gt; \n    group_by(id) |&gt; \n    arrange(year) |&gt;\n    summarize(\n      arm = arm[1L],\n      bsld = get_baseline(sld, year),\n      last_year = tail(year, 1L),\n      nadir = min(sld[year &gt;= 0], na.rm = TRUE),\n      max_cfn = max((sld[year &gt;= 0] - nadir) / nadir, na.rm = TRUE),\n      min_cfb = min((sld[year &gt;= 0] - bsld) / bsld, na.rm = TRUE),\n      contig_below_0.2 = get_contig_below_thresh(sld, year, bsld, thresh = 0.2),\n      approx_response = case_when(\n        min_cfb &lt;= -0.3 ~ \"PR\",\n        contig_below_0.2 &gt;= 2 ~ \"SD\",\n        max_cfn &gt;= 0.2 ~ \"PD\",        \n        .default = \"NE\"\n      )   \n    )\nhead(tumor_data_summary)\n\n\n# A tibble: 6 × 9\n  id                arm    bsld last_year nadir max_cfn min_cfb contig_below_0.2\n  &lt;fct&gt;             &lt;fct&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n1 -100096848245159… Doce…    53     0.356    55  0       0.0377                3\n2 -102777882269110… MPDL…    11     0.901     2  2      -0.818                 7\n3 -105049343897269… MPDL…    20     0.345    21  1.62    0.05                  2\n4 -106934201929572… Doce…    15     1.72     11  0.273  -0.267                12\n5 -108949367808374… Doce…    33     0.709    31  0.0968 -0.0606                6\n6 -110624773747042… MPDL…    24     1.32     16  0.562  -0.333                 9\n# ℹ 1 more variable: approx_response &lt;chr&gt;\n\n\nNow let’s try to fuzzy join this with the OS data set, based on the baseline SLD and the treatment arm. In addition, we know that the last time point in the tumor data needs to be before the OS time point. We will also check that the best overall response is the same in both data sets. There will be some errors here: For example, we don’t know whether there were new lesions leading to a progression assessment, or whether the patient had later better results. So we need to expect that the matching will not be perfect.\n\n\nShow the code\nos_data_keys &lt;- os_data |&gt; \n    select(id, arm, sld, pfs_time, os_time, response)\nhead(os_data_keys)\n\n\n# A tibble: 6 × 6\n  id    arm         sld pfs_time os_time response\n  &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n1 318   Docetaxel 152      0.107   0.433 PD      \n2 1088  Docetaxel  36      0.194   0.402 PD      \n3 345   Docetaxel  92      0.162   0.162 &lt;NA&gt;    \n4 588   Docetaxel  36      1.02    2.05  SD      \n5 306   MPDL3280A  86      0.118   0.498 PD      \n6 718   Docetaxel  45.7    0.712   1.60  SD      \n\n\nShow the code\ndist_match &lt;- function(v1, v2) {\n    dist &lt;- abs(v1 - v2)\n    data.frame(include = (dist &lt;= 0.05))\n}\n\nless_match &lt;- function(lower, upper) {\n    data.frame(include = lower &lt;= upper)\n}\n\ntumor_os_data_joined &lt;- tumor_data_summary |&gt; \n    fuzzy_left_join(\n        os_data_keys,\n        by = c(\n          \"arm\" = \"arm\", \n          \"bsld\" = \"sld\",          \n          \"last_year\" = \"os_time\",\n          \"approx_response\" = \"response\"\n        ),\n        match_fun = list(\n          `==`, \n          dist_match, \n          less_match,\n          `==`\n        )\n    )\nnrow(tumor_os_data_joined)\n\n\n[1] 954\n\n\nWe see that we cannot match the two data sets exactly:\n\nWe have less patients to start with in the tumor size data set.\nWe have patients in the tumor size data set which match multiple patients in the OS data set based on the treatment arm and the baseline SLD, and also the last measurement time point cannot disambiguate them.\nWe have patients in the tumor size data set which do not match any patient in the OS data set.\n\nHowever, for the purpose of this training we don’t need to be able to recover the true full data set. Therefore we will just subset the matched data set to a reasonable combination of the two data sets:\n\n\nShow the code\ntumor_os_data_joined_subset &lt;- tumor_os_data_joined |&gt; \n    na.omit() |&gt; \n    filter(!duplicated(id.y)) |&gt;\n    filter(!duplicated(id.x))\nnrow(tumor_os_data_joined_subset)\n\n\n[1] 203\n\n\nNow let’s save the joining information:\n\n\nShow the code\ntgi_os_join_keys &lt;- tumor_os_data_joined_subset |&gt; \n    select(id.x, id.y, arm.x) |&gt; \n    rename(\n        id_tgi = id.x,\n        id_os = id.y,\n        arm = arm.x\n    )\ntable(tgi_os_join_keys$arm)\n\n\n\nDocetaxel MPDL3280A \n       96       107 \n\n\nSo we have about 100 patients in each arm.\nWe subset the two data sets accordingly:\n\n\nShow the code\ntumor_data &lt;- tumor_data |&gt; \n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_tgi\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id) |&gt; \n    rename(id = id_os)\n\nos_data &lt;- os_data |&gt;\n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_os\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id_tgi)",
    "crumbs": [
      "Session 3: TGI-OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html",
    "href": "session-os/2_os_weibull_brms.html",
    "title": "2. OS model minimal workflow with brms",
    "section": "",
    "text": "Let’s try to fit the same model now with brms. We will use the same data as in the previous notebook.",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html#setup-and-load-data",
    "href": "session-os/2_os_weibull_brms.html#setup-and-load-data",
    "title": "2. OS model minimal workflow with brms",
    "section": "Setup and load data",
    "text": "Setup and load data\nHere we directly start from the overall survival data with the log kg estimates, as we have obtained them in the previous notebook:\n\n\nShow the code\nos_data_with_log_kg &lt;- readRDS(here(\"session-os/os_data_with_log_kg.rds\"))\nhead(os_data_with_log_kg)\n\n\n# A tibble: 6 × 9\n  id    arm       ecog    age race  sex   os_time os_event log_kg_est\n  &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;\n1 588   Docetaxel 0        61 WHITE F       2.05  FALSE       -0.571 \n2 330   MPDL3280A 1        56 WHITE F       1.68  FALSE       -1.89  \n3 791   Docetaxel 0        72 WHITE F       0.901 TRUE        -0.518 \n4 635   Docetaxel 0        42 OTHER F       1.66  TRUE        -0.642 \n5 365   MPDL3280A 0        64 WHITE F       1.43  TRUE        -0.427 \n6 773   Docetaxel 0        65 WHITE M       1.63  FALSE        0.0340",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html#model-fitting-with-brms",
    "href": "session-os/2_os_weibull_brms.html#model-fitting-with-brms",
    "title": "2. OS model minimal workflow with brms",
    "section": "Model fitting with brms",
    "text": "Model fitting with brms\nLet’s first fit the model with brms. We will use the same (first) model as in the previous notebook, but we will use the brms package to fit it.\nAn important ingredient for the model formula is the censoring information, passed via the cens() syntax: This should point to a variable containing the value 0 for observed events, i.e. no censoring, and the value 1 for right censored times (see ?brmsformula for more details). Therefore we first add such a variable to the data set:\n\n\nShow the code\nos_data_with_log_kg &lt;- os_data_with_log_kg |&gt;\n    mutate(\n        os_cens = ifelse(os_event, 0, 1)\n    )\n\n\nWe define our own design matrix with a column of ones:\n\n\nShow the code\nos_data_with_log_kg_design &lt;- model.matrix(\n    ~ os_time + os_cens + ecog + age + race + sex + log_kg_est,\n    data = os_data_with_log_kg\n) |&gt;\n    as.data.frame() |&gt;\n    rename(ones = \"(Intercept)\")\nhead(os_data_with_log_kg_design)\n\n\n  ones   os_time os_cens ecog1 age raceOTHER raceWHITE sexM  log_kg_est\n1    1 2.0506502       1     0  61         0         1    0 -0.57141474\n2    1 1.6755647       1     1  56         0         1    0 -1.88737445\n3    1 0.9007529       0     0  72         0         1    0 -0.51807234\n4    1 1.6591376       0     0  42         1         0    0 -0.64165688\n5    1 1.4291581       0     0  64         0         1    0 -0.42656542\n6    1 1.6290212       1     0  65         0         1    1  0.03404769\n\n\nNow we can define the model formula:\n\n\nShow the code\nformula &lt;- bf(\n    os_time | cens(os_cens) ~\n        0 +\n        ones +\n        ecog1 +\n        age +\n        raceOTHER +\n        raceWHITE +\n        sexM +\n        log_kg_est\n)\n\n\nSo here we suppress the automatic intercept provided by brms by using the 0 + syntax, and instead we our “own” vector of ones. This is because we want to avoid the default centering of covariates which is performed by brms when using an automatic intercept. Otherwise it would be difficult to exactly match the prior distributions we used in the jmpost model further below.\nIn order to find out about the parametrization of the Weibull model with brms and Stan here, let’s check the Stan code generated by brms for it:\n\n\nShow the code\nstancode(formula, data = os_data_with_log_kg_design, family = weibull())\n\n\n// generated with brms 2.23.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; shape;  // shape parameter\n}\ntransformed parameters {\n  // prior contributions to the log posterior\n  real lprior = 0;\n  lprior += gamma_lpdf(shape | 0.01, 0.01);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += X * b;\n    mu = exp(mu);\n    // vectorized log-likelihood contributions of censored data\n    target += weibull_lpdf(Y[Jevent[1:Nevent]] | shape, mu[Jevent[1:Nevent]] ./ tgamma(1 + 1 ./ shape));\n    target += weibull_lccdf(Y[Jrcens[1:Nrcens]] | shape, mu[Jrcens[1:Nrcens]] ./ tgamma(1 + 1 ./ shape));\n    target += weibull_lcdf(Y[Jlcens[1:Nlcens]] | shape, mu[Jlcens[1:Nlcens]] ./ tgamma(1 + 1 ./ shape));\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\nWe can see that the Stan code uses the weibull_* function family to define the log-likelihood contributions.\nWe can check the Stan reference doc here for details of the parametrization. We can see that this is the so-called “standard” parametrization (see Wikipedia) with shape parameter \\(\\alpha\\) and scale parameter \\(\\sigma\\). The mean of this distribution is \\(\\sigma \\Gamma(1 + 1/\\alpha)\\), where \\(\\Gamma\\) is the gamma function. We can see in the brms generated code that accordingly the sigma parameter is defined as mu / tgamma(1 + 1 / shape), such that mu is really the mean of the distribution.\nNow the problem is that this is a different parametrization than what we have used in jmpost (see the specification), which is the proportional hazards parametrization (see Wikipedia), where the covariate effects are on the log hazard scale instead of on the log mean scale. This has been identified by other brms users as a gap in the package, see e.g. here. So we can hope that this will be added in the future, but for now we need to implement a workaround.\nFortunately, we can define a custom distribution in brms to use the proportional hazards parametrization. This parametrization relates to the Stan Weibull density definition with the transformation of \\(\\sigma := \\gamma^{-1 / \\alpha}\\). The code here has been first written by Bjoern Holzhauer and was extended by Sebastian Weber to integrate more tightly with brms (source). One thing to keep in mind here is that for technical reasons the first parameter of the custom distribution needs to be named mu and not gamma.\n\n\nShow the code\nfamily_weibull_ph &lt;- function(link_gamma = \"log\", link_alpha = \"log\") {\n    brms::custom_family(\n        name = \"weibull_ph\",\n        # first param needs to be \"mu\" cannot be \"gamma\"; alpha is the shape:\n        dpars = c(\"mu\", \"alpha\"),\n        links = c(link_gamma, link_alpha),\n        lb = c(0, 0),\n        # ub = c(NA, NA), # would be redundant\n        # no need for `vars` like for `cens`, brms can handle this.\n        type = \"real\",\n        loop = TRUE\n    )\n}\n\nsv_weibull_ph &lt;- brms::stanvar(\n    name = \"weibull_ph_stan_code\",\n    scode = \"\nreal weibull_ph_lpdf(real y, real mu, real alpha) {\n  // real sigma = pow(1 / mu, 1 / alpha);\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lpdf(y | alpha, sigma);\n}\nreal weibull_ph_lccdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lccdf(y | alpha, sigma);\n}\nreal weibull_ph_lcdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lcdf(y | alpha, sigma);\n}\nreal weibull_ph_rng(real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_rng(alpha, sigma);\n}\n\",\n    block = \"functions\"\n)\n\n## R definitions of auxilary helper functions of brms, these are based\n## on the respective weibull (internal) brms implementations:\n\nlog_lik_weibull_ph &lt;- function(i, prep) {\n    shape &lt;- get_dpar(prep, \"alpha\", i = i)\n    sigma &lt;- get_dpar(prep, \"mu\", i = i)^(-1 / shape)\n    args &lt;- list(shape = shape, scale = sigma)\n    out &lt;- brms:::log_lik_censor(\n        dist = \"weibull\",\n        args = args,\n        i = i,\n        prep = prep\n    )\n    out &lt;- brms:::log_lik_truncate(\n        out,\n        cdf = pweibull,\n        args = args,\n        i = i,\n        prep = prep\n    )\n    brms:::log_lik_weight(out, i = i, prep = prep)\n}\n\nposterior_predict_weibull_ph &lt;- function(i, prep, ntrys = 5, ...) {\n    shape &lt;- get_dpar(prep, \"alpha\", i = i)\n    sigma &lt;- get_dpar(prep, \"mu\", i = i)^(-1 / shape)\n    brms:::rcontinuous(\n        n = prep$ndraws,\n        dist = \"weibull\",\n        shape = shape,\n        scale = sigma,\n        lb = prep$data$lb[i],\n        ub = prep$data$ub[i],\n        ntrys = ntrys\n    )\n}\n\nposterior_epred_weibull_ph &lt;- function(prep) {\n    shape &lt;- get_dpar(prep, \"alpha\")\n    sigma &lt;- get_dpar(prep, \"mu\")^(-1 / shape)\n    sigma * gamma(1 + 1 / shape)\n}\n\n\nWe can again check the Stan code that is generated for this custom distribution:\n\n\nShow the code\nstancode(\n    formula,\n    data = os_data_with_log_kg_design,\n    stanvars = sv_weibull_ph, # We pass the custom Stan functions' code here.\n    family = family_weibull_ph()\n)\n\n\n// generated with brms 2.23.0\nfunctions {\n  \nreal weibull_ph_lpdf(real y, real mu, real alpha) {\n  // real sigma = pow(1 / mu, 1 / alpha);\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lpdf(y | alpha, sigma);\n}\nreal weibull_ph_lccdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lccdf(y | alpha, sigma);\n}\nreal weibull_ph_lcdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lcdf(y | alpha, sigma);\n}\nreal weibull_ph_rng(real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_rng(alpha, sigma);\n}\n\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; alpha;  // skewness parameter\n}\ntransformed parameters {\n  // prior contributions to the log posterior\n  real lprior = 0;\n  lprior += normal_lpdf(alpha | 0, 4)\n    - 1 * normal_lccdf(0 | 0, 4);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += X * b;\n    mu = exp(mu);\n    for (n in 1:N) {\n      // special treatment of censored data\n      if (cens[n] == 0) {\n        target += weibull_ph_lpdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == 1) {\n        target += weibull_ph_lccdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == -1) {\n        target += weibull_ph_lcdf(Y[n] | mu[n], alpha);\n      }\n    }\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\nIndeed we can now use the custom distribution. We also see the default priors in the transformed parameters block on the shape parameter (alpha). We don’t see an explicit prior on the regression coefficients (b), which means an improper flat prior is used by default.\nThe remaining challenge is that in jmpost we specified a Gamma prior for \\(\\lambda\\) which is now here the exponentiated intercept parameter. So in principle, we would need an ExpGamma prior on the intercept, meaning that if we exponentiate the intercept, it has a gamma distribution. However, this would again require a custom distribution. Let’s try to go with an approximation: we can just draw samples from the ExpGamma distribution (by sampling from a gamma distribution and taking the log) and then approximate this with a skewed normal distribution (see here for the Stan documentation):\n\n\nShow the code\nset.seed(123)\nintercept_samples &lt;- log(rgamma(1000, 0.7, 1))\n\nlibrary(sn)\nfit &lt;- selm(intercept_samples ~ 1, family = \"SN\")\nxi &lt;- coef(fit, \"DP\")[1]\nomega &lt;- coef(fit, \"DP\")[2]\nalpha &lt;- coef(fit, \"DP\")[3]\n\nhist(intercept_samples, probability = TRUE)\ncurve(dsn(x, xi, omega, alpha), add = TRUE, col = \"red\", lwd = 3)\n\n\n\n\n\n\n\n\n\nThe skew normal density curve approximates the histogram of the log gamma samples well.\nNow we can finally specify the priors:\n\n\nShow the code\npriors &lt;- c(\n    set_prior(\n        glue::glue(\"skew_normal({xi}, {omega}, {alpha})\"),\n        class = \"b\",\n        coef = \"ones\"\n    ),\n    prior(normal(0, 20), class = \"b\"),\n    prior(gamma(0.7, 1), class = \"alpha\")\n)\n\n\nLet’s do a final check of the Stan code:\n\n\nShow the code\nstancode(\n    formula,\n    data = os_data_with_log_kg_design,\n    prior = priors,\n    stanvars = sv_weibull_ph,\n    family = family_weibull_ph()\n)\n\n\n// generated with brms 2.23.0\nfunctions {\n  \nreal weibull_ph_lpdf(real y, real mu, real alpha) {\n  // real sigma = pow(1 / mu, 1 / alpha);\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lpdf(y | alpha, sigma);\n}\nreal weibull_ph_lccdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lccdf(y | alpha, sigma);\n}\nreal weibull_ph_lcdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lcdf(y | alpha, sigma);\n}\nreal weibull_ph_rng(real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_rng(alpha, sigma);\n}\n\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; alpha;  // skewness parameter\n}\ntransformed parameters {\n  // prior contributions to the log posterior\n  real lprior = 0;\n  lprior += skew_normal_lpdf(b[1] | 0.758392879263355, 2.57341668661882, -4.8061893638208);\n  lprior += normal_lpdf(b[2] | 0, 20);\n  lprior += normal_lpdf(b[3] | 0, 20);\n  lprior += normal_lpdf(b[4] | 0, 20);\n  lprior += normal_lpdf(b[5] | 0, 20);\n  lprior += normal_lpdf(b[6] | 0, 20);\n  lprior += normal_lpdf(b[7] | 0, 20);\n  lprior += gamma_lpdf(alpha | 0.7, 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += X * b;\n    mu = exp(mu);\n    for (n in 1:N) {\n      // special treatment of censored data\n      if (cens[n] == 0) {\n        target += weibull_ph_lpdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == 1) {\n        target += weibull_ph_lccdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == -1) {\n        target += weibull_ph_lcdf(Y[n] | mu[n], alpha);\n      }\n    }\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-os/brms1.rds\")\nif (file.exists(save_file)) {\n    fit &lt;- readRDS(save_file)\n} else {\n    fit &lt;- brm(\n        formula = formula,\n        data = os_data_with_log_kg_design,\n        prior = priors,\n        stanvars = sv_weibull_ph,\n        family = family_weibull_ph(),\n        chains = CHAINS,\n        iter = ITER + WARMUP,\n        warmup = WARMUP,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveRDS(fit, save_file)\n}\n\nsummary(fit)\n\n\n Family: weibull_ph \n  Links: mu = log \nFormula: os_time | cens(os_cens) ~ 0 + ones + ecog1 + age + raceOTHER + raceWHITE + sexM + log_kg_est \n   Data: os_data_with_log_kg_design (Number of observations: 203) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nones          -1.59      0.63    -2.85    -0.38 1.00     1935     2531\necog1          0.71      0.21     0.29     1.13 1.00     3317     2840\nage            0.00      0.01    -0.01     0.02 1.00     1953     2532\nraceOTHER      0.54      0.40    -0.30     1.27 1.00     3071     2139\nraceWHITE     -0.03      0.23    -0.46     0.42 1.00     3052     2735\nsexM           0.31      0.20    -0.07     0.69 1.00     3740     2938\nlog_kg_est     0.59      0.17     0.25     0.94 1.00     3821     3144\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha     1.68      0.14     1.42     1.96 1.00     4072     2992\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSo the model converged fast and well.",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html#comparison-of-results",
    "href": "session-os/2_os_weibull_brms.html#comparison-of-results",
    "title": "2. OS model minimal workflow with brms",
    "section": "Comparison of results",
    "text": "Comparison of results\nLet’s compare the results of the brms model with the jmpost model.\nFirst we load again the jmpost results:\n\n\nShow the code\ndraws_jmpost &lt;- readRDS(here(\"session-os/os_draws.rds\")) |&gt;\n    rename_variables(\n        \"gamma\" = \"sm_weibull_ph_gamma\",\n        \"lambda\" = \"sm_weibull_ph_lambda\"\n    )\nsummary(draws_jmpost)\n\n\n# A tibble: 8 × 10\n  variable       mean   median      sd     mad      q5    q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 ecog1       0.724    0.728   0.207   0.214    0.386  1.06   1.00      957.\n2 age         0.00445  0.00463 0.00920 0.00937 -0.0101 0.0194 0.999    1043.\n3 raceOTHER   0.542    0.542   0.400   0.419   -0.124  1.17   0.998     942.\n4 raceWHITE  -0.0268  -0.0307  0.221   0.221   -0.373  0.333  1.00     1046.\n5 sexM        0.302    0.303   0.201   0.198   -0.0342 0.630  1.00      718.\n6 log_kg_est  0.597    0.587   0.172   0.170    0.321  0.880  1.00      742.\n7 gamma       1.69     1.68    0.135   0.131    1.47   1.92   1.00      933.\n8 lambda      0.248    0.208   0.164   0.115    0.0720 0.556  0.999     983.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nWe prepare above brms results in the same format:\n\n\nShow the code\ndraws_brms &lt;- as_draws_array(fit) |&gt;\n    mutate_variables(lambda = exp(b_ones)) |&gt;\n    rename_variables(\n        \"gamma\" = \"alpha\",\n        \"ecog1\" = \"b_ecog1\",\n        \"age\" = \"b_age\",\n        \"raceOTHER\" = \"b_raceOTHER\",\n        \"raceWHITE\" = \"b_raceWHITE\",\n        \"sexM\" = \"b_sexM\",\n        \"log_kg_est\" = \"b_log_kg_est\"\n    ) |&gt;\n    subset_draws(\n        variable = c(\n            \"ecog1\",\n            \"age\",\n            \"raceOTHER\",\n            \"raceWHITE\",\n            \"sexM\",\n            \"log_kg_est\",\n            \"gamma\",\n            \"lambda\"\n        )\n    )\nsummary(draws_brms)\n\n\n# A tibble: 8 × 10\n  variable       mean   median      sd     mad      q5    q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 ecog1       0.713    0.713   0.215   0.217    0.362  1.06   1.00     3317.\n2 age         0.00459  0.00455 0.00964 0.00952 -0.0112 0.0205 1.00     1953.\n3 raceOTHER   0.537    0.550   0.400   0.394   -0.155  1.16   1.00     3071.\n4 raceWHITE  -0.0259  -0.0250  0.226   0.228   -0.389  0.353  1.00     3052.\n5 sexM        0.312    0.317   0.199   0.200   -0.0149 0.631  1.00     3740.\n6 log_kg_est  0.589    0.593   0.174   0.172    0.310  0.874  1.00     3821.\n7 gamma       1.68     1.67    0.140   0.139    1.45   1.91   1.000    4072.\n8 lambda      0.249    0.206   0.170   0.125    0.0719 0.565  1.00     1935.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nSo the results agree well. We can also see this in density plots:\n\n\nShow the code\n# Combine the draws into one data frame\ndraws_combined &lt;- bind_rows(\n    mutate(as_draws_df(draws_jmpost), source = \"jmpost\"),\n    mutate(as_draws_df(draws_brms), source = \"brms\")\n) |&gt;\n    select(-.chain, -.iteration, -.draw)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\n# Convert to long format for ggplot2\ndraws_long &lt;- pivot_longer(\n    draws_combined,\n    cols = -source,\n    names_to = \"parameter\",\n    values_to = \"value\"\n)\n\n# Plot the densities\nggplot(draws_long, aes(x = value, fill = source)) +\n    geom_density(alpha = 0.5) +\n    facet_wrap(~parameter, scales = \"free\") +\n    theme_minimal() +\n    labs(\n        title = \"Posterior Parameter Samples Comparison\",\n        x = \"Value\",\n        y = \"Density\"\n    )\n\n\n\n\n\n\n\n\n\nGenerally these agree very well with each other. Overall we can expect a slight difference between the two results, because for the \\(\\lambda\\) parameter we only approximately used the same prior distribution in brms compared to jmpost.",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/0_setup.html",
    "href": "session-os/0_setup.html",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "This part is required for both the jmpost chapter as well as the brms chapter.\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 2: OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-os/0_setup.html#setup",
    "href": "session-os/0_setup.html#setup",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "This part is required for both the jmpost chapter as well as the brms chapter.\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 2: OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-os/0_setup.html#data-preparation",
    "href": "session-os/0_setup.html#data-preparation",
    "title": "0. Setup and data preparation",
    "section": "Data preparation",
    "text": "Data preparation\nThis part is only required for the jmpost chapter.\nWe will again use the OAK study data.\nFor the tumor growth data, we are using the S1 data set from here. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\ntumor_data &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\") |&gt; \n    read_excel(sheet = \"Study4\") |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm)),\n        arm = fct_recode(arm, \"Docetaxel\" = \"1\", \"MPDL3280A\" = \"2\")\n    ) |&gt; \n    select(id, year, sld, arm)\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 4\n  id                       year   sld arm      \n  &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 -594245265511412736  -0.00821  42   Docetaxel\n2 -594245265511412736   0.0986   44   Docetaxel\n3 -594245265511412736   0.222    44   Docetaxel\n4 -4078394139718744064 -0.0110   65.8 Docetaxel\n5 -4078394139718744064  0.832    66   Docetaxel\n6 -4078394139718744064  1.07     68   Docetaxel\n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        \n 5394902984416364544 :  16   Min.   :-0.1068   Min.   :  2.00  \n 7160320731596789760 :  16   1st Qu.: 0.1123   1st Qu.: 19.05  \n -4159496062492130816:  15   Median : 0.2519   Median : 32.00  \n -7900338178541499392:  15   Mean   : 0.3995   Mean   : 38.16  \n 4878279915140903936 :  15   3rd Qu.: 0.5749   3rd Qu.: 51.00  \n 9202154259772598272 :  15   Max.   : 2.0753   Max.   :228.00  \n (Other)             :4034                     NA's   :27      \n        arm      \n Docetaxel:1658  \n MPDL3280A:2468  \n                 \n                 \n                 \n                 \n                 \n\n\nHere we have 701 patients, and we know from Fig. 1 in the publication that this is the subset of patients with at least 3 tumor size measurements. We have guessed from the second Excel sheet with parameter estimates that arm 1 means docetaxel and arm 2 means MPDL3280A. This also seems reasonable given the larger number of measurements per patient:\n\n\nShow the code\n# make a table\ntumor_data |&gt; \n    group_by(id) |&gt; \n    summarize(arm = arm[1], n = n())  |&gt; \n    group_by(arm) |&gt; \n    summarize(mean_n = mean(n))\n\n\n# A tibble: 2 × 2\n  arm       mean_n\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Docetaxel   5.10\n2 MPDL3280A   6.56\n\n\nFor the OS data, we will be using the Supplementary Table 8 from here. Again, we have copied the data set in this GitHub repository.\nFrom the supplementary material, we know that:\n\nPFS/OS: time in months\nPFS.CNSR/OS.CNSR: 1 for censor, 0 for event\n\nTherefore we will convert the time to years and the censoring to a logical variable:\n\n\nShow the code\nos_data &lt;- here(\"data/41591_2018_134_MOESM3_ESM.xlsx\") |&gt; \n    read_excel(sheet = \"OAK_Clinical_Data\") |&gt; \n    clean_names() |&gt; \n    rename(\n        age = bage,\n        race = race2,\n        ecog = ecoggr,\n        arm = trt01p,\n        response = bcor\n    ) |&gt; \n    mutate(\n        id = factor(as.character(pt_id)),\n        sld = as.numeric(na_if(bl_sld, \".\")),\n        pfs_time = as.numeric(pfs) / 12,\n        pfs_event = as.numeric(pfs_cnsr) == 0,\n        os_time = as.numeric(os) / 12,\n        os_event = as.numeric(os_cnsr) == 0,\n        race = factor(race),\n        ecog = factor(ecog),\n        arm = factor(arm),\n        response = factor(\n          response, \n          levels = c(\"CR\", \"PR\", \"SD\", \"PD\", \"NE\")\n        ),\n        sex = factor(sex)\n    ) |&gt; \n    select(\n      id,\n      arm,\n      ecog,\n      age,\n      race,\n      sex,\n      sld,\n      response,\n      pfs_time,\n      pfs_event,\n      os_time,\n      os_event\n    )\n\nhead(os_data)\n\n\n# A tibble: 6 × 12\n  id    arm    ecog    age race  sex     sld response pfs_time pfs_event os_time\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;lgl&gt;       &lt;dbl&gt;\n1 318   Docet… 1        64 WHITE M     152   PD          0.107 TRUE        0.433\n2 1088  Docet… 0        65 WHITE M      36   PD          0.194 TRUE        0.402\n3 345   Docet… 1        75 WHITE M      92   &lt;NA&gt;        0.162 TRUE        0.162\n4 588   Docet… 0        61 WHITE F      36   SD          1.02  TRUE        2.05 \n5 306   MPDL3… 1        53 WHITE F      86   PD          0.118 TRUE        0.498\n6 718   Docet… 1        80 WHITE M      45.7 SD          0.712 TRUE        1.60 \n# ℹ 1 more variable: os_event &lt;lgl&gt;\n\n\nShow the code\nsummary(os_data)\n\n\n       id             arm      ecog         age           race     sex    \n 1000   :  1   Docetaxel:425   0:315   Min.   :33.00   ASIAN:180   F:330  \n 1001   :  1   MPDL3280A:425   1:535   1st Qu.:57.00   OTHER: 72   M:520  \n 1002   :  1                           Median :64.00   WHITE:598          \n 1003   :  1                           Mean   :63.23                      \n 1004   :  1                           3rd Qu.:70.00                      \n 1005   :  1                           Max.   :85.00                      \n (Other):844                                                              \n      sld         response      pfs_time        pfs_event      \n Min.   : 10.00   CR  :  7   Min.   :0.002738   Mode :logical  \n 1st Qu.: 41.00   PR  :108   1st Qu.:0.117728   FALSE:95       \n Median : 66.00   SD  :327   Median :0.240931   TRUE :755      \n Mean   : 76.69   PD  :304   Mean   :0.443174                  \n 3rd Qu.: 99.00   NE  : 24   3rd Qu.:0.580424                  \n Max.   :316.00   NA's: 80   Max.   :2.242300                  \n NA's   :1                                                     \n    os_time          os_event      \n Min.   :0.002738   Mode :logical  \n 1st Qu.:0.364134   FALSE:281      \n Median :0.803559   TRUE :569      \n Mean   :0.949841                  \n 3rd Qu.:1.620808                  \n Max.   :2.253251                  \n                                   \n\n\nNow we would like to join the two data sets, i.e. know which tumor growth data belongs to which baseline covariates and overall survival time and event indicator. Unfortunately we cannot use the id variable as it is not consistent between the two data sets, because both were anonymized. Instead, we will use the sld variable in combination with the treatment arm. We can also double check with the best overall response variable, because that is closely related to the tumor growth.\nFirst, we will summarize the patient information int he tumor data set. We will calculate the baseline SLD, determine the time of the last tumor measurement and approximate the best overall response by looking at the change from baseline:\n\n\nShow the code\nget_baseline &lt;- function(sld, year) {\n    which_base &lt;- tail(which(year &lt;= 0), 1L)\n    if (length(which_base) == 0) {\n      which_base &lt;- which.min(year)\n    }\n    sld[which_base]\n}\n\nget_contig_below_thresh &lt;- function(sld, year, bsld, thresh) {\n    rle_res &lt;- with(\n        rle(((sld[year &gt; 0] - bsld) / bsld) &lt; 0.2), \n        lengths[values]\n    )\n    if (length(rle_res) == 0) {\n      0\n    } else {\n      max(rle_res)\n    }\n}\n\ntumor_data_summary &lt;- tumor_data |&gt; \n    group_by(id) |&gt; \n    arrange(year) |&gt;\n    summarize(\n      arm = arm[1L],\n      bsld = get_baseline(sld, year),\n      last_year = tail(year, 1L),\n      nadir = min(sld[year &gt;= 0], na.rm = TRUE),\n      max_cfn = max((sld[year &gt;= 0] - nadir) / nadir, na.rm = TRUE),\n      min_cfb = min((sld[year &gt;= 0] - bsld) / bsld, na.rm = TRUE),\n      contig_below_0.2 = get_contig_below_thresh(sld, year, bsld, thresh = 0.2),\n      approx_response = case_when(\n        min_cfb &lt;= -0.3 ~ \"PR\",\n        contig_below_0.2 &gt;= 2 ~ \"SD\",\n        max_cfn &gt;= 0.2 ~ \"PD\",        \n        .default = \"NE\"\n      )   \n    )\nhead(tumor_data_summary)\n\n\n# A tibble: 6 × 9\n  id                arm    bsld last_year nadir max_cfn min_cfb contig_below_0.2\n  &lt;fct&gt;             &lt;fct&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n1 -100096848245159… Doce…    53     0.356    55  0       0.0377                3\n2 -102777882269110… MPDL…    11     0.901     2  2      -0.818                 7\n3 -105049343897269… MPDL…    20     0.345    21  1.62    0.05                  2\n4 -106934201929572… Doce…    15     1.72     11  0.273  -0.267                12\n5 -108949367808374… Doce…    33     0.709    31  0.0968 -0.0606                6\n6 -110624773747042… MPDL…    24     1.32     16  0.562  -0.333                 9\n# ℹ 1 more variable: approx_response &lt;chr&gt;\n\n\nNow let’s try to fuzzy join this with the OS data set, based on the baseline SLD and the treatment arm. In addition, we know that the last time point in the tumor data needs to be before the OS time point. We will also check that the best overall response is the same in both data sets. There will be some errors here: For example, we don’t know whether there were new lesions leading to a progression assessment, or whether the patient had later better results. So we need to expect that the matching will not be perfect.\n\n\nShow the code\nos_data_keys &lt;- os_data |&gt; \n    select(id, arm, sld, pfs_time, os_time, response)\nhead(os_data_keys)\n\n\n# A tibble: 6 × 6\n  id    arm         sld pfs_time os_time response\n  &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n1 318   Docetaxel 152      0.107   0.433 PD      \n2 1088  Docetaxel  36      0.194   0.402 PD      \n3 345   Docetaxel  92      0.162   0.162 &lt;NA&gt;    \n4 588   Docetaxel  36      1.02    2.05  SD      \n5 306   MPDL3280A  86      0.118   0.498 PD      \n6 718   Docetaxel  45.7    0.712   1.60  SD      \n\n\nShow the code\ndist_match &lt;- function(v1, v2) {\n    dist &lt;- abs(v1 - v2)\n    data.frame(include = (dist &lt;= 0.05))\n}\n\nless_match &lt;- function(lower, upper) {\n    data.frame(include = lower &lt;= upper)\n}\n\ntumor_os_data_joined &lt;- tumor_data_summary |&gt; \n    fuzzy_left_join(\n        os_data_keys,\n        by = c(\n          \"arm\" = \"arm\", \n          \"bsld\" = \"sld\",          \n          \"last_year\" = \"os_time\",\n          \"approx_response\" = \"response\"\n        ),\n        match_fun = list(\n          `==`, \n          dist_match, \n          less_match,\n          `==`\n        )\n    )\nnrow(tumor_os_data_joined)\n\n\n[1] 954\n\n\nWe see that we cannot match the two data sets exactly:\n\nWe have less patients to start with in the tumor size data set.\nWe have patients in the tumor size data set which match multiple patients in the OS data set based on the treatment arm and the baseline SLD, and also the last measurement time point cannot disambiguate them.\nWe have patients in the tumor size data set which do not match any patient in the OS data set.\n\nHowever, for the purpose of this training we don’t need to be able to recover the true full data set. Therefore we will just subset the matched data set to a reasonable combination of the two data sets:\n\n\nShow the code\ntumor_os_data_joined_subset &lt;- tumor_os_data_joined |&gt; \n    na.omit() |&gt; \n    filter(!duplicated(id.y)) |&gt;\n    filter(!duplicated(id.x))\nnrow(tumor_os_data_joined_subset)\n\n\n[1] 203\n\n\nNow let’s save the joining information:\n\n\nShow the code\ntgi_os_join_keys &lt;- tumor_os_data_joined_subset |&gt; \n    select(id.x, id.y, arm.x) |&gt; \n    rename(\n        id_tgi = id.x,\n        id_os = id.y,\n        arm = arm.x\n    )\ntable(tgi_os_join_keys$arm)\n\n\n\nDocetaxel MPDL3280A \n       96       107 \n\n\nSo we have about 100 patients in each arm.\nWe subset the two data sets accordingly:\n\n\nShow the code\ntumor_data &lt;- tumor_data |&gt; \n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_tgi\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id) |&gt; \n    rename(id = id_os)\n\nos_data &lt;- os_data |&gt;\n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_os\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id_tgi)",
    "crumbs": [
      "Session 2: OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html",
    "href": "session-os/1_os_weibull_jmpost.html",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "",
    "text": "The purpose of this document is to show a minimal workflow for fitting a Weibull OS model using the jmpost package.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#setup-and-load-data",
    "href": "session-os/1_os_weibull_jmpost.html#setup-and-load-data",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Setup and load data",
    "text": "Setup and load data\nHere we execute the R code from the setup and data preparation chapter, see the full code here.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#tgi-model-fitting",
    "href": "session-os/1_os_weibull_jmpost.html#tgi-model-fitting",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "TGI model fitting",
    "text": "TGI model fitting\nLet’s use jmpost to fit the Stein-Fojo model to the TGI dataset. This works analogously to what we showed in the previous session.\nFirst we again prepare the data objects, starting with the subject level data:\n\n\nShow the code\nsubj_df &lt;- os_data |&gt;\n    mutate(study = \"OAK\") |&gt;\n    select(study, id, arm)\n\nsubj_data &lt;- DataSubject(\n    data = subj_df,\n    subject = \"id\",\n    arm = \"arm\",\n    study = \"study\"\n)\n\n\nNext we prepare the longitudinal data object.\n\n\nShow the code\nlong_df &lt;- tumor_data |&gt;\n    select(id, year, sld)\nlong_data &lt;- DataLongitudinal(\n    data = long_df,\n    formula = sld ~ year\n)\n\n\nNow we can create the JointData object for the TGI model:\n\n\nShow the code\ntgi_joint_data &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data\n)\n\n\nWe specify the Stein-Fojo model together with the priors for the model parameters:\n\n\nShow the code\ntgi_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    )\n)\n\n\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-os/tgi1.rds\")\nif (file.exists(save_file)) {\n    tgi_results &lt;- readRDS(save_file)\n} else {\n    tgi_results &lt;- sampleStanModel(\n        tgi_mod,\n        data = tgi_joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(tgi_results, file = save_file)\n}\n\n\nThe function saveObject() was added to the package recently, please update your installation if it is not yet available.\nNote that this is considerably faster than fitting the larger dataset of 701 patients. Let’s check the convergence of the population parameters:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nmcmc_tgi_results &lt;- cmdstanr::as.CmdStanMCMC(tgi_results)\nmcmc_tgi_results$summary(vars)\n\n\n# A tibble: 11 × 10\n   variable    mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu…  3.75   3.75  0.0375  0.0376   3.69   3.81  1.01      298.     572.\n 2 lm_sf_mu… -0.270 -0.255 0.298   0.308   -0.789  0.163 1.00      641.     680.\n 3 lm_sf_mu… -1.22  -1.16  0.360   0.343   -1.90  -0.708 0.999     723.     852.\n 4 lm_sf_mu… -0.731 -0.723 0.174   0.167   -1.02  -0.462 1.00      604.     762.\n 5 lm_sf_mu… -0.973 -0.969 0.157   0.150   -1.25  -0.728 1.00      641.     869.\n 6 lm_sf_si…  0.129  0.129 0.00381 0.00373  0.123  0.135 1.00      883.     892.\n 7 lm_sf_om…  0.531  0.529 0.0288  0.0294   0.486  0.583 1.00      518.     751.\n 8 lm_sf_om…  1.09   1.08  0.212   0.207    0.771  1.44  1.00      704.     861.\n 9 lm_sf_om…  1.45   1.42  0.271   0.251    1.07   1.95  0.999     731.     994.\n10 lm_sf_om…  0.694  0.684 0.0963  0.0881   0.556  0.860 1.01      930.     904.\n11 lm_sf_om…  0.994  0.983 0.103   0.100    0.840  1.17  1.01      762.     869.\n\n\nShow the code\ndraws_tgi_results &lt;- mcmc_tgi_results$draws(vars)\nmcmc_trace(draws_tgi_results)\n\n\n\n\n\n\n\n\n\nSo this looks good.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#nonmem-comparison",
    "href": "session-os/1_os_weibull_jmpost.html#nonmem-comparison",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "NONMEM comparison",
    "text": "NONMEM comparison\nHere we quickly compare the parameter estimates with the NONMEM results. Note that NONMEM is using maximum likelihood inference to estimate the parameters (typically First Order Conditional Estimation, FOCE, see here), instead of Bayesian inference. Therefore we do expect at least slightly different results.\nLet’s first again tabulate the parameter estimates. Here we use a certain order and the median of the random effects parameters, in order to be able to compare with the NONMEM results:\n\n\nShow the code\npost_samples_by_arm &lt;- as_draws_df(draws_tgi_results) |&gt;\n    rename(\n        mu_bsld = \"lm_sf_mu_bsld[1]\",\n        mu_ks1 = \"lm_sf_mu_ks[1]\",\n        mu_ks2 = \"lm_sf_mu_ks[2]\",\n        mu_kg1 = \"lm_sf_mu_kg[1]\",\n        mu_kg2 = \"lm_sf_mu_kg[2]\",\n        omega_bsld = \"lm_sf_omega_bsld[1]\",\n        omega_ks1 = \"lm_sf_omega_ks[1]\",\n        omega_ks2 = \"lm_sf_omega_ks[2]\",\n        omega_kg1 = \"lm_sf_omega_kg[1]\",\n        omega_kg2 = \"lm_sf_omega_kg[2]\",\n        sigma = lm_sf_sigma\n    ) |&gt;\n    mutate(\n        theta_b0 = exp(mu_bsld),\n        theta_ks1 = exp(mu_ks1),\n        theta_ks2 = exp(mu_ks2),\n        theta_kg1 = exp(mu_kg1),\n        theta_kg2 = exp(mu_kg2)\n    )\n\npost_sum_by_arm &lt;- post_samples_by_arm |&gt;\n    select(\n        theta_kg1, theta_kg2, theta_ks1, theta_ks2,\n        theta_b0,\n        sigma,\n        omega_kg1, omega_kg2, omega_ks1, omega_ks2, omega_bsld\n    ) |&gt;\n    summarize_draws() |&gt;\n    gt() |&gt;\n    fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum_by_arm\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_kg1\n0.489\n0.485\n0.0828\n0.0814\n0.359\n0.630\n1.00\n587\n722\n\n\ntheta_kg2\n0.382\n0.380\n0.0590\n0.0570\n0.287\n0.483\n0.999\n623\n841\n\n\ntheta_ks1\n0.797\n0.775\n0.228\n0.235\n0.454\n1.18\n0.999\n602\n638\n\n\ntheta_ks2\n0.313\n0.312\n0.104\n0.107\n0.150\n0.492\n1.00\n708\n844\n\n\ntheta_b0\n42.6\n42.5\n1.60\n1.59\n40.1\n45.4\n1.00\n297\n557\n\n\nsigma\n0.129\n0.129\n0.00381\n0.00373\n0.123\n0.135\n0.999\n850\n845\n\n\nomega_kg1\n0.694\n0.684\n0.0963\n0.0881\n0.556\n0.860\n1.00\n931\n859\n\n\nomega_kg2\n0.994\n0.983\n0.103\n0.100\n0.840\n1.17\n1.00\n736\n850\n\n\nomega_ks1\n1.09\n1.08\n0.212\n0.207\n0.771\n1.44\n1.00\n644\n798\n\n\nomega_ks2\n1.45\n1.42\n0.271\n0.251\n1.07\n1.95\n1.00\n718\n981\n\n\nomega_bsld\n0.531\n0.529\n0.0288\n0.0294\n0.486\n0.583\n1.00\n474\n683\n\n\n\n\n\n\n\nNow we can compare these with the NONMEM results (thanks to Victor Poon for providing these!)\n\n\n\nNONMEM results\n\n\nHere we need to look at the estimate_yrs column, because in our jmpost model we have been using the year scale. The results are different, but still quite consistent with the jmpost results.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#extract-individual-growth-rate-estimates",
    "href": "session-os/1_os_weibull_jmpost.html#extract-individual-growth-rate-estimates",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Extract individual growth rate estimates",
    "text": "Extract individual growth rate estimates\nWe can now extract the individual growth rate estimates from the model. Later, in the joint model, we are going to use the log of the growth parameter as the link. Therefore we also here first log transform the sampled values of the growth rate estimates \\(\\psi_{\\text{kg}, i}\\), and then take the mean. Since the relevant random effect parameter samples are already stored in the mcmc_tgi_results object, we can work with that via the rvars interface.\nThe only tricky part is that we need to match the IDs of the patients manually, because jmpost just numbers the patients in the order they appear in the data, which is then the index for all the random effects and individual growth parameters \\(\\psi_{\\text{kg}, i}\\).\nHowever, we need to be careful to extract the data order from the tgi_joint_data object, because the subject data set is reordered by the sorted patient ID during the creation of the DataJoint object. If we don’t do this correctly, then we would permute the growth rates randomly between the patients and thereby destroy the link between the growth rates and the patients.\n\n\nShow the code\nlog_growth_samples &lt;- mcmc_tgi_results |&gt;\n    # We use here `rvars` because it allows to apply the\n    # mutation across all subjects at once.\n    as_draws_rvars() |&gt;\n    mutate_variables(log_growth = log(lm_sf_psi_kg))\n\nsubj_log_kg_est &lt;- log_growth_samples |&gt;\n    subset_draws(variable = \"log_growth\") |&gt;\n    summary() |&gt;\n    # Important: Take the IDs from `tgi_joint_data` and not from `subj_data` here!\n    mutate(id = tgi_joint_data@subject@data$id)\n\nhead(subj_log_kg_est)\n\n\n# A tibble: 6 × 11\n  variable    mean median    sd   mad    q5    q95  rhat ess_bulk ess_tail id   \n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;\n1 log_grow… -0.982 -0.808 0.902 0.927 -2.65  0.130 1.00      937.    1037. 1008 \n2 log_grow… -1.09  -1.04  0.894 0.921 -2.62  0.246 0.999     929.     731. 1018 \n3 log_grow… -1.01  -0.911 0.367 0.264 -1.72 -0.590 1.00      888.     717. 1019 \n4 log_grow… -0.760 -0.729 0.594 0.619 -1.79  0.117 1.000     857.     950. 1021 \n5 log_grow… -1.09  -1.03  0.885 0.910 -2.62  0.217 1.01      976.     852. 1026 \n6 log_grow… -1.46  -1.38  0.814 0.845 -2.96 -0.313 1.00      905.     915. 1027 \n\n\nIn the latest version of jmpost this process has now been simplified, and we can use the LongitudinalRandomEffects() function as follows:\n\n\nShow the code\nsubj_log_kg_est_alt &lt;- LongitudinalRandomEffects(tgi_results) |&gt;\n    # We need to convert this to a data.frame to be able to do the log transformation.\n    as.data.frame() |&gt;\n    filter(parameter == \"g\") |&gt;\n    mutate(values = log(values)) |&gt;\n    group_by(subject) |&gt;\n    summarize(log_kg_est = mean(values))\nhead(subj_log_kg_est_alt)\n\n\n# A tibble: 6 × 2\n  subject log_kg_est\n  &lt;chr&gt;        &lt;dbl&gt;\n1 1008        -0.982\n2 1018        -1.09 \n3 1019        -1.01 \n4 1021        -0.760\n5 1026        -1.09 \n6 1027        -1.46 \n\n\nSo this gives the same resulting log growth rates, which is reassuring.\nWe now add the e.g. posterior mean estimate of the individual log growth rates to the OS data set, such that we will be able to use it below as a covariate in the OS model:\n\n\nShow the code\nos_data_with_log_kg_est &lt;- os_data |&gt;\n    select(id, arm, ecog, age, race, sex, os_time, os_event) |&gt;\n    left_join(select(subj_log_kg_est, mean, id), by = \"id\") |&gt;\n    rename(log_kg_est = mean)\nhead(os_data_with_log_kg_est)\n\n\n# A tibble: 6 × 9\n  id    arm       ecog    age race  sex   os_time os_event log_kg_est\n  &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;\n1 588   Docetaxel 0        61 WHITE F       2.05  FALSE       -0.571 \n2 330   MPDL3280A 1        56 WHITE F       1.68  FALSE       -1.89  \n3 791   Docetaxel 0        72 WHITE F       0.901 TRUE        -0.518 \n4 635   Docetaxel 0        42 OTHER F       1.66  TRUE        -0.642 \n5 365   MPDL3280A 0        64 WHITE F       1.43  TRUE        -0.427 \n6 773   Docetaxel 0        65 WHITE M       1.63  FALSE        0.0340\n\n\nShow the code\nsave_file &lt;- here(\"session-os/os_data_with_log_kg.rds\")\nif (!file.exists(save_file)) {\n    saveRDS(os_data_with_log_kg_est, file = save_file)\n}\n\n\nAs a sanity check to make sure we linked the growth rates correctly to the patients, let’s compare the average log growth rates computed from the above data set with the average we would expect based on the log normal distribution. Remember from the TGI session that we have:\n\\[\n\\log(\\psi_{k_{g}}) \\sim \\text{Normal}(\\mu_{k_{g}}, \\omega_{g})\n\\]\nwithin each treatment arm.\n\n\nShow the code\n# Compute the mean using the individual estimates:\nlog_growth_summary &lt;- os_data_with_log_kg_est |&gt;\n    group_by(arm) |&gt;\n    summarise(mean = mean(log_kg_est))\n\n# And now compute the mean from the original model parameter samples:\nlog_growth_check &lt;- mcmc_tgi_results$summary(\"lm_sf_mu_kg\") |&gt;\n    mutate(\n        arm = recode(\n            variable,\n            # The order here is given by the order of the arm factor levels.\n            \"lm_sf_mu_kg[1]\" = \"Docetaxel\",\n            \"lm_sf_mu_kg[2]\" = \"MPDL3280A\"\n        )\n    ) |&gt;\n    select(arm, mean)\n\n# We can compare:\nlog_growth_summary\n\n\n# A tibble: 2 × 2\n  arm         mean\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Docetaxel -0.738\n2 MPDL3280A -0.989\n\n\nShow the code\nlog_growth_check\n\n\n# A tibble: 2 × 2\n  arm         mean\n  &lt;chr&gt;      &lt;dbl&gt;\n1 Docetaxel -0.731\n2 MPDL3280A -0.973\n\n\nSo this looks good, and we can continue with this data set.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#os-model-fitting",
    "href": "session-os/1_os_weibull_jmpost.html#os-model-fitting",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "OS model fitting",
    "text": "OS model fitting\nNow we can fit the OS model.\nWe start by preparing the DataSurvival object:\n\n\nShow the code\nsurv_data &lt;- DataSurvival(\n    data = os_data_with_log_kg_est,\n    formula = Surv(os_time, os_event) ~\n        ecog + age + race + sex + log_kg_est\n)\n\n\nNote that we are not including the treatment arm here, but only the log growth rate estimates. In addition, the covariates in the model include the ECOG score, age, race and sex. The idea is that the treatment effect is fully captured in the log growth rate estimates, which is referred to as “Working assumption (1)” in the slides.\nNow we can create the JointData object for the OS model:\n\n\nShow the code\nos_joint_data &lt;- DataJoint(\n    subject = subj_data,\n    survival = surv_data\n)\n\n\nWe specify the Weibull model together with the priors for the model parameters. We take vague priors for the regression coefficients beta. For lambda and gamma, we start from the scale of the survival data at hand: the average survival time is 1.3 years, just taking a crude average of all survival times.\nWe can quickly write the function that gives the mean of the Weibull distribution with fixed lambda and gamma:\n\n\nShow the code\nweibull_mean &lt;- function(lambda, gamma) {\n    base::gamma(1 + 1 / gamma) / lambda\n}\n\n\nTherefore, playing around with this a bit, we can e.g. center the prior for lambda around 0.7 and the prior for gamma around 1.5, giving a mean survival time of 1.3 years.\nIf we want to use Gamma distributions e.g. for lambda and gamma, we can use the prior_gamma function. The two parameters of this distribution are the shape and the rate. The mean is shape divided by the rate. So easiest is to keep a rate of 1 and just set the shape to the mean value we need:\n\n\nShow the code\nos_mod &lt;- JointModel(\n    survival = SurvivalWeibullPH(\n        lambda = prior_gamma(0.7, 1),\n        gamma = prior_gamma(1.5, 1),\n        beta = prior_normal(0, 20)\n    )\n)\n\n\nBecause we use a large prior variance for beta, we need to adjust the default initial value construction used in jmpost. As explained here, we can change the shrinkage of the initial values to the mean. We can then check what the initial values will be, to make sure that they are reasonable:\n\n\nShow the code\noptions(\"jmpost.prior_shrinkage\" = 0.999)\n\ninitialValues(os_mod, n_chains = CHAINS)\n\n\n[[1]]\n[[1]]$sm_weibull_ph_lambda\n[1] 0.6994537\n\n[[1]]$sm_weibull_ph_gamma\n[1] 1.500344\n\n[[1]]$beta_os_cov\n[1] 0.03287165\n\n\n[[2]]\n[[2]]$sm_weibull_ph_lambda\n[1] 0.7005817\n\n[[2]]$sm_weibull_ph_gamma\n[1] 1.499301\n\n[[2]]$beta_os_cov\n[1] -0.01443968\n\n\n[[3]]\n[[3]]$sm_weibull_ph_lambda\n[1] 0.700252\n\n[[3]]$sm_weibull_ph_gamma\n[1] 1.498596\n\n[[3]]$beta_os_cov\n[1] -0.02049833\n\n\n[[4]]\n[[4]]$sm_weibull_ph_lambda\n[1] 0.6994922\n\n[[4]]$sm_weibull_ph_gamma\n[1] 1.499025\n\n[[4]]$beta_os_cov\n[1] -0.006054535\n\n\nSo the values are now close to the means of the respective prior distributions. We can then see later if the chains were converging well. If not, we could as an alternative also manually set initial values, as explained here.\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-os/os1.rds\")\nif (file.exists(save_file)) {\n    os_results &lt;- readRDS(save_file)\n} else {\n    os_results &lt;- sampleStanModel(\n        os_mod,\n        data = os_joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(os_results, file = save_file)\n}\n\n\nNote that here we can get warnings at the beginning of the chains’ sampling process (“The current Metropolis proposal is about to be rejected …”). As long as this only happens in the beginning, and not during the sampling later, then this is not a cause for concern.\nLet’s check the convergence of the population parameters:\n\n\nShow the code\nvars &lt;- c(\n    \"beta_os_cov\",\n    \"sm_weibull_ph_gamma\",\n    \"sm_weibull_ph_lambda\"\n)\n\nmcmc_os_results &lt;- cmdstanr::as.CmdStanMCMC(os_results)\nmcmc_os_results$summary(vars)\n\n\n# A tibble: 8 × 10\n  variable           mean   median      sd     mad      q5    q95  rhat ess_bulk\n  &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 beta_os_cov[1]  0.724    0.728   0.207   0.214    0.386  1.06   1.00      957.\n2 beta_os_cov[2]  0.00445  0.00463 0.00920 0.00937 -0.0101 0.0194 0.999    1043.\n3 beta_os_cov[3]  0.542    0.542   0.400   0.419   -0.124  1.17   0.998     942.\n4 beta_os_cov[4] -0.0268  -0.0307  0.221   0.221   -0.373  0.333  1.00     1046.\n5 beta_os_cov[5]  0.302    0.303   0.201   0.198   -0.0342 0.630  1.00      718.\n6 beta_os_cov[6]  0.597    0.587   0.172   0.170    0.321  0.880  1.00      742.\n7 sm_weibull_ph…  1.69     1.68    0.135   0.131    1.47   1.92   1.00      933.\n8 sm_weibull_ph…  0.248    0.208   0.164   0.115    0.0720 0.556  0.999     983.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nShow the code\ndraws_os_results &lt;- mcmc_os_results$draws(vars)\nmcmc_trace(draws_os_results)",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#interpret-covariate-effects",
    "href": "session-os/1_os_weibull_jmpost.html#interpret-covariate-effects",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Interpret covariate effects",
    "text": "Interpret covariate effects\nIn order to better see which of the coefficients relate to which covariates, we can rename them as follows:\n\n\nShow the code\nsurv_data_design &lt;- as_stan_list(surv_data)$os_cov_design\nos_cov_names &lt;- colnames(surv_data_design)\nold_coef_names &lt;- glue::glue(\"beta_os_cov[{seq_along(os_cov_names)}]\")\ndraws_os_results &lt;- do.call(\n    rename_variables,\n    c(list(draws_os_results), setNames(old_coef_names, os_cov_names))\n)\nmcmc_dens_overlay(draws_os_results) +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nShow the code\nsummary(draws_os_results)\n\n\n# A tibble: 8 × 10\n  variable           mean   median      sd     mad      q5    q95  rhat ess_bulk\n  &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 ecog1           0.724    0.728   0.207   0.214    0.386  1.06   1.00      957.\n2 age             0.00445  0.00463 0.00920 0.00937 -0.0101 0.0194 0.999    1043.\n3 raceOTHER       0.542    0.542   0.400   0.419   -0.124  1.17   0.998     942.\n4 raceWHITE      -0.0268  -0.0307  0.221   0.221   -0.373  0.333  1.00     1046.\n5 sexM            0.302    0.303   0.201   0.198   -0.0342 0.630  1.00      718.\n6 log_kg_est      0.597    0.587   0.172   0.170    0.321  0.880  1.00      742.\n7 sm_weibull_ph…  1.69     1.68    0.135   0.131    1.47   1.92   1.00      933.\n8 sm_weibull_ph…  0.248    0.208   0.164   0.115    0.0720 0.556  0.999     983.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nShow the code\nsave_file &lt;- here(\"session-os/os_draws.rds\")\nif (!file.exists(save_file)) {\n    saveRDS(draws_os_results, file = save_file)\n}\n\n\nSo we can see that the 90% credible interval (CI) for the log_kg_est and ecog1 covariate coefficients excludes 0, so both are “significant” predictors of the hazard rate. On the other hand, the race dummy variables’ and the age variable’s coefficient CIs clearly include 0. The situation is less clear for sex: here the CI barely includes 0.\nIn addition, we can also look at the posterior probabilities to have a hazard ratio above 1:\n\n\nShow the code\ndraws_os_results |&gt;\n    as_draws_df() |&gt;\n    summarise_all(~ mean(. &gt; 0))\n\n\n# A tibble: 1 × 11\n  ecog1   age raceOTHER raceWHITE  sexM log_kg_est sm_weibull_ph_gamma\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;               &lt;dbl&gt;\n1     1 0.677     0.912     0.432  0.93          1                   1\n# ℹ 4 more variables: sm_weibull_ph_lambda &lt;dbl&gt;, .chain &lt;dbl&gt;,\n#   .iteration &lt;dbl&gt;, .draw &lt;dbl&gt;\n\n\nSo we have a more than 90% posterior probability that male patients have a higher hazard than females, and we also see a similarly strong effect here for the OTHER category of race. As we saw from the CI already, the age effect is not so strong.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#observation-vs-model-fit",
    "href": "session-os/1_os_weibull_jmpost.html#observation-vs-model-fit",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nA useful plot displays the model predicted survival function and overlays the non-parametric Kaplan-Meier plot to it. Such a plot is easily obtained using the autoplot() function, as we will see below.\nThe first step consists in generating the survival predictions at the group level with the SurvivalQuantities() function. It is recommended to specify the sequence of time points at which the predictions should be made (using the argument times):\n\n\nShow the code\ntime_grid &lt;- seq(\n    from = 0,\n    to = max(os_data_with_log_kg_est$os_time),\n    length = 100\n)\nos_surv_group_grid &lt;- GridGrouped(\n    times = time_grid,\n    groups = with(\n        subj_df,\n        split(as.character(id), arm)\n    )\n)\nos_surv_pred &lt;- SurvivalQuantities(\n    object = os_results,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\n\n\nNow we can use the autoplot() method:\n\n\nShow the code\nautoplot(os_surv_pred, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nHere the fit seems ok but not perfect, especially for the Docetaxel arm it could be improved maybe. We will try below alternative models to see if we can improve the fit.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#hazard-and-hazard-rate-estimation",
    "href": "session-os/1_os_weibull_jmpost.html#hazard-and-hazard-rate-estimation",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Hazard and hazard rate estimation",
    "text": "Hazard and hazard rate estimation\nSimilarly to the survival function estimation, we can also estimate the hazard function by treatment group.\n\n\nShow the code\nos_hazard_pred &lt;- SurvivalQuantities(\n    object = os_results,\n    grid = os_surv_group_grid,\n    type = \"haz\"\n)\n\n\nAlso this can be plotted using the autoplot() method:\n\n\nShow the code\nautoplot(os_hazard_pred, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nFinally, we can also estimate the hazard rate, which is constant over time here - because we use the Weibull proportional hazards model. We still show this more complicated code here because it will also work later for joint TGI-OS models, where the hazard rate may not be constant any longer.\n\n\nShow the code\nos_hr_est &lt;- os_hazard_pred |&gt;\n    as.data.frame() |&gt;\n    group_by(group, time) |&gt;\n    mutate(sample = row_number()) |&gt;\n    pivot_wider(names_from = group, values_from = values) |&gt;\n    mutate(hr = MPDL3280A / Docetaxel) |&gt;\n    group_by(time) |&gt;\n    summarize(\n        mean = mean(hr, na.rm = TRUE),\n        lower = quantile(hr, 0.05, na.rm = TRUE),\n        upper = quantile(hr, 0.95, na.rm = TRUE)\n    ) |&gt;\n    na.omit() # Omit the time = 0 which has NA\nsummary(os_hr_est)\n\n\n      time              mean            lower          upper       \n Min.   :0.02276   Min.   :0.9028   Min.   :0.88   Min.   :0.9295  \n 1st Qu.:0.58038   1st Qu.:0.9028   1st Qu.:0.88   1st Qu.:0.9295  \n Median :1.13801   Median :0.9028   Median :0.88   Median :0.9295  \n Mean   :1.13801   Mean   :0.9028   Mean   :0.88   Mean   :0.9295  \n 3rd Qu.:1.69563   3rd Qu.:0.9028   3rd Qu.:0.88   3rd Qu.:0.9295  \n Max.   :2.25325   Max.   :0.9028   Max.   :0.88   Max.   :0.9295  \n\n\nNow we can plot this:\n\n\nShow the code\nggplot(os_hr_est, aes(x = time, y = mean, ymin = lower, ymax = upper)) +\n    geom_line() +\n    geom_ribbon(alpha = 0.3)",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#alternative-models",
    "href": "session-os/1_os_weibull_jmpost.html#alternative-models",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Alternative models",
    "text": "Alternative models\nAbove we felt that maybe we could improve the fit of the model further.\nOne idea is to add also the direct effect of the treatment arm as a covariate:\n\n\nShow the code\nsurv_data2 &lt;- DataSurvival(\n    data = os_data_with_log_kg_est,\n    # Here we add the arm covariate:\n    formula = update(surv_data@formula, . ~ . + arm)\n)\nos_joint_data2 &lt;- DataJoint(\n    subject = subj_data,\n    survival = surv_data2\n)\nsave_file &lt;- here(\"session-os/os2.rds\")\nif (file.exists(save_file)) {\n    os_results2 &lt;- readRDS(save_file)\n} else {\n    os_results2 &lt;- sampleStanModel(\n        os_mod,\n        data = os_joint_data2,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(os_results2, file = save_file)\n}\nmcmc_os_results2 &lt;- cmdstanr::as.CmdStanMCMC(os_results2)\n\n\nWe can easily plot the survival functions and compare them with the Kaplan-Meier curves of the treatment arms, because we can reuse the above os_surv_group_grid:\n\n\nShow the code\nos_surv_pred2 &lt;- SurvivalQuantities(\n    object = os_results2,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\nautoplot(os_surv_pred2, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nHere we see a bit better “coverage” of the Docetaxel Kaplan-Meier curve by the confidence intervals of the model.\nWe could also consider the model with only the direct treatment arm effect, without the log growth rate estimates:\n\n\nShow the code\nsurv_data3 &lt;- DataSurvival(\n    data = os_data_with_log_kg_est,\n    # Here we add the arm covariate:\n    formula = update(surv_data2@formula, . ~ . - log_kg_est)\n)\nos_joint_data3 &lt;- DataJoint(\n    subject = subj_data,\n    survival = surv_data3\n)\nsave_file &lt;- here(\"session-os/os3.rds\")\nif (file.exists(save_file)) {\n    os_results3 &lt;- readRDS(save_file)\n} else {\n    os_results3 &lt;- sampleStanModel(\n        os_mod,\n        data = os_joint_data3,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(os_results3, file = save_file)\n}\nmcmc_os_results3 &lt;- cmdstanr::as.CmdStanMCMC(os_results3)\n\n\nLet’s plot again the survival functions:\n\n\nShow the code\nos_surv_pred3 &lt;- SurvivalQuantities(\n    object = os_results3,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\nautoplot(os_surv_pred3, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nThis looks very similar to the previous model.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#model-comparison",
    "href": "session-os/1_os_weibull_jmpost.html#model-comparison",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Model comparison",
    "text": "Model comparison\nFor comparing models, we can use more formal tools, as we will see now.\nWe can use the Brier score to compare two different survival models. The Brier score is a measure of the mean squared difference between the predicted survival probability and the actual survival status. The lower the Brier score, the better the model.\nTo calculate it, we need to use the GridFixed input for SurvivalQuantities():\n\n\nShow the code\nos_fixed_surv &lt;- SurvivalQuantities(\n    object = os_results,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_mod1_bs &lt;- brierScore(os_fixed_surv)\n\n\nWe can also look at the LOOIC. As for the TGI model, we can use the loo() method in the CmdStanMCMC object to calculate it:\n\n\nShow the code\nos_mod1_looic &lt;- mcmc_os_results$loo(r_eff = FALSE)\n\n\nAlso for the two alternative models we can calculate the Brier score and LOOIC in the same way:\n\n\nShow the code\nos_fixed_surv2 &lt;- SurvivalQuantities(\n    object = os_results2,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_mod2_bs &lt;- brierScore(os_fixed_surv2)\nos_mod2_looic &lt;- mcmc_os_results2$loo(r_eff = FALSE)\n\nos_fixed_surv3 &lt;- SurvivalQuantities(\n    object = os_results3,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_mod3_bs &lt;- brierScore(os_fixed_surv3)\nos_mod3_looic &lt;- mcmc_os_results3$loo(r_eff = FALSE)\n\n\nOf course in a real application with many models we can easily write a little function that does this for us repeatedly instead of just copy/pasting the code as we do here.\nNow we can compare the three models.\nLet’s start with the LOOIC:\n\n\nShow the code\nos_mod1_looic\n\n\n\nComputed from 1000 by 203 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -188.6  6.5\np_loo         8.6  0.8\nlooic       377.3 13.0\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nShow the code\nos_mod2_looic\n\n\n\nComputed from 1000 by 203 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -189.4  6.6\np_loo         9.9  0.8\nlooic       378.8 13.1\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nShow the code\nos_mod3_looic\n\n\n\nComputed from 1000 by 203 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -193.4  5.6\np_loo         8.7  0.6\nlooic       386.8 11.1\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nShow the code\nloo_compare(os_mod1_looic, os_mod2_looic, os_mod3_looic)\n\n\n       elpd_diff se_diff\nmodel1  0.0       0.0   \nmodel2 -0.7       1.3   \nmodel3 -4.8       4.2   \n\n\nSo we see that according to the LOOIC, the first model with just the log_kg_est covariate is slightly better than the model with the additional arm covariate. If we omit the log_kg_est covariate altogether and only include the arm covariate, then the model is worse than both of the other two models.\nWe can plot the Brier scores:\n\n\nShow the code\ndata.frame(\n    time = time_grid,\n    `1 - 2` = os_mod1_bs - os_mod2_bs,\n    `1 - 3` = os_mod1_bs - os_mod3_bs,\n    `2 - 3` = os_mod2_bs - os_mod3_bs,\n    check.names = FALSE\n) |&gt;\n    pivot_longer(\n        cols = c(\"1 - 2\", \"1 - 3\", \"2 - 3\"),\n        names_to = \"diff\",\n        values_to = \"brier_score_diff\"\n    ) |&gt;\n    ggplot(aes(x = time, y = brier_score_diff, color = diff, group = diff)) +\n    geom_line() +\n    labs(y = \"Brier score difference\") +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nWhen we look at the difference model 1 minus model 2, we see that this is slightly positive. Since lower numbers of the Brier score are better, this means that model 2 is slightly better here.\nWhen we look at the difference model 1 minus model 3, we see that this is more clearly negative between times 0.25 and 1.75, meaning that model 1 is clearly better than model 3 there. Towards later times this reverses, and model 3 is better than model 1.\nThe Brier score provides a more nuanced and time-dependent way of comparing the different OS models. Here we could either select model 1 due to its simplicity and almost same performance as model 2.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Contents",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n2025-11-17\n\n\n0. Setup and data preparation\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n\n\n2025-11-17\n\n\n5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#session-5-bhm",
    "href": "index.html#session-5-bhm",
    "title": "Contents",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n2025-11-17\n\n\n0. Setup and data preparation\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n\n\n2025-11-17\n\n\n5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#session-4-pts",
    "href": "index.html#session-4-pts",
    "title": "Contents",
    "section": "Session 4: PTS",
    "text": "Session 4: PTS\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n2025-11-17\n\n\n0. Setup and data preparation\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n\n\n2025-11-17\n\n\n1. Calculate Bayesian Predictive Power\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#session-3-tgi-os",
    "href": "index.html#session-3-tgi-os",
    "title": "Contents",
    "section": "Session 3: TGI-OS",
    "text": "Session 3: TGI-OS\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n2025-11-17\n\n\n0. Setup and data preparation\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n\n\n2025-11-17\n\n\n1. TGI-OS joint model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#session-2-os",
    "href": "index.html#session-2-os",
    "title": "Contents",
    "section": "Session 2: OS",
    "text": "Session 2: OS\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n2025-11-17\n\n\n0. Setup and data preparation\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n\n\n2025-11-17\n\n\n1. OS model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\n\n2025-11-17\n\n\n2. OS model minimal workflow with brms\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#session-1-tgi",
    "href": "index.html#session-1-tgi",
    "title": "Contents",
    "section": "Session 1: TGI",
    "text": "Session 1: TGI\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\n2025-11-17\n\n\n0. Setup\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n\n\n2025-11-17\n\n\n1. TGI model minimal workflow with brms\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\n\n2025-11-17\n\n\n2. TGI model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\n\n2025-11-17\n\n\n3. Generalized Stein-Fojo model\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\n\n2025-11-17\n\n\n4. Claret-Bruno model\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Contents",
    "section": "License",
    "text": "License\nThe training material is licensed under a Creative Commons Attribution 4.0 International License. If you wish to reuse any part of this material, please ensure proper attribution is given to the original authors as specified by the Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Contents",
    "section": "Copyright",
    "text": "Copyright\n© 2025 Genentech Inc. All rights reserved.",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html",
    "href": "session-jm/1_jm-jmpost.html",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "",
    "text": "The purpose of this document is to show a minimal workflow for fitting a joint Stein-Fojo TGI + Weibull OS model using the jmpost package.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#setup-and-load-data",
    "href": "session-jm/1_jm-jmpost.html#setup-and-load-data",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Setup and load data",
    "text": "Setup and load data\nHere we execute the R code from the setup and data preparation chapter, see the full code here.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#data-object-preparation",
    "href": "session-jm/1_jm-jmpost.html#data-object-preparation",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Data object preparation",
    "text": "Data object preparation\nFirst we again prepare the data objects, starting with the subject level data:\n\n\nShow the code\nsubj_df &lt;- os_data |&gt;\n    mutate(study = \"OAK\") |&gt;\n    select(study, id, arm)\n\nsubj_data &lt;- DataSubject(\n    data = subj_df,\n    subject = \"id\",\n    arm = \"arm\",\n    study = \"study\"\n)\n\n\nNext we prepare the longitudinal data object.\n\n\nShow the code\nlong_df &lt;- tumor_data |&gt;\n    select(id, year, sld)\nlong_data &lt;- DataLongitudinal(\n    data = long_df,\n    formula = sld ~ year\n)\n\n\nNext we prepare the DataSurvival object, where specify the covariates for the survival model:\n\n\nShow the code\nsurv_data &lt;- DataSurvival(\n    data = os_data,\n    formula = Surv(os_time, os_event) ~ ecog + age + race + sex\n)\n\n\nNote that we don’t include the log growth rate estimate log_kg_est here, because here we are fitting a joint model - the log growth rate will be included later in the JointModel specification instead. For now we also don’t include the treatment arm covariate, because we assume Working Assumption (1) from the previous session, i.e. the treatment effect is fully captured by the mediator (or link).\nNow we can create the JointData object for the TGI model:\n\n\nShow the code\njoint_data &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data,\n    survival = surv_data\n)",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#model-specification",
    "href": "session-jm/1_jm-jmpost.html#model-specification",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Model specification",
    "text": "Model specification\nWe specify the Stein-Fojo model for the TGI data, the Weibull model for the OS data, as well as the link, i.e. the log growth rate from the TGI model which shall influence the hazard in the OS model:\n\n\nShow the code\njoint_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    ),\n    survival = SurvivalWeibullPH(\n        lambda = prior_gamma(0.7, 1),\n        gamma = prior_gamma(1.5, 1),\n        beta = prior_normal(0, 20)\n    ),\n    link = linkGrowth(\n        prior = prior_normal(0, 20)\n    )\n)\n\n\nHere we use a normal prior with mean 0 and standard deviation 20 for the link coefficient, which is a very uninformative prior. This corresponds to the same prior used for the regression coefficients of the “fixed” covariates in SurvivalWeibullPH.\nFor the other parameters, we keep the same prior as in the previous sessions on separate TGI and OS models.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#model-fitting",
    "href": "session-jm/1_jm-jmpost.html#model-fitting",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Model fitting",
    "text": "Model fitting\nAgain we need to be careful with the automatic selection of initial values due to the large standard deviation on beta and the link coefficient. We therefore set the shrinkage option and check the initial values:\n\n\nShow the code\noptions(\"jmpost.prior_shrinkage\" = 0.99)\n\ninitialValues(joint_mod, n_chains = CHAINS)\n\n\n[[1]]\n[[1]]$lm_sf_mu_bsld\n[1] 4.170746\n\n[[1]]$lm_sf_mu_ks\n[1] -0.6462188\n\n[[1]]$lm_sf_mu_kg\n[1] 0.0386178\n\n[[1]]$lm_sf_omega_bsld\n[1] 0.01712068\n\n[[1]]$lm_sf_omega_ks\n[1] 0.04168648\n\n[[1]]$lm_sf_omega_kg\n[1] 0.004440108\n\n[[1]]$lm_sf_sigma\n[1] 0.01695073\n\n[[1]]$lm_sf_eta_tilde_bsld\n[1] 0.003480171\n\n[[1]]$lm_sf_eta_tilde_ks\n[1] 0.0008337265\n\n[[1]]$lm_sf_eta_tilde_kg\n[1] 0.01726106\n\n[[1]]$sm_weibull_ph_lambda\n[1] 0.700348\n\n[[1]]$sm_weibull_ph_gamma\n[1] 1.504696\n\n[[1]]$beta_os_cov\n[1] 0.1243393\n\n[[1]]$link_growth\n[1] -0.05986066\n\n\n[[2]]\n[[2]]$lm_sf_mu_bsld\n[1] 4.174181\n\n[[2]]$lm_sf_mu_ks\n[1] -0.6567921\n\n[[2]]$lm_sf_mu_kg\n[1] 0.02116598\n\n[[2]]$lm_sf_omega_bsld\n[1] 0.005415861\n\n[[2]]$lm_sf_omega_ks\n[1] 0.04781523\n\n[[2]]$lm_sf_omega_kg\n[1] 0.04220833\n\n[[2]]$lm_sf_sigma\n[1] 0.03307321\n\n[[2]]$lm_sf_eta_tilde_bsld\n[1] 0.009484316\n\n[[2]]$lm_sf_eta_tilde_ks\n[1] 0.005045011\n\n[[2]]$lm_sf_eta_tilde_kg\n[1] 0.001999257\n\n[[2]]$sm_weibull_ph_lambda\n[1] 0.6977723\n\n[[2]]$sm_weibull_ph_gamma\n[1] 1.514985\n\n[[2]]$beta_os_cov\n[1] -0.02378184\n\n[[2]]$link_growth\n[1] 0.1833443\n\n\n[[3]]\n[[3]]$lm_sf_mu_bsld\n[1] 4.187616\n\n[[3]]$lm_sf_mu_ks\n[1] -0.6505919\n\n[[3]]$lm_sf_mu_kg\n[1] 0.0424524\n\n[[3]]$lm_sf_omega_bsld\n[1] 0.005655166\n\n[[3]]$lm_sf_omega_ks\n[1] 0.0681716\n\n[[3]]$lm_sf_omega_kg\n[1] 0.00261028\n\n[[3]]$lm_sf_sigma\n[1] 0.01208157\n\n[[3]]$lm_sf_eta_tilde_bsld\n[1] 0.0116784\n\n[[3]]$lm_sf_eta_tilde_ks\n[1] -0.0006022549\n\n[[3]]$lm_sf_eta_tilde_kg\n[1] -0.003411673\n\n[[3]]$sm_weibull_ph_lambda\n[1] 0.6950308\n\n[[3]]$sm_weibull_ph_gamma\n[1] 1.487697\n\n[[3]]$beta_os_cov\n[1] -0.1517763\n\n[[3]]$link_growth\n[1] -0.2903353\n\n\n[[4]]\n[[4]]$lm_sf_mu_bsld\n[1] 4.189381\n\n[[4]]$lm_sf_mu_ks\n[1] -0.6483557\n\n[[4]]$lm_sf_mu_kg\n[1] 0.0354674\n\n[[4]]$lm_sf_omega_bsld\n[1] 0.01754125\n\n[[4]]$lm_sf_omega_ks\n[1] 0.009195094\n\n[[4]]$lm_sf_omega_kg\n[1] 0.001501245\n\n[[4]]$lm_sf_sigma\n[1] 0.0400288\n\n[[4]]$lm_sf_eta_tilde_bsld\n[1] -0.002014783\n\n[[4]]$lm_sf_eta_tilde_ks\n[1] 0.005064267\n\n[[4]]$lm_sf_eta_tilde_kg\n[1] -0.006308845\n\n[[4]]$sm_weibull_ph_lambda\n[1] 0.7055057\n\n[[4]]$sm_weibull_ph_gamma\n[1] 1.487326\n\n[[4]]$beta_os_cov\n[1] -0.01508292\n\n[[4]]$link_growth\n[1] -0.1286216\n\n\nIf we don’t do this, then it is very likely that some chains will diverge because of very unrealistic initial values for beta and/or the link coefficient.\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-jm/jm1.rds\")\nif (file.exists(save_file)) {\n    joint_results &lt;- readRDS(save_file)\n} else {\n    joint_results &lt;- sampleStanModel(\n        joint_mod,\n        data = joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(joint_results, file = save_file)\n}\n\n\nAs mentioned before, also here we can get warnings at the beginning of the chains’ sampling process (“The current Metropolis proposal is about to be rejected …”). As long as this only happens in the beginning, and not during the sampling later, then this is not a cause for concern.\nWe note that the MCMC sampling process takes much longer here (about factor 10 more) compared to just fitting the TGI or the OS data separately. This is due to the more complex likelihood function calculations for the joint TGI-OS model.\nLet’s check the convergence of the population parameters. If we don’t remember their names, we can query them as follows:\n\n\nShow the code\njoint_results\n\n\n\n   JointModelSamples Object with:\n  \n      # of samples per chain = 1000\n      # of chains            = 4\n  \n      Variables:\n          beta_os_cov[5]\n          link_coefficients\n          link_function_inputs[203, 3]\n          link_growth\n          lm_sf_eta_tilde_bsld[203]\n          lm_sf_eta_tilde_kg[203]\n          lm_sf_eta_tilde_ks[203]\n          lm_sf_mu_bsld\n          lm_sf_mu_kg[2]\n          lm_sf_mu_ks[2]\n          lm_sf_omega_bsld\n          lm_sf_omega_kg[2]\n          lm_sf_omega_ks[2]\n          lm_sf_psi_bsld[203]\n          lm_sf_psi_kg[203]\n          lm_sf_psi_ks[203]\n          lm_sf_sigma\n          log_surv_fit_at_obs_times[203]\n          long_obvs_log_lik[1093]\n          lp__\n          os_cov_contribution[203]\n          os_subj_log_lik[203]\n          pars_os[2]\n          sm_weibull_ph_gamma\n          sm_weibull_ph_lambda\n          Ypred[1093] \n\n\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\",\n    \"beta_os_cov\",\n    \"sm_weibull_ph_gamma\",\n    \"sm_weibull_ph_lambda\",\n    \"link_growth\"\n)\n\nmcmc_joint_results &lt;- cmdstanr::as.CmdStanMCMC(joint_results)\nmcmc_joint_results$summary(vars)\n\n\n# A tibble: 19 × 10\n   variable         mean   median      sd     mad      q5     q95  rhat ess_bulk\n   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu_bs…  3.76     3.76    0.0371  0.0355   3.69    3.82   1.02      192.\n 2 lm_sf_mu_ks… -0.0188   0.00943 0.235   0.228   -0.445   0.339  1.00      462.\n 3 lm_sf_mu_ks… -1.06    -1.03    0.330   0.314   -1.66   -0.591  1.01      559.\n 4 lm_sf_mu_kg… -0.585   -0.576   0.139   0.146   -0.837  -0.375  0.999     500.\n 5 lm_sf_mu_kg… -0.892   -0.890   0.139   0.138   -1.14   -0.656  1.00      552.\n 6 lm_sf_sigma   0.129    0.129   0.00371 0.00366  0.123   0.135  1.00      840.\n 7 lm_sf_omega…  0.530    0.529   0.0265  0.0256   0.488   0.576  1.01      340.\n 8 lm_sf_omega…  0.936    0.913   0.178   0.169    0.680   1.27   1.01      409.\n 9 lm_sf_omega…  1.36     1.33    0.252   0.247    0.989   1.81   1.00      566.\n10 lm_sf_omega…  0.684    0.677   0.0812  0.0744   0.561   0.831  1.00      781.\n11 lm_sf_omega…  0.956    0.952   0.0953  0.0973   0.810   1.12   1.01      585.\n12 beta_os_cov…  0.842    0.829   0.234   0.229    0.464   1.24   1.000     812.\n13 beta_os_cov…  0.00316  0.00296 0.00981 0.00996 -0.0128  0.0197 0.998     975.\n14 beta_os_cov…  0.658    0.656   0.431   0.428   -0.0678  1.36   1.00      901.\n15 beta_os_cov… -0.00443 -0.00251 0.237   0.247   -0.371   0.371  1.00      760.\n16 beta_os_cov…  0.313    0.305   0.215   0.217   -0.0363  0.677  1.00      735.\n17 sm_weibull_…  1.81     1.81    0.167   0.170    1.55    2.09   1.00      794.\n18 sm_weibull_…  0.261    0.213   0.176   0.132    0.0733  0.635  0.998     805.\n19 link_growth   0.880    0.861   0.235   0.242    0.525   1.29   1.00      571.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nShow the code\ndraws_joint_results &lt;- mcmc_joint_results$draws(vars)\nmcmc_trace(draws_joint_results)\n\n\n\n\n\n\n\n\n\nSo this looks good.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#interpret-covariate-effects",
    "href": "session-jm/1_jm-jmpost.html#interpret-covariate-effects",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Interpret covariate effects",
    "text": "Interpret covariate effects\nIn order to better see which of the survival model coefficients relate to which covariates, we can again rename them as follows:\n\n\nShow the code\nos_cov_name_mapping &lt;- function(surv_data) {\n    surv_data_design &lt;- as_stan_list(surv_data)$os_cov_design\n    os_cov_names &lt;- colnames(surv_data_design)\n    old_coef_names &lt;- as.character(glue::glue(\"beta_os_cov[{seq_along(os_cov_names)}]\"))\n    setNames(old_coef_names, os_cov_names)\n}\nos_cov_renaming &lt;- os_cov_name_mapping(surv_data)\ndraws_joint_results &lt;- do.call(\n    rename_variables,\n    c(list(draws_joint_results), os_cov_renaming)\n)\nmcmc_dens_overlay(draws_joint_results) +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nIf we compare this with the covariate effect estimates from the 2-stage OS model we did in the last session, then we can see:\n\nthe log_kg_est effect (see link_growth here) is stronger here, but also with larger uncertainty\nthe ecog effect is similar (clearly higher risk with ECOG 1)\nthe age effect is similar (no effect)\nthe race effect is similar (almost no effect)\nthe sex effect is similar (higher risk for males)\n\nIt is also interesting to look at the shrinkage and growth rate estimates from the SF model part: We see e.g. 0 shrinkage in arm 1, which is the control arm, while we see a strong shrinkage in arm 2, which is the Atezo arm.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#sld-vs-longitudinal-model-fit",
    "href": "session-jm/1_jm-jmpost.html#sld-vs-longitudinal-model-fit",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "SLD vs longitudinal model fit",
    "text": "SLD vs longitudinal model fit\nLet’s first check the fit of the Stein-Fojo model to the SLD data.\nThe first step is to generate the predictions at the subject level. We can do this using the LongitudinalQuantities() function, which takes the MCMC results and the grid at which the predictions should be made. Here we use the GridObserved() function, which takes the IDs of the subjects for which the predictions should be made. Since each patient has its own plot, we sample a small subset of patient IDs here as an example only. In a real application we could write a simple loop that then processes batches of patients in sequence.\n\n\nShow the code\nset.seed(521)\npt_subset &lt;- as.character(sample(subj_df$id, 20))\n\ntgi_fit_pred &lt;- LongitudinalQuantities(\n    joint_results,\n    grid = GridObserved(subjects = pt_subset)\n)\n\n\nNote that here again a Stan program needs to be compiled, which can take some time (but only the first time, because the executable is cached). This is because we pass the posterior samples to a Stan program which then generates the quantities of interest, here the Stein-Fojo model fit for each patient at the observed time points.\nNow we can plot the predictions:\n\n\nShow the code\nautoplot(tgi_fit_pred) +\n    labs(x = \"Time (years)\", y = \"SLD (mm)\")\n\n\n\n\n\n\n\n\n\nWe can see that the model fits the data well, with the estimates Stein-Fojo model curves closely following the observed values.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#kaplan-meier-vs-survival-model-fit",
    "href": "session-jm/1_jm-jmpost.html#kaplan-meier-vs-survival-model-fit",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Kaplan-Meier vs survival model fit",
    "text": "Kaplan-Meier vs survival model fit\nAnother useful plot displays the model predicted survival function and overlays the non-parametric Kaplan-Meier plot to it.\nThe first step consists in generating the survival predictions at the group level with the SurvivalQuantities() function. In order to do this, we use now the GridGrouped() function, which takes the time points at which the predictions should be made and the groups for which the predictions should be made (as a list containing the IDs in each element defining the group). This works the same way as in the previous session with the OS model.\n\n\nShow the code\ntime_grid &lt;- seq(from = 0, to = max(os_data$os_time), length = 100)\nos_surv_group_grid &lt;- GridGrouped(\n    times = time_grid,\n    groups = with(\n        subj_df,\n        split(as.character(id), arm)\n    )\n)\nos_surv_pred &lt;- SurvivalQuantities(\n    object = joint_results,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\n\n\nNow we can use the autoplot() method:\n\n\nShow the code\nautoplot(os_surv_pred, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nWe can see that the fit looks adequate, with the modelled survival functions closely following the Kaplan-Meier curves in each treatment group. Of note, this fit looks better than in the 2-stage TGI-OS model from the previous session.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#hazard-and-hazard-rate-estimation",
    "href": "session-jm/1_jm-jmpost.html#hazard-and-hazard-rate-estimation",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Hazard and hazard rate estimation",
    "text": "Hazard and hazard rate estimation\nSimilarly to the survival function estimation, we can also estimate the hazard function by treatment group.\n\n\nShow the code\nos_hazard_pred &lt;- SurvivalQuantities(\n    object = joint_results,\n    grid = os_surv_group_grid,\n    type = \"haz\"\n)\n\n\nAlso this can be plotted using the autoplot() method:\n\n\nShow the code\nautoplot(os_hazard_pred, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nWe can already see here that this looks slightly different than the same plot from the 2-step TGI-OS model:\n\nThe hazards are higher\nThe uncertainty is considerably larger\n\nNow let’s look at the estimated hazard ratio:\n\n\nShow the code\nos_hr_est &lt;- os_hazard_pred |&gt;\n    as.data.frame() |&gt;\n    group_by(group, time) |&gt;\n    mutate(sample = row_number()) |&gt;\n    pivot_wider(names_from = group, values_from = values) |&gt;\n    mutate(hr = MPDL3280A / Docetaxel) |&gt;\n    group_by(time) |&gt;\n    summarize(\n        mean = mean(hr, na.rm = TRUE),\n        lower = quantile(hr, 0.05, na.rm = TRUE),\n        upper = quantile(hr, 0.95, na.rm = TRUE)\n    ) |&gt;\n    na.omit() # Omit the time = 0 which has NA\nsummary(os_hr_est)\n\n\n      time              mean            lower            upper      \n Min.   :0.02276   Min.   :0.8703   Min.   :0.6966   Min.   :1.063  \n 1st Qu.:0.58038   1st Qu.:0.8703   1st Qu.:0.6966   1st Qu.:1.063  \n Median :1.13801   Median :0.8703   Median :0.6966   Median :1.063  \n Mean   :1.13801   Mean   :0.8703   Mean   :0.6966   Mean   :1.063  \n 3rd Qu.:1.69563   3rd Qu.:0.8703   3rd Qu.:0.6966   3rd Qu.:1.063  \n Max.   :2.25325   Max.   :0.8703   Max.   :0.6966   Max.   :1.063  \n\n\nAlso here the hazard ratio is indeed constant over time, which was the same in the 2-step TGI-OS model. This is because the link between the longitudinal and the survival model is here the log growth rate of the Stein-Fojo model, which is constant over time. For other link functions that are time-varying, e.g. the derivative of the longitudinal model, the hazard ratio could change over time.\nSo here the estimated hazard ratio is 0.87 with a 90% credible interval of 0.7 to 1.06. We can see that this hazard ratio estimate is slightly lower, representing a slightly stronger effect estimate. Due to the larger uncertainty, which is due to the propagation of the log growth rate uncertainty to the OS model, the 90% CI now actually includes 1, in contrast to the 2-step TGI-OS model.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#alternative-covariate-specification",
    "href": "session-jm/1_jm-jmpost.html#alternative-covariate-specification",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Alternative covariate specification",
    "text": "Alternative covariate specification\nIn the previous OS session we tried to include the treatment arm as a direct covariate. We can also try this here.\n\n\nShow the code\nsurv_data_with_arm &lt;- DataSurvival(\n    data = os_data,\n    # Here we add the arm covariate:\n    formula = update(surv_data@formula, . ~ . + arm)\n)\njoint_data_with_arm &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data,\n    survival = surv_data_with_arm\n)\n\nsave_file &lt;- here(\"session-jm/jm2.rds\")\nif (file.exists(save_file)) {\n    joint_results_with_arm &lt;- readRDS(save_file)\n} else {\n    joint_results_with_arm &lt;- sampleStanModel(\n        joint_mod,\n        data = joint_data_with_arm,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(joint_results_with_arm, file = save_file)\n}\n\nmcmc_joint_arm_results &lt;- cmdstanr::as.CmdStanMCMC(joint_results_with_arm)\nmcmc_joint_arm_results$summary(vars)\n\n\n# A tibble: 20 × 10\n   variable         mean   median      sd     mad      q5     q95  rhat ess_bulk\n   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu_bs…  3.75     3.76    0.0373  0.0377   3.69    3.82   1.02      196.\n 2 lm_sf_mu_ks… -0.120   -0.0763  0.274   0.269   -0.590   0.271  1.01      455.\n 3 lm_sf_mu_ks… -1.00    -0.972   0.328   0.315   -1.59   -0.525  1.00      538.\n 4 lm_sf_mu_kg… -0.643   -0.638   0.154   0.153   -0.908  -0.402  1.01      500.\n 5 lm_sf_mu_kg… -0.868   -0.862   0.146   0.146   -1.12   -0.640  1.01      522.\n 6 lm_sf_sigma   0.129    0.129   0.00356 0.00356  0.124   0.136  1.00      944.\n 7 lm_sf_omega…  0.531    0.530   0.0279  0.0268   0.489   0.583  1.01      342.\n 8 lm_sf_omega…  0.979    0.959   0.191   0.181    0.713   1.34   1.00      481.\n 9 lm_sf_omega…  1.32     1.29    0.240   0.232    0.972   1.74   1.00      580.\n10 lm_sf_omega…  0.687    0.680   0.0871  0.0896   0.560   0.844  1.00      911.\n11 lm_sf_omega…  0.957    0.951   0.0951  0.0898   0.820   1.13   1.00      647.\n12 beta_os_cov…  0.864    0.858   0.235   0.237    0.477   1.26   1.00     1028.\n13 beta_os_cov…  0.00372  0.00329 0.00996 0.0104  -0.0125  0.0197 0.999     964.\n14 beta_os_cov…  0.690    0.694   0.445   0.447   -0.0367  1.44   1.00      930.\n15 beta_os_cov…  0.0296   0.0232  0.248   0.239   -0.371   0.439  0.999     906.\n16 beta_os_cov…  0.310    0.307   0.216   0.218   -0.0299  0.681  0.999     958.\n17 beta_os_cov… -0.207   -0.204   0.239   0.229   -0.606   0.176  0.999     767.\n18 sm_weibull_…  1.82     1.81    0.163   0.160    1.56    2.10   1.01      836.\n19 sm_weibull_…  0.272    0.228   0.193   0.148    0.0755  0.608  0.999    1000.\n20 link_growth   0.870    0.862   0.239   0.239    0.499   1.27   1.01      617.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nShow the code\ndraws_joint_arm_results &lt;- mcmc_joint_arm_results$draws(vars)\nmcmc_trace(draws_joint_arm_results)\n\n\n\n\n\n\n\n\n\nWe can easily plot the survival functions and compare them with the Kaplan-Meier curves of the treatment arms, because we can reuse the above os_surv_group_grid:\n\n\nShow the code\njoint_mod_with_arm_os_pred &lt;- SurvivalQuantities(\n    object = joint_results_with_arm,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\nautoplot(joint_mod_with_arm_os_pred, add_km = TRUE, add_wrap = FALSE)",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#alternative-time-varying-link",
    "href": "session-jm/1_jm-jmpost.html#alternative-time-varying-link",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Alternative time-varying link",
    "text": "Alternative time-varying link\nWe would like to illustrate how the hazard ratio could change over time if we used a time-varying link function. For this we will use the derivative of the Stein-Fojo model as the link function, utilizing the linkDLSD class in jmpost.\nNote that at the moment, we can unfortunately not conveniently reuse the slots from the JointModel object (joint_mod@longitudinal and joint_mod@survival) to create a new model with a different link function, because during the object creation already Stan parameter names are created, which don’t fit then anymore here. So if needed, it would be better to save the object from LongitudinalSteinFojo() first and then use it in both models, and similarly for the SurvivalWeibullPH() object. Here we just copy/paste the model specification now.\n\n\nShow the code\njoint_mod_dsld &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    ),\n    survival = SurvivalWeibullPH(\n        lambda = prior_gamma(0.7, 1),\n        gamma = prior_gamma(1.5, 1),\n        beta = prior_normal(0, 20)\n    ),\n    link = linkDSLD(\n        prior = prior_normal(0, 10) # Reduce here a bit to help with convergence ...\n    )\n)\n\n\nLet’s check the initial values again:\n\n\nShow the code\noptions(\"jmpost.prior_shrinkage\" = 0.999)\n\ninitialValues(joint_mod_dsld, n_chains = CHAINS)\n\n\n[[1]]\n[[1]]$lm_sf_mu_bsld\n[1] 4.174281\n\n[[1]]$lm_sf_mu_ks\n[1] -0.6536928\n\n[[1]]$lm_sf_mu_kg\n[1] 0.0384735\n\n[[1]]$lm_sf_omega_bsld\n[1] 0.002839366\n\n[[1]]$lm_sf_omega_ks\n[1] 0.0001291699\n\n[[1]]$lm_sf_omega_kg\n[1] 0.0007976942\n\n[[1]]$lm_sf_sigma\n[1] 0.005996095\n\n[[1]]$lm_sf_eta_tilde_bsld\n[1] 0.000470039\n\n[[1]]$lm_sf_eta_tilde_ks\n[1] -0.00154869\n\n[[1]]$lm_sf_eta_tilde_kg\n[1] 0.0008745867\n\n[[1]]$sm_weibull_ph_lambda\n[1] 0.6995336\n\n[[1]]$sm_weibull_ph_gamma\n[1] 1.498641\n\n[[1]]$beta_os_cov\n[1] 0.009185111\n\n[[1]]$link_dsld\n[1] -0.002447209\n\n\n[[2]]\n[[2]]$lm_sf_mu_bsld\n[1] 4.172685\n\n[[2]]$lm_sf_mu_ks\n[1] -0.6525602\n\n[[2]]$lm_sf_mu_kg\n[1] 0.04002759\n\n[[2]]$lm_sf_omega_bsld\n[1] 0.001985338\n\n[[2]]$lm_sf_omega_ks\n[1] 0.005169107\n\n[[2]]$lm_sf_omega_kg\n[1] 0.001121513\n\n[[2]]$lm_sf_sigma\n[1] 0.004110273\n\n[[2]]$lm_sf_eta_tilde_bsld\n[1] -0.0006774594\n\n[[2]]$lm_sf_eta_tilde_ks\n[1] -0.0008570452\n\n[[2]]$lm_sf_eta_tilde_kg\n[1] -0.0005290799\n\n[[2]]$sm_weibull_ph_lambda\n[1] 0.6993776\n\n[[2]]$sm_weibull_ph_gamma\n[1] 1.499257\n\n[[2]]$beta_os_cov\n[1] 0.001056433\n\n[[2]]$link_dsld\n[1] -0.01053035\n\n\n[[3]]\n[[3]]$lm_sf_mu_bsld\n[1] 4.173984\n\n[[3]]$lm_sf_mu_ks\n[1] -0.654667\n\n[[3]]$lm_sf_mu_kg\n[1] 0.03969482\n\n[[3]]$lm_sf_omega_bsld\n[1] 0.005543763\n\n[[3]]$lm_sf_omega_ks\n[1] 0.005430364\n\n[[3]]$lm_sf_omega_kg\n[1] 0.0001870963\n\n[[3]]$lm_sf_sigma\n[1] 0.0009273888\n\n[[3]]$lm_sf_eta_tilde_bsld\n[1] -0.0005258746\n\n[[3]]$lm_sf_eta_tilde_ks\n[1] 0.001356768\n\n[[3]]$lm_sf_eta_tilde_kg\n[1] 0.0001322014\n\n[[3]]$sm_weibull_ph_lambda\n[1] 0.7011391\n\n[[3]]$sm_weibull_ph_gamma\n[1] 1.499553\n\n[[3]]$beta_os_cov\n[1] -0.04009133\n\n[[3]]$link_dsld\n[1] 0.02460469\n\n\n[[4]]\n[[4]]$lm_sf_mu_bsld\n[1] 4.173544\n\n[[4]]$lm_sf_mu_ks\n[1] -0.6541084\n\n[[4]]$lm_sf_mu_kg\n[1] 0.03994846\n\n[[4]]$lm_sf_omega_bsld\n[1] 0.001749874\n\n[[4]]$lm_sf_omega_ks\n[1] 0.0004688517\n\n[[4]]$lm_sf_omega_kg\n[1] 0.002471432\n\n[[4]]$lm_sf_sigma\n[1] 0.0006564231\n\n[[4]]$lm_sf_eta_tilde_bsld\n[1] -0.00169486\n\n[[4]]$lm_sf_eta_tilde_ks\n[1] -5.820686e-05\n\n[[4]]$lm_sf_eta_tilde_kg\n[1] -0.0002779012\n\n[[4]]$sm_weibull_ph_lambda\n[1] 0.7002806\n\n[[4]]$sm_weibull_ph_gamma\n[1] 1.499579\n\n[[4]]$beta_os_cov\n[1] 0.02254746\n\n[[4]]$link_dsld\n[1] 0.00770938\n\n\nNow we fit this model:\n\n\nShow the code\nsave_file &lt;- here(\"session-jm/jm3.rds\")\nif (file.exists(save_file)) {\n    joint_results_dsld &lt;- readRDS(save_file)\n} else {\n    joint_results_dsld &lt;- sampleStanModel(\n        joint_mod_dsld,\n        data = joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(joint_results_dsld, file = save_file)\n}\n\n\nLet’s check the convergence of the population parameters:\n\n\nShow the code\nvars_dsld &lt;- c(\"link_dsld\", setdiff(vars, \"link_growth\"))\nmcmc_joint_results_dsld &lt;- cmdstanr::as.CmdStanMCMC(joint_results_dsld)\nmcmc_joint_results_dsld$summary(vars_dsld)\n\n\n# A tibble: 19 × 10\n   variable        mean   median      sd     mad       q5     q95  rhat ess_bulk\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 link_dsld    0.00695  0.00669 0.00240 0.00212  0.00350  0.0111 1.00      932.\n 2 lm_sf_mu_b…  3.76     3.76    0.0379  0.0360   3.69     3.82   1.02      329.\n 3 lm_sf_mu_k… -0.0327  -0.00146 0.244   0.234   -0.488    0.319  1.00      638.\n 4 lm_sf_mu_k… -1.31    -1.28    0.361   0.349   -1.90    -0.744  1.000     648.\n 5 lm_sf_mu_k… -0.591   -0.581   0.142   0.138   -0.834   -0.385  1.00      633.\n 6 lm_sf_mu_k… -1.01    -1.01    0.143   0.142   -1.24    -0.772  1.00      619.\n 7 lm_sf_sigma  0.130    0.130   0.00365 0.00360  0.125    0.136  1.00      731.\n 8 lm_sf_omeg…  0.529    0.529   0.0276  0.0283   0.488    0.577  1.00      568.\n 9 lm_sf_omeg…  0.951    0.932   0.176   0.164    0.702    1.28   1.00      700.\n10 lm_sf_omeg…  1.54     1.51    0.279   0.270    1.12     2.02   1.00      709.\n11 lm_sf_omeg…  0.689    0.683   0.0803  0.0748   0.570    0.825  1.01      752.\n12 lm_sf_omeg…  0.964    0.956   0.107   0.106    0.803    1.15   1.00      630.\n13 beta_os_co…  0.810    0.812   0.239   0.243    0.422    1.21   0.998    1001.\n14 beta_os_co… -0.00132 -0.00149 0.0106  0.00989 -0.0183   0.0168 1.00     1121.\n15 beta_os_co…  0.589    0.613   0.436   0.434   -0.118    1.27   1.00      889.\n16 beta_os_co…  0.0833   0.0861  0.251   0.245   -0.350    0.507  1.00     1017.\n17 beta_os_co…  0.334    0.329   0.223   0.227   -0.0253   0.699  1.00     1109.\n18 sm_weibull…  1.53     1.52    0.144   0.144    1.30     1.78   1.00     1014.\n19 sm_weibull…  0.140    0.114   0.0996  0.0760   0.0359   0.334  1.00     1039.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nSo that looks ok.\nLet’s now calculate the hazard function and hazard ratio for this model. Again we will need to wait for the compilation, because now it is a different model.\n\n\nShow the code\nos_hazard_dsld_pred &lt;- SurvivalQuantities(\n    object = joint_results_dsld,\n    grid = os_surv_group_grid,\n    type = \"haz\"\n)\n\nos_hr_est_dsld &lt;- os_hazard_dsld_pred |&gt;\n    as.data.frame() |&gt;\n    group_by(group, time) |&gt;\n    mutate(sample = row_number()) |&gt;\n    mutate(values = pmin(values, 100)) |&gt;\n    pivot_wider(names_from = group, values_from = values) |&gt;\n    mutate(hr = MPDL3280A / Docetaxel) |&gt;\n    group_by(time) |&gt;\n    summarize(\n        mean = mean(hr, na.rm = TRUE),\n        median = median(hr, na.rm = TRUE),\n        lower = quantile(hr, 0.05, na.rm = TRUE),\n        upper = quantile(hr, 0.95, na.rm = TRUE)\n    ) |&gt;\n    na.omit() # Omit the time = 0 which has NA\nsummary(os_hr_est_dsld)\n\n\n      time              mean             median           lower       \n Min.   :0.02276   Min.   :  1.100   Min.   :  1.00   Min.   : 1.000  \n 1st Qu.:0.58038   1st Qu.:  1.713   1st Qu.:  1.00   1st Qu.: 1.000  \n Median :1.13801   Median :  6.817   Median :  1.00   Median : 1.000  \n Mean   :1.13801   Mean   : 41.550   Mean   : 36.39   Mean   : 1.621  \n 3rd Qu.:1.69563   3rd Qu.: 56.551   3rd Qu.: 24.21   3rd Qu.: 1.087  \n Max.   :2.25325   Max.   :207.570   Max.   :244.91   Max.   :10.414  \n     upper        \n Min.   :  1.000  \n 1st Qu.:  1.493  \n Median : 41.445  \n Mean   :100.978  \n 3rd Qu.:180.477  \n Max.   :402.655  \n\n\nHere we truncate the hazard values at an upper bound of 100, because due to the time-varying link function, the hazard can become very large at some time points, which can lead to numerical issues.\nSo now we can see that the estimated hazard ratio is no longer constant over time. Let’s try to plot it:\n\n\nShow the code\nos_hr_est_dsld |&gt;\n    ggplot(aes(x = time, y = median)) +\n    geom_line() +\n    geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +\n    labs(\n        y = \"Hazard ratio (MPDL3280A / Docetaxel)\",\n        title = \"Time-varying hazard ratio\"\n    )\n\n\n\n\n\n\n\n\n\nAlso here we see very large hazard ratio values between 0.25 and 1 year. So in this case this model would not be very useful in practice.\nLet’s still quickly have a look at the fitted survival functions if they look reasonable:\n\n\nShow the code\nos_surv_dsld_pred &lt;- SurvivalQuantities(\n    object = joint_results_dsld,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\nautoplot(os_surv_dsld_pred, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nSo this looks reasonable. So we might need to revisit the derivative calculation again for this model to see if there is something to improve in the jmpost code.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-jm/1_jm-jmpost.html#model-comparison",
    "href": "session-jm/1_jm-jmpost.html#model-comparison",
    "title": "1. TGI-OS joint model minimal workflow with jmpost",
    "section": "Model comparison",
    "text": "Model comparison\nWe can again use the Brier score to compare the three different survival models as part of the joint models. The Brier score is a measure of the mean squared difference between the predicted survival probability and the actual survival status. The lower the Brier score, the better the model.\nTo calculate it, we need to use the GridFixed input for SurvivalQuantities(). This is because the Brier score is calculated at fixed time points across all patients, and not at the observed time points of specific patients. Because we don’t specify patient IDs, the quantities are generated for all patients.\n\n\nShow the code\nos_surv_fixed &lt;- SurvivalQuantities(\n    object = joint_results,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_bs &lt;- brierScore(os_surv_fixed)\n\n\nLet’s first compare this with the second model, which includes the treatment arm as a direct covariate in the survival model:\n\n\nShow the code\nos_surv_with_arm_fixed &lt;- SurvivalQuantities(\n    object = joint_results_with_arm,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_with_arm_bs &lt;- brierScore(os_surv_with_arm_fixed)\n\n\nNow let’s compare this with the third model where we have the derivative link. We have a suspicion that the third model will perform worse, because of the hazard ratio results, but let’s have a look.\n\n\nShow the code\nos_surv_dsld_fixed &lt;- SurvivalQuantities(\n    object = joint_results_dsld,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\nos_dsld_bs &lt;- brierScore(os_surv_dsld_fixed)\n\n\nWe can plot then all three Brier scores over time to compare them visually:\n\n\nShow the code\ndata.frame(\n    time = time_grid,\n    `1 - 2` = os_bs - os_with_arm_bs,\n    `1 - 3` = os_bs - os_dsld_bs,\n    `2 - 3` = os_with_arm_bs - os_dsld_bs,\n    check.names = FALSE\n) |&gt;\n    pivot_longer(\n        cols = c(\"1 - 2\", \"1 - 3\", \"2 - 3\"),\n        names_to = \"diff\",\n        values_to = \"brier_score_diff\"\n    ) |&gt;\n    ggplot(aes(x = time, y = brier_score_diff, color = diff, group = diff)) +\n    geom_line() +\n    labs(y = \"Brier score difference\") +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nAs expected, we see that especially for the times between 0.5 and 2 years the first and second models with the growth link performs better than the third model with the derivative link. On the other hand, there is almost no difference between the first and second model, which includes the treatment arm covariate.\nCurrently, the LOOIC does not work yet for joint models, because the log_lik is not included in the samples. There is an ongoing discussion on GitHub about this topic.",
    "crumbs": [
      "Session 3: TGI-OS",
      "1. TGI-OS joint model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-pts/1_pts-jmpost.html",
    "href": "session-pts/1_pts-jmpost.html",
    "title": "1. Calculate Bayesian Predictive Power",
    "section": "",
    "text": "The purpose of this document is to show how we can calculate the probability of success given interim data of a clinical trial, based on the TGI-OS joint model results.",
    "crumbs": [
      "Session 4: PTS",
      "1. Calculate Bayesian Predictive Power"
    ]
  },
  {
    "objectID": "session-pts/1_pts-jmpost.html#setup-and-load-data",
    "href": "session-pts/1_pts-jmpost.html#setup-and-load-data",
    "title": "1. Calculate Bayesian Predictive Power",
    "section": "Setup and load data",
    "text": "Setup and load data\nHere we execute the R code from the setup and data preparation chapter, see the full code here.",
    "crumbs": [
      "Session 4: PTS",
      "1. Calculate Bayesian Predictive Power"
    ]
  },
  {
    "objectID": "session-pts/1_pts-jmpost.html#fit-our-joint-tgi-os-model",
    "href": "session-pts/1_pts-jmpost.html#fit-our-joint-tgi-os-model",
    "title": "1. Calculate Bayesian Predictive Power",
    "section": "Fit our joint TGI-OS model",
    "text": "Fit our joint TGI-OS model\nLet’s fit again the same joint TGI-OS model as in the previous session. Now we just put all the code together in a single chunk, and it is a good repetition to see all the steps in one place:\n\n\nShow the code\nsubj_df &lt;- os_data |&gt;\n    mutate(study = \"OAK\") |&gt;\n    select(study, id, arm)\nsubj_data &lt;- DataSubject(\n    data = subj_df,\n    subject = \"id\",\n    arm = \"arm\",\n    study = \"study\"\n)\nlong_df &lt;- tumor_data |&gt;\n    select(id, year, sld)\nlong_data &lt;- DataLongitudinal(\n    data = long_df,\n    formula = sld ~ year\n)\nsurv_data &lt;- DataSurvival(\n    data = os_data,\n    formula = Surv(os_time, os_event) ~ ecog + age + race + sex\n)\njoint_data &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data,\n    survival = surv_data\n)\n\njoint_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    ),\n    survival = SurvivalWeibullPH(\n        lambda = prior_gamma(0.7, 1),\n        gamma = prior_gamma(1.5, 1),\n        beta = prior_normal(0, 20)\n    ),\n    link = linkGrowth(\n        prior = prior_normal(0, 20)\n    )\n)\n\noptions(\"jmpost.prior_shrinkage\" = 0.99)\n# initialValues(joint_mod, n_chains = CHAINS)\n\nsave_file &lt;- here(\"session-pts/jm1.rds\")\nif (file.exists(save_file)) {\n    joint_results &lt;- readRDS(save_file)\n} else {\n    joint_results &lt;- sampleStanModel(\n        joint_mod,\n        data = joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(joint_results, file = save_file)\n}",
    "crumbs": [
      "Session 4: PTS",
      "1. Calculate Bayesian Predictive Power"
    ]
  },
  {
    "objectID": "session-pts/1_pts-jmpost.html#marginal-hazard-ratio-estimation",
    "href": "session-pts/1_pts-jmpost.html#marginal-hazard-ratio-estimation",
    "title": "1. Calculate Bayesian Predictive Power",
    "section": "Marginal Hazard Ratio Estimation",
    "text": "Marginal Hazard Ratio Estimation\nFirst, we will try to apply the methodology from Oudenhoven et al (2020) to estimate the marginal hazard ratio, using our joint model results. We use the new jmpost function called populationHR() for this:\n\n\nShow the code\nsave_file &lt;- here(\"session-pts/jm1hr.rds\")\nif (file.exists(save_file)) {\n    pop_hr &lt;- readRDS(save_file)\n} else {\n    pop_hr &lt;- populationHR(\n        joint_results,\n        hr_formula = ~arm\n    )\n    saveRDS(pop_hr, file = save_file)\n}\npop_hr$summary\n\n\n                          mean     median      X2.5.      X97.5.\nbs(time, df = 10)1  -3.1898619 -3.1492166 -4.6595480 -1.94483471\nbs(time, df = 10)2  -2.1685007 -2.1557204 -2.9129270 -1.44921371\nbs(time, df = 10)3  -1.7541631 -1.7457712 -2.2606129 -1.27922919\nbs(time, df = 10)4  -1.7402008 -1.7351076 -2.1929390 -1.32483535\nbs(time, df = 10)5  -1.2832478 -1.2744974 -1.6118038 -0.99434352\nbs(time, df = 10)6  -1.3567217 -1.3465337 -1.7256844 -1.02999961\nbs(time, df = 10)7  -0.8752984 -0.8702259 -1.1870137 -0.60171732\nbs(time, df = 10)8  -1.2314025 -1.2226766 -1.8595341 -0.71176107\nbs(time, df = 10)9  -0.1212043 -0.1255963 -0.6237809  0.40773756\nbs(time, df = 10)10 -0.5070327 -0.4897161 -1.1333247  0.02179326\narmMPDL3280A        -0.2448721 -0.2361508 -0.5174069 -0.00558422\n\n\nSo here we have the marginal log hazard ratio estimates of the baseline spline components and the treatment arm. Therefore we can get the hazard ratio estimates by exponentiating the log hazard ratio estimates:\n\n\nShow the code\nhr_est &lt;- pop_hr$summary[\"armMPDL3280A\", ] |&gt;\n    sapply(exp)\n\n\nSo we get a marginal hazard ratio estimate of around 0.783 and a 95% credible interval of 0.6 - 0.99.",
    "crumbs": [
      "Session 4: PTS",
      "1. Calculate Bayesian Predictive Power"
    ]
  },
  {
    "objectID": "session-pts/1_pts-jmpost.html#comparison-with-simple-cox-ph-results",
    "href": "session-pts/1_pts-jmpost.html#comparison-with-simple-cox-ph-results",
    "title": "1. Calculate Bayesian Predictive Power",
    "section": "Comparison with simple Cox PH results",
    "text": "Comparison with simple Cox PH results\nWe can do a little sanity check by comparing this with a very simple Cox model:\n\n\nShow the code\ncox_mod &lt;- survival::coxph(\n    update(joint_results@data@survival@formula, ~ . + arm),\n    data = joint_results@data@survival@data\n)\nsummary(cox_mod)\n\n\nCall:\nsurvival::coxph(formula = update(joint_results@data@survival@formula, \n    ~. + arm), data = joint_results@data@survival@data)\n\n  n= 203, number of events= 108 \n\n                  coef exp(coef)  se(coef)      z Pr(&gt;|z|)   \necog1         0.617895  1.855019  0.209174  2.954  0.00314 **\nage           0.002656  1.002659  0.009982  0.266  0.79018   \nraceOTHER     0.602237  1.826200  0.400528  1.504  0.13268   \nraceWHITE     0.082344  1.085829  0.229687  0.359  0.71997   \nsexM          0.297556  1.346563  0.201567  1.476  0.13989   \narmMPDL3280A -0.330645  0.718460  0.198119 -1.669  0.09513 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n             exp(coef) exp(-coef) lower .95 upper .95\necog1           1.8550     0.5391    1.2311     2.795\nage             1.0027     0.9973    0.9832     1.022\nraceOTHER       1.8262     0.5476    0.8329     4.004\nraceWHITE       1.0858     0.9210    0.6922     1.703\nsexM            1.3466     0.7426    0.9071     1.999\narmMPDL3280A    0.7185     1.3919    0.4873     1.059\n\nConcordance= 0.621  (se = 0.027 )\nLikelihood ratio test= 14.6  on 6 df,   p=0.02\nWald test            = 14.34  on 6 df,   p=0.03\nScore (logrank) test = 14.63  on 6 df,   p=0.02\n\n\nHere we get a different HR estimate of around 0.72, which is lower than the one we got from the joint model. We should expect different results because the joint model takes into account the TGI effect, while the Cox model does not.\nAnd if we just put the treatment arm into the Cox model we get:\n\n\nShow the code\ncox_mod_arm &lt;- survival::coxph(\n    update(joint_results@data@survival@formula, . ~ arm),\n    data = joint_results@data@survival@data\n)\ncox_mod_arm\n\n\nCall:\nsurvival::coxph(formula = update(joint_results@data@survival@formula, \n    . ~ arm), data = joint_results@data@survival@data)\n\n                coef exp(coef) se(coef)      z    p\narmMPDL3280A -0.2414    0.7855   0.1928 -1.252 0.21\n\nLikelihood ratio test=1.56  on 1 df, p=0.2109\nn= 203, number of events= 108 \n\n\nHere we get a HR of 0.785 that is close to the marginal HR estimate above (when we did not condition on the other covariates), which is reassuring. On the other hand, the 95% confidence interval is:\n\n\nShow the code\nexp(confint(cox_mod_arm)[\"armMPDL3280A\", ])\n\n\n    2.5 %    97.5 % \n0.5383409 1.1461249 \n\n\nso actually overlaps the null hypothesis of 1. So we see that the joint model helped us to obtain a more precise estimate of the marginal hazard ratio.",
    "crumbs": [
      "Session 4: PTS",
      "1. Calculate Bayesian Predictive Power"
    ]
  },
  {
    "objectID": "session-pts/1_pts-jmpost.html#pts-based-on-frequentist-hr-properties",
    "href": "session-pts/1_pts-jmpost.html#pts-based-on-frequentist-hr-properties",
    "title": "1. Calculate Bayesian Predictive Power",
    "section": "PTS based on frequentist HR properties",
    "text": "PTS based on frequentist HR properties\nFrom now on we pretend that the data we have analyzed so far is the interim data of the Oak trial, and we want to calculate the predictive power of the trial based on this interim data.\nLet’s first try to use the log HR (\\(\\delta\\)) frequentist distribution to calculate the predictive power of the trial. We have:\n\\[\n\\hat{\\delta}_{\\text{fin}} \\vert \\hat{\\delta}_{\\text{int}}\n\\sim\n\\text{Normal}(\\hat{\\delta}_{\\text{int}}, \\sigma^2_{\\text{int}})\n\\]\nwhere the predictive variance is given by\n\\[\n\\sigma^2_{\\text{int}} = \\frac{1}{4\\overline{p}r(1-r)} \\frac{\\overline{m}}{\\overline{n} (\\overline{n} + \\overline{m})}\n\\]\nOn the other hand, assuming that we have observed the final data, then the variance estimate for the estimator \\(\\hat{\\delta}_{\\text{fin}}\\) is given by:\n\\[\n\\sigma^2_{\\text{fin}} = \\frac{1}{4\\overline{p}r(1-r)} \\frac{1}{\\overline{n} + \\overline{m}}\n\\]\nSo the \\((1-\\alpha)\\)-confidence interval for the final log HR is given by:\n\\[\n\\hat{\\delta}_{\\text{fin}} \\pm z_{1 - \\alpha/2} \\sigma_{\\text{fin}}\n\\]\nand the null hypothesis is rejected if the upper bound of this confidence interval is below 0, i.e. if:\n\\[\n\\hat{\\delta}_{\\text{fin}} + z_{1 - \\alpha/2} \\sigma_{\\text{fin}} &lt; 0.\n\\]\nNow let’s again take the perspective that we are at the interim analysis and we want to calculate the predictive probability of this event, then:\n\\[\n\\begin{align*}\n\\mathbb{P}(\\hat{\\delta}_{\\text{fin}} + z_{1 - \\alpha/2} \\sigma_{\\text{fin}} &lt; 0)\n&=\n\\mathbb{P}(\\hat{\\delta}_{\\text{fin}} &lt; - z_{1 - \\alpha/2} \\sigma_{\\text{fin}}) \\\\\n&=\n\\mathbb{P}\\left(\n\\frac{\\hat{\\delta}_{\\text{fin}} - \\hat{\\delta}_{\\text{int}}}{\\sigma_{\\text{int}}} &lt;\n\\frac{- z_{1 - \\alpha/2} \\sigma_{\\text{fin}} - \\hat{\\delta}_{\\text{int}}}{\\sigma_{\\text{int}}}\n\\right) \\\\\n&=\n\\Phi\\left(\n  \\frac{- z_{1 - \\alpha/2} \\sigma_{\\text{fin}} - \\hat{\\delta}_{\\text{int}}}{\\sigma_{\\text{int}}}\n\\right)\n\\end{align*}\n\\]\nbecause the left-hand side is a standard normal variable according to our formula above. Let’s first write a little function to calculate this:\n\n\nShow the code\npts_freq_hr &lt;- function(delta_int, var_int, var_final, alpha = 0.05) {\n    z_alpha &lt;- qnorm(1 - alpha / 2)\n    delta_final &lt;- -z_alpha * sqrt(var_final)\n    pnorm((delta_final - delta_int) / sqrt(var_int))\n}\n\n\nFor \\(\\hat{\\delta}_{\\text{int}}\\) we are going to plug in our MCMC samples for the marginal log HR \\(\\hat{\\delta}_{\\text{int}}\\) to compute the PTS based on this:\n\n\nShow the code\nlog_hr_samples &lt;- pop_hr[[2]][\"armMPDL3280A\", ]\n\n\nBut first we also need the quantities that go into the variance formula:\n\n\\(\\overline{p}\\): average event rate\n\\(r\\): proportion of patients in the treatment arm\n\\(\\overline{n}\\): average arm size in interim data\n\\(\\overline{m}\\): average arm size in follow up\n\nSo let’s calculate these and the resulting variances:\n\n\nShow the code\navg_event_rate &lt;- mean(os_data$os_event)\nprop_pts_treatment &lt;- mean(os_data$arm == \"MPDL3280A\")\navg_arm_size_interim &lt;- mean(table(os_data$arm))\ntotal_size_final &lt;- 850\navg_arm_size_followup &lt;- (total_size_final - 2 * avg_arm_size_interim) / 2\nvar_int &lt;- 1 /\n    (4 * avg_event_rate * prop_pts_treatment * (1 - prop_pts_treatment)) *\n    avg_arm_size_followup /\n    (avg_arm_size_interim * (avg_arm_size_interim + avg_arm_size_followup))\nvar_final &lt;- 1 /\n    (4 * avg_event_rate * prop_pts_treatment * (1 - prop_pts_treatment)) *\n    1 /\n    (avg_arm_size_interim + avg_arm_size_followup)\n\n\nNow we can calculate the PTS based on the frequentist HR properties:\n\n\nShow the code\npts_freq_hr_result &lt;- mean(pts_freq_hr(\n    delta_int = log_hr_samples,\n    var_int = var_int,\n    var_final = var_final\n))\npts_freq_hr_result\n\n\n[1] 0.7318325\n\n\nSo we obtain a PTS of 73.2% here.",
    "crumbs": [
      "Session 4: PTS",
      "1. Calculate Bayesian Predictive Power"
    ]
  },
  {
    "objectID": "session-pts/1_pts-jmpost.html#pts-based-on-frequentist-log-rank-test-properties",
    "href": "session-pts/1_pts-jmpost.html#pts-based-on-frequentist-log-rank-test-properties",
    "title": "1. Calculate Bayesian Predictive Power",
    "section": "PTS based on frequentist log-rank test properties",
    "text": "PTS based on frequentist log-rank test properties\nNow we want to leverage the rpact function getConditionalPower() to calculate the predictive power of the trial based on the log-rank test statistic properties.\nLooking at the function’s example, we first have to create a DataSet object with the interim data. Here it is important that we for cumLogRanks also the z scores from a Cox regression can be used, which means for our case that can insert here the z-scores defined as:\n\\[\nz = \\frac{\\hat{\\delta}_{\\text{int}}}{\\sigma_{\\text{int}}}\n\\]\nvalues.\nBut let’s first try this with a simple fixed \\(z\\) value to see how it works:\n\n\nShow the code\nlibrary(rpact)\n\n\nInstallation qualification for rpact 4.2.1 has not yet been performed.\n\n\nPlease run testPackage() before using the package in GxP relevant environments.\n\n\nShow the code\nz &lt;- log(0.9) / sqrt(var_int) # Instead of log(0.9) we put later the marginal log HR MCMC sample\ndata &lt;- getDataset(\n    cumEvents = sum(os_data$os_event),\n    cumLogRanks = z,\n    cumAllocationRatios = prop_pts_treatment\n)\ndata\n\n\nDataset of survival data\n\nStages: 1\nCumulative events: 108\nCumulative allocation ratios: 0.527\nCumulative log-ranks: -0.886\n\nCalculated data\n\nNumber of events: 108\nAllocation ratios: 0.527093596059113\nLog-ranks: -0.886\n\n\n\nThis is the data set from the first stage, i.e. our interim analysis.\nNow we need to define the design of the group sequential trial:\n\n\nShow the code\nevents_final &lt;- total_size_final * avg_event_rate\ndesign &lt;- getDesignGroupSequential(\n    kMax = 2,\n    informationRates = c(sum(os_data$os_event) / events_final, 1),\n    alpha = 0.05\n)\ndesign\n\n\nDesign parameters and output of group sequential design\nUser defined parameters\n\nInformation rates: 0.239, 1.000\nSignificance level: 0.0500\n\nDerived from user defined parameters\n\nMaximum number of stages: 2\nStages: 1, 2\nFutility bounds (non-binding): -Inf\n\nDefault parameters\n\nType of design: O’Brien & Fleming\nType II error rate: 0.2000\nBinding futility: FALSE\nTest: one-sided\nTolerance: 1e-08\n\nOutput\n\nCumulative alpha spending: 0.000377, 0.050000\nCritical values: 3.369, 1.646\nStage levels (one-sided): 0.000377, 0.049833\n\n\n\nWe see that the significance level is almost full for the final analysis, because by default the O’Brien & Fleming design is used, which only assigns a very small \\(\\alpha\\) to the interim analysis. This is kind of what we need here.\nFinally we put the design and data in a StageResults object, which is the input for the getConditionalPower() function:\n\n\nShow the code\nstageResults &lt;- getStageResults(\n    design = design,\n    dataInput = data,\n    stage = 1,\n    directionUpper = FALSE\n)\ncondPower &lt;- getConditionalPower(\n    stageResults = stageResults,\n    thetaH1 = 0.9, # Here we put later the marginal hazard ratio MCMC sample\n    nPlanned = total_size_final,\n    allocationRatioPlanned = 1\n)\nsummary(condPower)\n\n\nTechnical developer summary of the Conditional power results survival object (\"ConditionalPowerResultsSurvival\"):\n\n  [u] Planned sample size              : NA, 850 \n  [d] Planned allocation ratio         : 1 \n  [.] %simulated%                      : FALSE \n  [g] Conditional power                : NA, 0.5577 \n  [u] Assumed effect under alternative : 0.9 \n\nLegend:\n  u: user defined\n  &gt;: derived value\n  d: default value\n  g: generated/calculated value\n  .: not applicable or hidden\n\nConditional power results survival table:\n     Planned sample size Conditional power\n[1,]                  NA                NA\n[2,]                 850         0.5576662\n\n\nShow the code\ncondPower$conditionalPower[2]\n\n\n[1] 0.5576662\n\n\nOK so now that we know how it works we wrap this in a little function again.\n\n\nShow the code\npts_freq_hr2 &lt;- function(\n    delta_int,\n    var_int,\n    var_final,\n    events_int,\n    alloc_int,\n    events_final,\n    alpha = 0.05) {\n    data &lt;- getDataset(\n        cumEvents = events_int,\n        cumLogRanks = delta_int / sqrt(var_int),\n        cumAllocationRatios = alloc_int\n    )\n    design &lt;- getDesignGroupSequential(\n        kMax = 2,\n        informationRates = c(events_int / events_final, 1),\n        alpha = alpha\n    )\n    stageResults &lt;- getStageResults(\n        design = design,\n        dataInput = data,\n        stage = 1,\n        directionUpper = FALSE\n    )\n    condPower &lt;- getConditionalPower(\n        stageResults = stageResults,\n        thetaH1 = exp(delta_int),\n        nPlanned = total_size_final,\n        allocationRatioPlanned = 1\n    )\n    condPower$conditionalPower[2]\n}\n\n\nLet’s try it out with the same example first again:\n\n\nShow the code\npts_freq_hr2(\n    delta_int = log(0.9),\n    var_int = var_int,\n    var_final = var_final,\n    events_int = sum(os_data$os_event),\n    alloc_int = prop_pts_treatment,\n    events_final = total_size_final * avg_event_rate\n)\n\n\n[1] 0.5576662\n\n\nOK so that gives the same result as before, which is good. Now we can plug in the MCMC samples for the marginal log HR:\n\n\nShow the code\nsave_file &lt;- here(\"session-pts/jm1hr2.rds\")\nif (file.exists(save_file)) {\n    pts_freq_hr2_samples &lt;- readRDS(save_file)\n} else {\n    pts_freq_hr2_samples &lt;- sapply(\n        log_hr_samples,\n        pts_freq_hr2,\n        var_int = var_int,\n        var_final = var_final,\n        events_int = sum(os_data$os_event),\n        alloc_int = prop_pts_treatment,\n        events_final = total_size_final * avg_event_rate\n    )\n    saveRDS(pts_freq_hr2_samples, file = save_file)\n}\npts_freq_hr2_result &lt;- mean(pts_freq_hr2_samples)\npts_freq_hr2_result\n\n\n[1] 0.8415316\n\n\nSo here we get a PTS of 84.2%, which is higher than the one we got before.",
    "crumbs": [
      "Session 4: PTS",
      "1. Calculate Bayesian Predictive Power"
    ]
  },
  {
    "objectID": "session-pts/1_pts-jmpost.html#individual-samples-based-predictive-power",
    "href": "session-pts/1_pts-jmpost.html#individual-samples-based-predictive-power",
    "title": "1. Calculate Bayesian Predictive Power",
    "section": "Individual Samples based Predictive Power",
    "text": "Individual Samples based Predictive Power\nThis is a good motivation to try out the conditional sampling approach to calculate the predictive power, which is based on the individual samples of the joint model. It does not involved any frequentist assumptions to calculate the PTS.\nWe will perform the following steps in this algorithm:\n\nWe add as many new patients to the survival data set as we expect to still enter the trial.\n\nThese patients have survival time 0 and are censored at that time, which means they are completely new patients.\n\nWe refit the joint model with this augmented data set. This simplifies the subsequent sampling process.\nFor each patient who is still being followed up for OS at IA (=does not have an event yet and did not drop out)\n\nObtain individual survival distribution MCMC samples over a long enough time grid\nCondition on last observed censored OS time, sample once per MCMC sample from the conditional survival distribution\n\nDefine “End of Study” time, e.g. based on number of events observed, calendar time, etc.\nApply summary statistic of interest to the complete “End of Study” OS data set samples and aggregate appropriately\n\nFor example, we can use the log-rank test statistic as summary statistic\n\nCalculate the proportion of samples that are above the critical value for the summary statistic, e.g. log-rank test statistic\n\nNote that for the first step, we need to add some random information to the data set, because unfortunately the real data does not differentiate between patients who are still being followed up and those who have dropped out.\n\nAdd new patients and generate random lost-to-follow-up flag\nBasically for those patients with a censored survival time, we randomly assign a lost-to-follow-up flag with a probability of 10%, so that we can simulate some patients who are still being followed up and some who are not. We sample the covariates from the distribution of the already observed patients.\n\n\nShow the code\nset.seed(689)\nlost_to_follow_up_flag &lt;- sample(\n    c(TRUE, FALSE),\n    size = nrow(os_data),\n    replace = TRUE,\n    prob = c(0.1, 0.9)\n)\nos_data &lt;- os_data |&gt;\n    mutate(lost_to_follow_up = !os_event & lost_to_follow_up_flag)\nn_lost_to_follow_up &lt;- sum(os_data$lost_to_follow_up)\nn_lost_to_follow_up\n\n\n[1] 10\n\n\nShow the code\nset.seed(359)\nn_new_patients &lt;- 2 * avg_arm_size_followup\nos_data_new &lt;- data.frame(\n    id = seq(from = max(as.integer(as.character(os_data$id))) + 1, length.out = n_new_patients),\n    arm = rep(c(\"Docetaxel\", \"MPDL3280A\"), c(floor(avg_arm_size_followup), ceiling(avg_arm_size_followup))),\n    os_time = 0,\n    os_event = FALSE,\n    lost_to_follow_up = FALSE,\n    ecog = sample(os_data$ecog, n_new_patients, replace = TRUE),\n    age = sample(os_data$age, n_new_patients, replace = TRUE),\n    race = sample(os_data$race, n_new_patients, replace = TRUE),\n    sex = sample(os_data$sex, n_new_patients, replace = TRUE)\n)\nos_data_augmented &lt;- os_data[, names(os_data_new)] |&gt;\n    mutate(id = as.character(id)) |&gt;\n    rbind(os_data_new) |&gt;\n    mutate(id = factor(id))\n\nn_censored &lt;- sum(!os_data_augmented$os_event & !os_data_augmented$lost_to_follow_up)\nn_censored\n\n\n[1] 732\n\n\nSo now we have 10 patients who have been lost to follow-up and 732 patients who are still being followed up for OS (or who will still be entering the trial). These 732 patients will be the ones we will perform the individual sampling for.\n\n\nRefit the joint model with augmented data\nFirst we need to refit the joint model to this augmented data set. That is the easiest way to also get survival curve samples for the patients who are yet to be enrolled in the trial.\nWe notice that here we also need to add at least some random baseline samples to the longitudinal data set, otherwise it does not work.\n\n\nShow the code\nsubj_aug_df &lt;- os_data_augmented |&gt;\n    mutate(study = \"OAK\") |&gt;\n    select(study, id, arm)\nsubj_aug_data &lt;- DataSubject(\n    data = subj_aug_df,\n    subject = \"id\",\n    arm = \"arm\",\n    study = \"study\"\n)\nlong_df &lt;- tumor_data |&gt;\n    select(id, year, sld)\nnew_long_df &lt;- os_data_new |&gt;\n    select(id, os_time) |&gt;\n    rename(year = os_time) |&gt;\n    mutate(sld = sample(tumor_data$sld, n_new_patients, replace = TRUE))\nlong_aug_df &lt;- long_df |&gt;\n    mutate(id = as.character(id)) |&gt;\n    rbind(new_long_df) |&gt;\n    mutate(id = factor(id))\nstopifnot(identical(levels(long_aug_df$id), levels(subj_aug_df$id)))\n\nlong_aug_data &lt;- DataLongitudinal(\n    data = long_aug_df,\n    formula = sld ~ year\n)\nsurv_aug_data &lt;- DataSurvival(\n    data = os_data_augmented,\n    formula = Surv(os_time, os_event) ~ ecog + age + race + sex\n)\njoint_aug_data &lt;- DataJoint(\n    subject = subj_aug_data,\n    longitudinal = long_aug_data,\n    survival = surv_aug_data\n)\n\nsave_file &lt;- here(\"session-pts/jm2.rds\")\nif (file.exists(save_file)) {\n    joint_aug_results &lt;- readRDS(save_file)\n} else {\n    joint_aug_results &lt;- sampleStanModel(\n        joint_mod,\n        data = joint_aug_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(joint_aug_results, file = save_file)\n}\n\n\n\n\nIndividual Sampling of OS events\nNow we can perform the individual sampling of the OS events for the patients who are still being followed up. Let’s start with a single patient:\n\n\nShow the code\npatient_id &lt;- os_data_augmented |&gt;\n    filter(!os_event & !lost_to_follow_up) |&gt;\n    pull(id) |&gt;\n    unique() |&gt;\n    head(1)\n\n\nSo for this patient 588, we can extract the individual survival distribution MCMC samples from the joint model results along a time grid that extrapolates long enough into the future, say 10 years after the last observed OS time:\n\n\nShow the code\nlength_time_grid &lt;- 100\ntime_grid_end &lt;- round(max(os_data_augmented$os_time) + 10, 1)\ntime_grid &lt;- seq(0, time_grid_end, length = length_time_grid)\n\nsave_file &lt;- here(\"session-pts/jm2_surv_samples.rds\")\nif (file.exists(save_file)) {\n    os_surv_samples &lt;- readRDS(save_file)\n} else {\n    os_surv_samples &lt;- SurvivalQuantities(\n        object = joint_aug_results,\n        grid = GridFixed(times = time_grid),\n        type = \"surv\"\n    )\n    saveRDS(os_surv_samples, file = save_file)\n}\n\nos_surv_samples_df &lt;- as.data.frame(os_surv_samples)\nos_surv_samples_patient &lt;- os_surv_samples_df |&gt;\n    filter(group == patient_id) |&gt;\n    select(-group) |&gt;\n    mutate(sample_id = rep(1:ITER, length_time_grid))\nhead(os_surv_samples_patient)\n\n\nNow let’s zoom in on the first sampled survival curve of this patient, where we show also the last observed OS time as a vertical dashed line:\n\n\nShow the code\nfirst_surv_sample &lt;- os_surv_samples_patient |&gt;\n    filter(sample_id == 1)\npatient_last_os_time &lt;- os_data_augmented$os_time[os_data_augmented$id == patient_id]\nfirst_surv_sample_plot &lt;- ggplot(first_surv_sample, aes(x = time, y = values)) +\n    geom_line() +\n    labs(\n        title = paste(\"Individual Survival Curve for Patient\", patient_id),\n        x = \"Time (years)\",\n        y = \"Survival Probability\"\n    ) +\n    geom_vline(\n        xintercept = patient_last_os_time,\n        linetype = \"dashed\",\n        color = \"red\"\n    ) +\n    theme_minimal()\nfirst_surv_sample_plot\n\n\n\n\n\n\n\n\n\nAs described in the slides, we can now sample a survival time for this patient from the conditional survival distribution, given the last observed OS time, by:\n\nDrawing a standard uniform random number \\(p \\sim U(0, 1)\\)\nFinding the time \\(t\\) such that \\((1-p)S(c) - S(t) = 0\\) where $c = 2.1 is the last observed OS time\n\n\n\nShow the code\nset.seed(123)\np &lt;- runif(1)\n\n# Linear approximation function for the survival function:\nsurv_approx &lt;- approxfun(\n    x = first_surv_sample$time,\n    y = first_surv_sample$values,\n    rule = 2 # extrapolation outside interval via closest data extreme\n)\n\n# Survival function value at the time of censoring:\nsurv_at_censoring &lt;- surv_approx(patient_last_os_time)\n\n# Find the time t such that (1-p) * S(c) - S(t) = 0:\nt &lt;- uniroot(\n    function(t) (1 - p) * surv_at_censoring - surv_approx(t),\n    interval = c(0, time_grid_end)\n)$root\nt\n\n\n[1] 2.819322\n\n\nWe can plot the conditional survival function together with this sampled survival time:\n\n\nShow the code\nfirst_surv_sample &lt;- first_surv_sample |&gt;\n    mutate(\n        cond_values = ifelse(\n            time &lt;= patient_last_os_time,\n            1,\n            values / surv_at_censoring\n        )\n    )\n\nfirst_cond_surv_sample_plot &lt;- ggplot(\n    first_surv_sample,\n    aes(x = time, y = cond_values)\n) +\n    geom_line() +\n    labs(\n        title = paste(\n            \"Individual Conditional Survival Curve for Patient\",\n            patient_id\n        ),\n        x = \"Time (years)\",\n        y = \"Survival Probability\"\n    ) +\n    geom_vline(\n        xintercept = patient_last_os_time,\n        linetype = \"dashed\",\n        color = \"red\"\n    ) +\n    theme_minimal() +\n    geom_vline(\n        xintercept = t,\n        linetype = \"dotted\",\n        color = \"blue\"\n    ) +\n    geom_hline(\n        yintercept = 1 - p,\n        linetype = \"dotted\",\n        color = \"blue\"\n    ) +\n    geom_point(\n        data = data.frame(t = t, p = p),\n        aes(x = t, y = 1 - p),\n        color = \"blue\",\n        size = 5\n    )\nfirst_cond_surv_sample_plot\n\n\n\n\n\n\n\n\n\nThe challenge is now to do this efficiently for all patients who are still being followed up for OS, and for all of their MCMC samples.\n\n\nSimplified Rcpp implementation for single patient/sample\nFor now, let’s create a simpler Rcpp function that handles a single patient’s single MCMC sample:\n\n\nShow the code\nlibrary(Rcpp)\n\n# Use Rcpp function for single patient, single sample.\nsourceCpp(here(\"session-pts/conditional_sampling.cpp\"))\n\n# Test the function with a simple example.\nset.seed(3453)\n\nt_cpp_simple &lt;- sample_single_conditional_survival_time(\n    time_grid = first_surv_sample$time,\n    surv_values = first_surv_sample$values,\n    censoring_time = patient_last_os_time\n)\n\nfirst_cond_surv_sample_plot +\n    geom_point(\n        data = data.frame(\n            t = t_cpp_simple$t_result,\n            p = t_cpp_simple$uniform_sample\n        ),\n        aes(x = t, y = 1 - p),\n        color = \"blue\",\n        size = 5\n    )\n\n\n\n\n\n\n\n\n\n\n\nRcpp implementation for all patients and samples\nLet’s use the second Rcpp function that handles all patients and all MCMC samples at once. We just need to prepare the data in the right format:\n\nThe first argument is the time grid, which we already have.\nThe second argument is a matrix of survival values, where each row is a sample/patient combination and each column is a time point.\nThe third argument is a vector of censoring times, where each element corresponds to a sample/patient combination.\n\n\n\nShow the code\n# First subset to the patients where we need to sample from the conditional survival distribution.\npt_to_sample &lt;- os_data_augmented |&gt;\n    filter(!os_event & !lost_to_follow_up)\npt_to_sample_ids &lt;- pt_to_sample$id\n\nqs &lt;- os_surv_samples@quantities\ninclude_qs &lt;- qs@groups %in% pt_to_sample_ids\n\nqs_samples &lt;- qs@quantities[, include_qs]\nqs_times &lt;- qs@times[include_qs]\nqs_groups &lt;- qs@groups[include_qs]\n\nsurv_values_samples &lt;- matrix(\n    qs_samples,\n    nrow = ITER * length(pt_to_sample_ids),\n    ncol = length_time_grid\n)\ndim(surv_values_samples)\n\n\n[1] 732000    100\n\n\nShow the code\nsurv_values_pt_ids &lt;- rep(\n    head(qs_groups, length(pt_to_sample_ids)),\n    each = ITER\n)\n\ncensoring_times &lt;- pt_to_sample$os_time[match(\n    surv_values_pt_ids,\n    pt_to_sample_ids\n)]\nlength(censoring_times)\n\n\n[1] 732000\n\n\nShow the code\ncond_surv_time_samples &lt;- sample_conditional_survival_times(\n    time_grid = time_grid,\n    surv_values = surv_values_samples,\n    censoring_times = censoring_times\n)\nmean(cond_surv_time_samples$beyond_max_time)\n\n\n[1] 0.003687158\n\n\nShow the code\nsummary(cond_surv_time_samples$t_results)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n1.170e-05 9.700e-01 1.793e+00 2.221e+00 2.889e+00 1.230e+01 \n\n\nShow the code\nhist(cond_surv_time_samples$uniform_samples)\n\n\n\n\n\n\n\n\n\nShow the code\nos_cond_samples &lt;- matrix(\n    cond_surv_time_samples$t_results,\n    nrow = ITER,\n    ncol = length(pt_to_sample_ids),\n    dimnames = list(seq_len(ITER), head(qs_groups, length(pt_to_sample_ids)))\n)\n\n\nSo this function is very fast, and in this case we have less than 1% of the samples where we did not have a long enough time grid, which seems sufficient for our purposes.\n\n\nGenerating OS data set samples\nNow that we have the OS events for the patients who are still being followed up, we can generate the complete OS data set samples for each MCMC sample.\n\n\nShow the code\n# Where do we need to replace the OS times?\nos_rows_to_replace &lt;- match(\n    pt_to_sample_ids,\n    os_data_augmented$id\n)\n\n# From which column do we take the sampled OS times?\nos_cond_samples_cols &lt;- match(\n    pt_to_sample_ids,\n    colnames(os_cond_samples)\n)\n\nos_data_samples &lt;- lapply(1:ITER, function(i) {\n    # Create a copy of the original OS data\n    os_data_sample &lt;- os_data_augmented[, c(\"id\", \"arm\", \"os_time\", \"os_event\", \"race\", \"sex\", \"ecog\", \"age\")]\n\n    # Add the sampled OS times for the patients who are still being followed up.\n    os_data_sample$os_time[os_rows_to_replace] &lt;- os_cond_samples[\n        i,\n        os_cond_samples_cols\n    ]\n\n    # Set the OS event to TRUE for those patients, because we assume they have an event now.\n    os_data_sample$os_event[os_rows_to_replace] &lt;- TRUE\n\n    os_data_sample\n})\n\n\n\n\nCalculate log rank statistic for each sampled OS data set\nNow it is easy to calculate the log-rank test statistic for each sampled OS data set. Here we don’t adjust for any covariates.\n\n\nShow the code\nlog_rank_stats_fun &lt;- function(df) {\n    surv_diff &lt;- survival::survdiff(\n        Surv(os_time, os_event) ~ arm,\n        data = df\n    )\n    surv_diff$chisq\n}\n\nlog_rank_stats &lt;- sapply(os_data_samples, log_rank_stats_fun)\nhist(log_rank_stats)\n\nlog_rank_at_ia &lt;- log_rank_stats_fun(os_data_augmented)\nlog_rank_at_ia\n\n\n[1] 1.584927\n\n\nShow the code\nabline(v = log_rank_at_ia, col = \"red\", lwd = 2)\ncritical_value &lt;- qchisq(0.95, df = 1)\nabline(v = critical_value, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nAnd using the critical value of the log-rank test statistic for a significance level of 0.05, we can calculate the PTS:\n\n\nShow the code\npts_log_rank &lt;- mean(log_rank_stats &gt; critical_value)\npts_log_rank\n\n\n[1] 0.693\n\n\nSo we see that this PTS with 69.3% is a bit lower than the previous PTS estimates.\n\n\nHazard Ratio based PTS\nWe can also calculate the PTS in an analogous way using the hazard ratio estimates:\n\n\nShow the code\nhr_pval_fun &lt;- function(df) {\n    cox_mod &lt;- survival::coxph(\n        Surv(os_time, os_event) ~ arm + ecog + age + race + sex,\n        data = df\n    )\n    summary(cox_mod)$coefficients[1, 5]\n}\n\nhr_pvals &lt;- sapply(os_data_samples, hr_pval_fun)\n\npts_hr &lt;- mean(hr_pvals &lt; 0.05)\npts_hr\n\n\n[1] 0.828\n\n\nIn this PTS estimate we can adjust for the covariates. We see that the result of 82.8% is in line with the frequentist based PTS estimate for the HR which we calculated earlier.\n\n\nPlot Kaplan-Meier curves predictions\nSimilarly we can extract the Kaplan-Meier curves for each sampled OS data set and plot them:\n\n\nShow the code\nkm_curves &lt;- lapply(os_data_samples, function(df) {\n    survfit_obj &lt;- survival::survfit(Surv(os_time, os_event) ~ arm, data = df)\n    times &lt;- survfit_obj$time\n    surv_probs &lt;- survfit_obj$surv\n    strata &lt;- survfit_obj$strata\n    n_control &lt;- strata[1]\n    n_treatment &lt;- strata[2]\n\n    control_fun &lt;- stepfun(\n        times[1:n_control],\n        c(1, surv_probs[1:n_control]),\n        right = TRUE\n    )\n    treatment_fun &lt;- stepfun(\n        times[(n_control + 1):length(times)],\n        c(1, surv_probs[(n_control + 1):length(surv_probs)]),\n        right = TRUE\n    )\n    control_vals &lt;- control_fun(time_grid)\n    treatment_vals &lt;- treatment_fun(time_grid)\n\n    list(control = control_vals, treatment = treatment_vals)\n})\n\nkm_control_samples &lt;- sapply(km_curves, function(x) x$control)\nkm_treatment_samples &lt;- sapply(km_curves, function(x) x$treatment)\n\nsurvfit_ia &lt;- survival::survfit(Surv(os_time, os_event) ~ arm, data = os_data_augmented)\n\nplot(survfit_ia, col = c(1, 2), xlim = c(0, max(time_grid)))\n\n# Add the mean KM curves for each arm.\nlines(time_grid, rowMeans(km_control_samples), col = 1)\nlines(time_grid, rowMeans(km_treatment_samples), col = 2)\n\n# Add pointwise confidence intervals as shaded areas.\nkm_control_ci &lt;- apply(km_control_samples, 1, quantile, probs = c(0.025, 0.975))\nkm_treatment_ci &lt;- apply(\n    km_treatment_samples,\n    1,\n    quantile,\n    probs = c(0.025, 0.975)\n)\n\npolygon(\n    c(time_grid, rev(time_grid)),\n    c(km_control_ci[1, ], rev(km_control_ci[2, ])),\n    col = adjustcolor(1, alpha.f = 0.2),\n    border = NA\n)\npolygon(\n    c(time_grid, rev(time_grid)),\n    c(km_treatment_ci[1, ], rev(km_treatment_ci[2, ])),\n    col = adjustcolor(2, alpha.f = 0.2),\n    border = NA\n)\n\n\n\n\n\n\n\n\n\nWe can see that we assume a very long follow up of the data here. Obviously we could cut the data sets at a certain number of events or a specific time point easily.",
    "crumbs": [
      "Session 4: PTS",
      "1. Calculate Bayesian Predictive Power"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html",
    "href": "session-tgi/0_setup.html",
    "title": "0. Setup",
    "section": "",
    "text": "This is a repository with training material for Tumor Growth Inhibition (TGI) and joint TGI-OS (Overall Survival) modeling.\nHere is an overview of the required setup steps, which are described in more detail below:\n\nInstall RTools (if you are on Windows)\nInstall necessary R packages\nInstall cmdstanr (optional but highly recommended)\nClone the repository from GitHub (https://github.com/RCONIS/tgi-os-training)\nOpen the folder in RStudio or VSCode\n\n\n\nIf you are running the examples on a Windows PC, you will need to install RTools, see\nhere.\nYou need to use the version of RTools that matches your R version. You can check your R version by running R.Version()$version.string.\nYou can then afterwards check the installation of RTools with:\nif(!require(pkgbuild)) install.packages(\"pkgbuild\")\npkgbuild::has_build_tools()\n\n\n\nThe following code will install the packages that are required to run the examples in this repository.\ninstall_if_not_available &lt;- function(pkg, min_version = NULL) {\n    is_installed &lt;- suppressWarnings(suppressPackageStartupMessages(\n      require(pkg, character.only = TRUE)\n    ))\n    if (is_installed & !is.null(min_version)) {\n      version_ok &lt;- packageVersion(pkg) &gt;= min_version\n    }    \n    if (!is_installed | !version_ok) {\n        install.packages(pkg)\n    }\n  }\npackages &lt;- c(\n  \"bayesplot\", \n  \"brms\", \n  \"ggplot2\",\n  \"gt\",\n  \"here\", \n  \"janitor\",\n  \"modelr\",\n  \"posterior\",\n  \"readxl\",\n  \"rstan\",\n  \"tidybayes\", \n  \"tidyverse\",   \n  \"truncnorm\",\n  \"sn\",\n  \"fuzzyjoin\",\n  \"glue\"\n)\nsapply(packages, install_if_not_available)\nremotes::install_github(\"genentech/jmpost\")\n\n\n\nOptionally, you can use cmdstanr as the backend of brms for fitting the models.\nThere are a few advantages of using cmdstanr over the default rstan:\n\nWith cmdstanr, you can cache the compiled model, therefore you don’t need to recompile the model as long as the same formula is used, even after restarting R session (or re-opening RStudio).\ncmdstanr is more up-to-date and more actively maintained compared to rstan.\n\nA detailed installation guide for cmdstanr is available here. Here is a brief summary:\n\nInstall cmdstanr with:\n# Typically, you install cmdstanr from the R-universe as follows:\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\n\n# If the above does not work, e.g. because you in a company network or VPN, you can install cmdstanr from GitHub instead:\nremotes::install_github(\"stan-dev/cmdstanr@*release\")    \nCheck the compiler toolchain with:\ncmdstanr::check_cmdstan_toolchain()\n\nThis should not be a problem on Mac and Linux (including RStudio Cloud instances), but might be a problem on Windows.\nIf you have problems, please check the installation guide for Windows.\n\nRTools is the easiest way.\nEven if you have RTools, you might still see an error like \"Rtools44 installation found but the toolchain was not installed.\".\nIn this case, you can run cmdstanr::check_cmdstan_toolchain(fix = TRUE) and this will likely resolve the issue.\n\n\nInstall the CmdStan backend with:\ncmdstanr::install_cmdstan(cores = 2)\nTesting the cmdstanr installation\n\nIf the above installation was successful, you should now be able to run the following simple model.\n\nlibrary(cmdstanr)\nfile &lt;- file.path(\n  cmdstan_path(), \n  \"examples\", \"bernoulli\", \"bernoulli.stan\"\n)\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500\n)",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#install-rtools",
    "href": "session-tgi/0_setup.html#install-rtools",
    "title": "0. Setup",
    "section": "",
    "text": "If you are running the examples on a Windows PC, you will need to install RTools, see\nhere.\nYou need to use the version of RTools that matches your R version. You can check your R version by running R.Version()$version.string.\nYou can then afterwards check the installation of RTools with:\nif(!require(pkgbuild)) install.packages(\"pkgbuild\")\npkgbuild::has_build_tools()",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#install-necessary-r-packages",
    "href": "session-tgi/0_setup.html#install-necessary-r-packages",
    "title": "0. Setup",
    "section": "",
    "text": "The following code will install the packages that are required to run the examples in this repository.\ninstall_if_not_available &lt;- function(pkg, min_version = NULL) {\n    is_installed &lt;- suppressWarnings(suppressPackageStartupMessages(\n      require(pkg, character.only = TRUE)\n    ))\n    if (is_installed & !is.null(min_version)) {\n      version_ok &lt;- packageVersion(pkg) &gt;= min_version\n    }    \n    if (!is_installed | !version_ok) {\n        install.packages(pkg)\n    }\n  }\npackages &lt;- c(\n  \"bayesplot\", \n  \"brms\", \n  \"ggplot2\",\n  \"gt\",\n  \"here\", \n  \"janitor\",\n  \"modelr\",\n  \"posterior\",\n  \"readxl\",\n  \"rstan\",\n  \"tidybayes\", \n  \"tidyverse\",   \n  \"truncnorm\",\n  \"sn\",\n  \"fuzzyjoin\",\n  \"glue\"\n)\nsapply(packages, install_if_not_available)\nremotes::install_github(\"genentech/jmpost\")",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#optional-install-cmdstanr",
    "href": "session-tgi/0_setup.html#optional-install-cmdstanr",
    "title": "0. Setup",
    "section": "",
    "text": "Optionally, you can use cmdstanr as the backend of brms for fitting the models.\nThere are a few advantages of using cmdstanr over the default rstan:\n\nWith cmdstanr, you can cache the compiled model, therefore you don’t need to recompile the model as long as the same formula is used, even after restarting R session (or re-opening RStudio).\ncmdstanr is more up-to-date and more actively maintained compared to rstan.\n\nA detailed installation guide for cmdstanr is available here. Here is a brief summary:\n\nInstall cmdstanr with:\n# Typically, you install cmdstanr from the R-universe as follows:\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\n\n# If the above does not work, e.g. because you in a company network or VPN, you can install cmdstanr from GitHub instead:\nremotes::install_github(\"stan-dev/cmdstanr@*release\")    \nCheck the compiler toolchain with:\ncmdstanr::check_cmdstan_toolchain()\n\nThis should not be a problem on Mac and Linux (including RStudio Cloud instances), but might be a problem on Windows.\nIf you have problems, please check the installation guide for Windows.\n\nRTools is the easiest way.\nEven if you have RTools, you might still see an error like \"Rtools44 installation found but the toolchain was not installed.\".\nIn this case, you can run cmdstanr::check_cmdstan_toolchain(fix = TRUE) and this will likely resolve the issue.\n\n\nInstall the CmdStan backend with:\ncmdstanr::install_cmdstan(cores = 2)\nTesting the cmdstanr installation\n\nIf the above installation was successful, you should now be able to run the following simple model.\n\nlibrary(cmdstanr)\nfile &lt;- file.path(\n  cmdstan_path(), \n  \"examples\", \"bernoulli\", \"bernoulli.stan\"\n)\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500\n)",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html",
    "href": "session-tgi/3_tgi_gsf_brms.html",
    "title": "3. Generalized Stein-Fojo model",
    "section": "",
    "text": "This appendix shows how the generalized Stein-Fojo model can be implemented in a Bayesian framework using the brms package in R.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#setup-and-load-data",
    "href": "session-tgi/3_tgi_gsf_brms.html#setup-and-load-data",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#generalized-stein-fojo-model",
    "href": "session-tgi/3_tgi_gsf_brms.html#generalized-stein-fojo-model",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Generalized Stein-Fojo model",
    "text": "Generalized Stein-Fojo model\nHere we have an additional parameter \\(\\phi\\), which is the weight for the shrinkage in the double exponential model. The model is then:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\{\n  \\psi_{\\phi i} \\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) +\n  (1 - \\psi_{\\phi i}) \\exp(\\psi_{k_{g}i} \\cdot t_{ij})\n\\}\n\\]\nfor positive times \\(t_{ij}\\). Again, if the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. That is, we have then \\(\\phi_i = 0\\). Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\{\n  \\psi_{\\phi i} \\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) +\n  (1 - \\psi_{\\phi i}) \\exp(\\psi_{k_{g}i} \\cdot t_{ij})\n\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\nIn terms of likelihood and priors, we can use the same assumptions as in the previous model. The only difference is that we have to model the \\(\\phi\\) parameter. We can use a logit-normal distribution for this parameter. This is a normal distribution on the logit scale, which is then transformed to the unit interval. \\[\n\\psi_{\\phi i} \\sim \\text{LogitNormal}(\\text{logit}(0.5) = 0, 0.5)\n\\]",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#fit-model",
    "href": "session-tgi/3_tgi_gsf_brms.html#fit-model",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (phi * exp(-ks * year) + (1 - phi) * exp(kg * year))) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # As before:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(phi ~ inv_logit(tphi)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(tphi ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + (1 | id)) +\n  lf(lkg ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\"),\n  prior(normal(log(1.04), 1), nlpar = \"lkg\"),\n  prior(normal(0, 0.5), nlpar = \"tphi\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(student_t(3, 0, 22.2), lb = 0, nlpar = \"tphi\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lks = array(-1.25),\n  b_lkg = array(-1.33),\n  b_tphi = array(0),\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  sd_4 = array(0.1),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients),\n  z_4 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/gsf1.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = WARMUP + ITER, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsummary(fit)\n\n\nWarning: There were 41 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (phi * exp(-ks * year) + (1 - phi) * exp(kg * year))) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         phi ~ inv_logit(tphi)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         tphi ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)      0.58      0.02     0.55     0.61 1.01      382      606\nsd(tphi_Intercept)     2.12      0.18     1.78     2.49 1.01      611     1216\nsd(lks_Intercept)      2.16      0.14     1.90     2.46 1.00      520     1282\nsd(lkg_Intercept)      1.18      0.09     1.01     1.36 1.00      899     1766\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept      0.15      0.00     0.14     0.15 1.00     1611     2351\nlb0_Intercept      3.63      0.02     3.59     3.68 1.03      221      547\ntphi_Intercept    -0.14      0.22    -0.57     0.27 1.00      424      839\nlks_Intercept     -0.62      0.10    -0.81    -0.43 1.00     1170     1868\nlkg_Intercept     -1.15      0.14    -1.43    -0.89 1.00      699     1488\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn total this took 76 minutes on my laptop.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#parameter-estimates",
    "href": "session-tgi/3_tgi_gsf_brms.html#parameter-estimates",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\n\nShow the code\npost_df &lt;- as_draws_df(fit)\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_tphi_Intercept\"      \n [4] \"b_lks_Intercept\"        \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"  \n [7] \"sd_id__tphi_Intercept\"  \"sd_id__lks_Intercept\"   \"sd_id__lkg_Intercept\"  \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_phi = plogis(b_tphi_Intercept),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    omega_phi = sd_id__tphi_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nLet’s first look at the population level parameters:\n\n\nShow the code\ngsf_pop_params &lt;- c(\"theta_b0\", \"theta_ks\", \"theta_kg\", \"theta_phi\", \"sigma\")\n\nmcmc_trace(post_df, pars = gsf_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = gsf_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nThe trace plots look good. The chains seem to have converged and the pairs plot shows no strong correlations between the parameters. Let’s check the table:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  select(theta_b0, theta_ks, theta_kg, theta_phi, omega_0, omega_s, omega_g, omega_phi, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.766\n44.746\n1.057\n1.073\n43.074\n46.528\n1.007\n179.361\n378.906\n\n\ntheta_ks\n5.857\n5.444\n1.924\n1.582\n3.589\n9.560\n1.003\n384.622\n905.388\n\n\ntheta_kg\n0.640\n0.637\n0.061\n0.062\n0.541\n0.745\n1.000\n685.379\n1,885.264\n\n\ntheta_phi\n0.467\n0.466\n0.055\n0.058\n0.376\n0.555\n1.003\n419.761\n831.991\n\n\nomega_0\n0.578\n0.578\n0.016\n0.017\n0.553\n0.606\n1.001\n376.483\n628.807\n\n\nomega_s\n2.159\n2.155\n0.145\n0.145\n1.929\n2.412\n1.002\n521.866\n1,265.639\n\n\nomega_g\n1.177\n1.174\n0.090\n0.090\n1.037\n1.333\n1.002\n868.075\n1,746.640\n\n\nomega_phi\n2.117\n2.108\n0.182\n0.184\n1.833\n2.423\n1.003\n603.806\n1,215.066\n\n\nsigma\n0.147\n0.147\n0.002\n0.002\n0.143\n0.150\n1.000\n1,575.916\n2,168.010\n\n\n\n\n\n\n\nSo \\(\\theta_{\\phi}\\) is estimated around 0.5. The other parameters are similar to the previous Stein-Fojo model, but we see a larger \\(\\theta_{k_s}\\) e.g.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#observation-vs-model-fit",
    "href": "session-tgi/3_tgi_gsf_brms.html#observation-vs-model-fit",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_epred_draws(fit) |&gt;\n  median_qi()\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = .epred, ymin = .lower, ymax = .upper), \n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = .epred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"GSF model fit\")\n\n\n\n\n\n\n\n\n\nThis also looks good. The model seems to capture the data well.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#with-jmpost",
    "href": "session-tgi/3_tgi_gsf_brms.html#with-jmpost",
    "title": "3. Generalized Stein-Fojo model",
    "section": "With jmpost",
    "text": "With jmpost\nThis model can also be fit with the jmpost package. The corresponding function is LongitudinalGSF. The statistical model is specified in the vignette here.\nHomework: Implement the generalized Stein-Fojo model with jmpost and compare the results with the brms implementation.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html",
    "href": "session-bhm/1_bhm-jmpost.html",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "",
    "text": "The purpose of this document is to show a minimal workflow for Bayesian hierarchical joint modeling for historical data borrowing using the jmpost package.",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html#setup-and-load-data",
    "href": "session-bhm/1_bhm-jmpost.html#setup-and-load-data",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "Setup and load data",
    "text": "Setup and load data\nHere we execute the R code from the setup and data preparation chapter, see the full code here.",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html#descriptive-analysis-of-the-current-study-data",
    "href": "session-bhm/1_bhm-jmpost.html#descriptive-analysis-of-the-current-study-data",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "Descriptive analysis of the current study data",
    "text": "Descriptive analysis of the current study data\nLet’s have a quick look at the (artificially reduced to maximum 1 year follow-up) current study data, in order to illustrate the typical situation where we have immature OS data:\n\n\nShow the code\nlibrary(survival)\n\n\n\nAttaching package: 'survival'\n\n\nThe following object is masked from 'package:brms':\n\n    kidney\n\n\nShow the code\nlibrary(survminer)\n\n\nLoading required package: ggpubr\n\n\n\nAttaching package: 'survminer'\n\n\nThe following object is masked from 'package:survival':\n\n    myeloma\n\n\nShow the code\n# Overall Survival KM plot\nfit_os_current &lt;- survfit(Surv(os_time, os_event) ~ arm, data = current_os_data)\n\nggsurvplot(\n    fit_os_current,\n    data = current_os_data,\n    risk.table = TRUE,\n    pval = TRUE,\n    conf.int = TRUE,\n    xlab = \"Time (years)\",\n    title = \"Overall Survival in Current Study\"\n)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\nIgnoring unknown labels:\n• colour : \"Strata\"\n\n\n\n\n\n\n\n\n\nSo no matter which model we might fit to this data, we will not be able to trust it to estimate the hazard ratio reliably.",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html#check-compatibility-of-historical-and-current-data",
    "href": "session-bhm/1_bhm-jmpost.html#check-compatibility-of-historical-and-current-data",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "Check compatibility of historical and current data",
    "text": "Check compatibility of historical and current data\nJust to mention that we would need to double check the compatibility of historical and current clinical trial design (including patient populations, endpoints, assessment schedules, etc.) and data distributions before applying any kind of historical data borrowing technique.\nIn our dummy example here, this is fulfilled by construction.",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html#fitting-the-joint-model-only-to-the-historical-data",
    "href": "session-bhm/1_bhm-jmpost.html#fitting-the-joint-model-only-to-the-historical-data",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "Fitting the joint model only to the historical data",
    "text": "Fitting the joint model only to the historical data\nThe first step is to fit the joint model only to the historical data. In reality, this also includes model selection in order to determine:\n\nchoice of parametric baseline survival model (log-logistic, Weibull, etc.?)\nselection of baseline covariates\nchoice of the TGI metric to correlate with the survival model, i.e. the link function (SLD, tumor growth rate, time to tumor regrowth, etc.?)\n\nWe have discussed model comparison and selection in session 3 here, so we will skip this step here and directly fit a joint model to the historical data only.\n\n\nShow the code\nhistorical_subj_df &lt;- historical_os_data |&gt;\n    select(study, id, arm)\nhistorical_subj_data &lt;- DataSubject(\n    data = historical_subj_df,\n    subject = \"id\",\n    arm = \"arm\",\n    study = \"study\"\n)\n\nhistorical_long_df &lt;- historical_tumor_data |&gt;\n    select(id, year, sld)\nhistorical_long_data &lt;- DataLongitudinal(\n    data = historical_long_df,\n    formula = sld ~ year\n)\n\nhistorical_surv_data &lt;- DataSurvival(\n    data = historical_os_data,\n    formula = Surv(os_time, os_event) ~ ecog + age + race + sex\n)\n\nhistorical_joint_data &lt;- DataJoint(\n    subject = historical_subj_data,\n    longitudinal = historical_long_data,\n    survival = historical_surv_data\n)\n\nhistorical_joint_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    ),\n    survival = SurvivalWeibullPH(\n        lambda = prior_gamma(0.7, 1),\n        gamma = prior_gamma(1.5, 1),\n        beta = prior_normal(0, 20)\n    ),\n    link = linkGrowth(\n        prior = prior_normal(0, 20)\n    )\n)\n\noptions(\"jmpost.prior_shrinkage\" = 0.999)\n\nsave_file &lt;- here(\"session-bhm/histmod1.rds\")\nif (file.exists(save_file)) {\n    historical_joint_results &lt;- readRDS(save_file)\n} else {\n    historical_joint_results &lt;- sampleStanModel(\n        historical_joint_mod,\n        data = historical_joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(historical_joint_results, file = save_file)\n}\n\n\nLet’s have a look at the results:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\",\n    \"beta_os_cov\",\n    \"sm_weibull_ph_gamma\",\n    \"sm_weibull_ph_lambda\",\n    \"link_growth\"\n)\n\nmcmc_historical_joint_results &lt;- cmdstanr::as.CmdStanMCMC(historical_joint_results)\n\nmcmc_historical_summary &lt;- mcmc_historical_joint_results$summary(vars)\n\nprint(mcmc_historical_summary, n = 30)\n\n\n# A tibble: 19 × 10\n   variable         mean   median      sd     mad      q5     q95  rhat ess_bulk\n   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu_bs…  3.75     3.76    0.0519  0.0510   3.67    3.84   1.01      208.\n 2 lm_sf_mu_ks…  0.0560   0.0818  0.257   0.246   -0.399   0.440  1.00      775.\n 3 lm_sf_mu_ks… -0.913   -0.865   0.388   0.385   -1.60   -0.353  1.00      683.\n 4 lm_sf_mu_kg… -0.496   -0.491   0.126   0.120   -0.699  -0.290  0.999     732.\n 5 lm_sf_mu_kg… -0.782   -0.773   0.198   0.189   -1.12   -0.473  1.00      614.\n 6 lm_sf_sigma   0.121    0.120   0.00479 0.00459  0.113   0.129  1.00      951.\n 7 lm_sf_omega…  0.493    0.490   0.0359  0.0340   0.438   0.557  1.00      590.\n 8 lm_sf_omega…  0.913    0.891   0.195   0.181    0.640   1.27   1.00      850.\n 9 lm_sf_omega…  1.26     1.21    0.302   0.281    0.837   1.80   1.00      649.\n10 lm_sf_omega…  0.458    0.453   0.0903  0.0907   0.324   0.607  1.00      775.\n11 lm_sf_omega…  0.990    0.975   0.133   0.131    0.796   1.24   1.00      766.\n12 beta_os_cov…  1.17     1.17    0.380   0.364    0.518   1.79   1.00      895.\n13 beta_os_cov…  0.00931  0.00952 0.0153  0.0155  -0.0150  0.0342 1.000     987.\n14 beta_os_cov…  0.942    0.940   0.653   0.628   -0.153   2.00   1.00     1015.\n15 beta_os_cov…  0.165    0.157   0.374   0.379   -0.440   0.778  1.00     1051.\n16 beta_os_cov…  0.246    0.239   0.331   0.325   -0.272   0.789  1.00     1049.\n17 sm_weibull_…  1.70     1.70    0.212   0.203    1.35    2.06   1.00      980.\n18 sm_weibull_…  0.151    0.0983  0.163   0.0881   0.0162  0.460  0.998    1001.\n19 link_growth   0.988    0.985   0.375   0.370    0.377   1.59   1.00      796.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nLet’s also look at the covariate effect estimates again:\n\n\nShow the code\nos_cov_name_mapping &lt;- function(surv_data) {\n    surv_data_design &lt;- as_stan_list(surv_data)$os_cov_design\n    os_cov_names &lt;- colnames(surv_data_design)\n    old_coef_names &lt;- as.character(glue::glue(\"beta_os_cov[{seq_along(os_cov_names)}]\"))\n    setNames(old_coef_names, os_cov_names)\n}\nos_cov_renaming &lt;- os_cov_name_mapping(historical_surv_data)\n\ndraws_historical_joint_results &lt;- mcmc_historical_joint_results$draws(vars)\ndraws_historical_joint_results &lt;- do.call(\n    rename_variables,\n    c(list(draws_historical_joint_results), os_cov_renaming)\n)\nmcmc_dens_overlay(draws_historical_joint_results) +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\")",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html#pool-historical-and-current-data",
    "href": "session-bhm/1_bhm-jmpost.html#pool-historical-and-current-data",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "Pool historical and current data",
    "text": "Pool historical and current data\nNow we pool the historical and the current data for the hierarchical joint modeling step:\n\n\nShow the code\npooled_os_data &lt;- bind_rows(\n    historical_os_data,\n    current_os_data\n)\npooled_tumor_data &lt;- bind_rows(\n    historical_tumor_data,\n    current_tumor_data\n)\npooled_subj_df &lt;- pooled_os_data |&gt;\n    select(study, id, arm)\n\npooled_subj_data &lt;- DataSubject(\n    data = pooled_subj_df,\n    subject = \"id\",\n    arm = \"arm\",\n    study = \"study\"\n)\n\npooled_long_df &lt;- pooled_tumor_data |&gt;\n    select(id, year, sld)\npooled_long_data &lt;- DataLongitudinal(\n    data = pooled_long_df,\n    formula = sld ~ year\n)\n\npooled_surv_data &lt;- DataSurvival(\n    data = pooled_os_data,\n    formula = Surv(os_time, os_event) ~ ecog + age + race + sex\n)\npooled_joint_data &lt;- DataJoint(\n    subject = pooled_subj_data,\n    longitudinal = pooled_long_data,\n    survival = pooled_surv_data\n)\n\n\nSo now we have pooled our 101 historical and 102 current patients into a single data set with 203 patients.",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html#fit-the-bayesian-hierarchical-joint-model",
    "href": "session-bhm/1_bhm-jmpost.html#fit-the-bayesian-hierarchical-joint-model",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "Fit the Bayesian hierarchical joint model",
    "text": "Fit the Bayesian hierarchical joint model\nNow we can fit the Bayesian hierarchical joint model to the pooled data set, borrowing information from the historical data to inform the current data analysis.\nNote that we use the same model specification historical_joint_mod as for the historical data only model fit above. The reason is that the LongitudinalSteinFojo class already accounts for hierarchical modeling, see the statistical specification here.\n\n\nShow the code\nsave_file &lt;- here(\"session-bhm/pooledmod1.rds\")\nif (file.exists(save_file)) {\n    pooled_joint_results &lt;- readRDS(save_file)\n} else {\n    pooled_joint_results &lt;- sampleStanModel(\n        historical_joint_mod,\n        data = pooled_joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(pooled_joint_results, file = save_file)\n}\n\n\nLet’s first check again if the MCMC sampling went fine:\n\n\nShow the code\nmcmc_pooled_joint_results &lt;- cmdstanr::as.CmdStanMCMC(pooled_joint_results)\n\nmcmc_pooled_summary &lt;- mcmc_pooled_joint_results$summary(vars)\n\nprint(mcmc_pooled_summary, n = 30)\n\n\n# A tibble: 21 × 10\n   variable         mean   median      sd     mad      q5     q95  rhat ess_bulk\n   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu_bs…  3.75     3.75    0.0578  0.0577   3.65    3.84   1.02      247.\n 2 lm_sf_mu_bs…  3.76     3.76    0.0490  0.0479   3.67    3.84   1.02      140.\n 3 lm_sf_mu_ks… -0.0438  -0.0213  0.228   0.215   -0.454   0.301  1.01      582.\n 4 lm_sf_mu_ks… -1.04    -1.01    0.316   0.305   -1.58   -0.581  1.00      618.\n 5 lm_sf_mu_kg… -0.587   -0.581   0.129   0.133   -0.807  -0.386  1.01      510.\n 6 lm_sf_mu_kg… -0.873   -0.875   0.141   0.144   -1.10   -0.646  1.00      519.\n 7 lm_sf_sigma   0.129    0.129   0.00366 0.00361  0.123   0.136  1.00      719.\n 8 lm_sf_omega…  0.573    0.569   0.0408  0.0416   0.514   0.644  1.00      393.\n 9 lm_sf_omega…  0.492    0.487   0.0370  0.0360   0.436   0.555  1.00      386.\n10 lm_sf_omega…  0.956    0.945   0.165   0.163    0.720   1.24   1.01      530.\n11 lm_sf_omega…  1.35     1.32    0.248   0.227    1.02    1.79   1.01      512.\n12 lm_sf_omega…  0.682    0.675   0.0831  0.0826   0.557   0.827  1.00      884.\n13 lm_sf_omega…  0.961    0.950   0.101   0.0995   0.810   1.14   1.00      595.\n14 beta_os_cov…  1.16     1.15    0.285   0.285    0.687   1.62   1.00      934.\n15 beta_os_cov…  0.00105  0.00131 0.0118  0.0114  -0.0190  0.0202 0.999     965.\n16 beta_os_cov…  1.16     1.16    0.508   0.489    0.338   1.97   1.00     1003.\n17 beta_os_cov…  0.171    0.167   0.299   0.301   -0.303   0.666  1.00      897.\n18 beta_os_cov…  0.263    0.262   0.262   0.268   -0.159   0.708  1.00      834.\n19 sm_weibull_…  1.92     1.91    0.201   0.195    1.60    2.25   0.998     930.\n20 sm_weibull_…  0.255    0.191   0.212   0.137    0.0551  0.699  1.00     1002.\n21 link_growth   1.18     1.18    0.281   0.276    0.708   1.64   0.999     816.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nIndeed all rhat values are close to 1 indicating convergence.",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html#investigating-the-parameter-estimates",
    "href": "session-bhm/1_bhm-jmpost.html#investigating-the-parameter-estimates",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "Investigating the parameter estimates",
    "text": "Investigating the parameter estimates\nWe can see that now we have more model parameters, let’s see which are additional compared to the historical data only model:\n\n\nShow the code\npooled_param_names &lt;- mcmc_pooled_summary$variable\nhistorical_param_names &lt;- mcmc_historical_summary$variable\nnew_params &lt;- setdiff(pooled_param_names, historical_param_names)\nnew_params\n\n\n[1] \"lm_sf_mu_bsld[2]\"    \"lm_sf_omega_bsld[2]\"\n\n\nWe see that the additional parameters correspond to the baseline value, which is supposed to differ between studies: both the mean mu parameter and the standard deviation omega parameter gain an additional second dimension here with the addition of the second study.\nLet’s look at the estimates of these new parameters and compare them between the studies:\n\n\nShow the code\nbaseline_vars &lt;- c(\"lm_sf_mu_bsld\", \"lm_sf_omega_bsld\")\nbaseline_pooled_joint_results &lt;- mcmc_pooled_joint_results$draws(baseline_vars)\n\nmcmc_dens_overlay(baseline_pooled_joint_results)\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(posterior)\n\n# Convert draws to rvars for easier manipulation\nbaseline_rvars &lt;- as_draws_rvars(baseline_pooled_joint_results)\n\nmu_diff &lt;- baseline_rvars$lm_sf_mu_bsld[1] - baseline_rvars$lm_sf_mu_bsld[2]\nsummary(mu_diff)\n\n\n# A tibble: 1 × 10\n  variable     mean   median     sd    mad     q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 mu_diff  -0.00927 -0.00803 0.0742 0.0752 -0.134 0.112  1.00     228.     356.\n\n\nShow the code\nomega_diff &lt;- baseline_rvars$lm_sf_omega_bsld[1] - baseline_rvars$lm_sf_omega_bsld[2]\nsummary(omega_diff)\n\n\n# A tibble: 1 × 10\n  variable     mean median     sd    mad       q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 omega_diff 0.0812 0.0803 0.0553 0.0548 -0.00772 0.172  1.00     351.     601.\n\n\nAs expected we don’t see a big difference in the baseline SLD between the historical and current study, neither in the mean nor in the standard deviation.",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html#goodness-of-fit",
    "href": "session-bhm/1_bhm-jmpost.html#goodness-of-fit",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nLet’s check the goodness of fit to our current study data:\n\n\nShow the code\ncurrent_subj_df &lt;- current_os_data |&gt;\n    select(study, id, arm)\n\ntime_grid &lt;- seq(from = 0, to = max(pooled_os_data$os_time), length = 100)\ncurrent_os_surv_group_grid &lt;- GridGrouped(\n    times = time_grid,\n    groups = with(\n        current_subj_df,\n        split(as.character(id), arm)\n    )\n)\ncurrent_os_surv_pred &lt;- SurvivalQuantities(\n    object = pooled_joint_results,\n    grid = current_os_surv_group_grid,\n    type = \"surv\"\n)\n\nautoplot(current_os_surv_pred, add_km = TRUE, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nSo the fit looks satisfactory, considering that the current data are quite immature.",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html#posterior-distribution-of-the-hazard-ratio",
    "href": "session-bhm/1_bhm-jmpost.html#posterior-distribution-of-the-hazard-ratio",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "Posterior distribution of the hazard ratio",
    "text": "Posterior distribution of the hazard ratio\nNow let’s look at the posterior distribution of the hazard ratio between the two treatment arms in the current study. We do this as follows:\n\nFor a given MCMC sample \\(i\\):\n\nSample predicted survival times for the current study subjects which are still being followed up\nFit a Cox proportional hazards regression model\nExtract the hazard ratio estimate\n\nRepeat for all MCMC samples to get the posterior distribution of the hazard ratio\n\nThis is the same algorithm we used in the session 4 here.\nIn this example we assume for simplicity:\n\nno additional patients will be enrolled, i.e. we don’t need to add new patients to the data set\nall currently censored patients will be followed up until event occurs\n\nTherefore we can directly jump into the sampling step as described here:\n\n\nShow the code\n# Determine for which patients we want to sample the individual survival times.\npt_to_sample &lt;- pooled_os_data |&gt;\n    filter(study == \"current\" & os_event == 0)\npt_to_sample_ids &lt;- pt_to_sample$id\n\n# Obtain the survival function samples (for all patients here).\nlength_time_grid &lt;- 100\ntime_grid_end &lt;- round(max(pooled_os_data$os_time) + 10, 1)\ntime_grid &lt;- seq(0, time_grid_end, length = length_time_grid)\n\nsave_file &lt;- here(\"session-bhm/bhm1_surv_samples.rds\")\nif (file.exists(save_file)) {\n    os_surv_samples &lt;- readRDS(save_file)\n} else {\n    os_surv_samples &lt;- SurvivalQuantities(\n        object = pooled_joint_results,\n        grid = GridFixed(times = time_grid),\n        type = \"surv\"\n    )\n    saveRDS(os_surv_samples, file = save_file)\n}\n\nqs &lt;- os_surv_samples@quantities\ninclude_qs &lt;- qs@groups %in% pt_to_sample_ids\n\nqs_samples &lt;- qs@quantities[, include_qs]\nqs_times &lt;- qs@times[include_qs]\nqs_groups &lt;- qs@groups[include_qs]\n\nsurv_values_samples &lt;- matrix(\n    qs_samples,\n    nrow = ITER * length(pt_to_sample_ids),\n    ncol = length_time_grid\n)\n\nsurv_values_pt_ids &lt;- rep(\n    head(qs_groups, length(pt_to_sample_ids)),\n    each = ITER\n)\n\ncensoring_times &lt;- pt_to_sample$os_time[match(\n    surv_values_pt_ids,\n    pt_to_sample_ids\n)]\n\nlibrary(Rcpp)\n\n# Use Rcpp function for single patient, single sample.\nsourceCpp(here(\"session-pts/conditional_sampling.cpp\"))\n\ncond_surv_time_samples &lt;- sample_conditional_survival_times(\n    time_grid = time_grid,\n    surv_values = surv_values_samples,\n    censoring_times = censoring_times\n)\n\nos_cond_samples &lt;- matrix(\n    cond_surv_time_samples$t_results,\n    nrow = ITER,\n    ncol = length(pt_to_sample_ids),\n    dimnames = list(seq_len(ITER), head(qs_groups, length(pt_to_sample_ids)))\n)\n\n\nThe next step is again to create the complete OS data sets. Here we just use the current data set and replace the censored times with the sampled times above:\n\n\nShow the code\nos_rows_to_replace &lt;- match(\n    pt_to_sample_ids,\n    current_os_data$id\n)\n\nos_cond_samples_cols &lt;- match(\n    pt_to_sample_ids,\n    colnames(os_cond_samples)\n)\n\nos_data_samples &lt;- lapply(1:ITER, function(i) {\n    # Create a copy of the original OS data\n    os_data_sample &lt;- current_os_data[, c(\"id\", \"arm\", \"os_time\", \"os_event\", \"race\", \"sex\", \"ecog\", \"age\")]\n\n    # Add the sampled OS times for the patients who are still being followed up.\n    os_data_sample$os_time[os_rows_to_replace] &lt;- os_cond_samples[\n        i,\n        os_cond_samples_cols\n    ]\n\n    # Set the OS event to TRUE for those patients, because we assume they have an event now.\n    os_data_sample$os_event[os_rows_to_replace] &lt;- TRUE\n\n    os_data_sample\n})\n\n\nNow we can fit a Cox proportional hazards regression model to each of the completed data sets and extract the hazard ratio estimates:\n\n\nShow the code\nhr_est_fun &lt;- function(df) {\n    cox_mod &lt;- coxph(Surv(os_time, os_event) ~ arm,\n        data = df\n    )\n    exp(unname(coef(cox_mod)))\n}\n\n\nWe can try on one data set first:\n\n\nShow the code\nhr_est_fun(os_data_samples[[1]])\n\n\n[1] 0.8503437\n\n\nNow we can apply this to all data sets:\n\n\nShow the code\nhr_estimates &lt;- sapply(os_data_samples, hr_est_fun)\n\n\nWe can visualize this:\n\n\nShow the code\nlibrary(ggplot2)\nhr_df &lt;- data.frame(hr = hr_estimates)\nggplot(hr_df, aes(x = hr)) +\n    geom_density(fill = \"lightblue\", alpha = 0.7) +\n    geom_vline(xintercept = 1, linetype = \"dashed\", color = \"red\") +\n    xlab(\"Hazard Ratio (treatment vs. control)\") +\n    ylab(\"Density\") +\n    ggtitle(\"Posterior Distribution of Hazard Ratio in Current Study\")",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  },
  {
    "objectID": "session-bhm/1_bhm-jmpost.html#probability-of-success",
    "href": "session-bhm/1_bhm-jmpost.html#probability-of-success",
    "title": "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing",
    "section": "Probability of success",
    "text": "Probability of success\nLet’s assume the minimal detectable difference, i.e. the hazard ratio that we would consider clinically relevant, is 0.75. Then our probability of success is given by the posterior probability that the hazard ratio is below 0.75:\n\n\nShow the code\nmean(hr_estimates &lt; 0.75)\n\n\n[1] 0.326",
    "crumbs": [
      "Session 5: BHM",
      "5. Bayesian Hierarchical Joint Modeling for Historical Data Borrowing"
    ]
  }
]