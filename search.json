[
  {
    "objectID": "session-os/0_setup.html",
    "href": "session-os/0_setup.html",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "This part is required for both the jmpost chapter as well as the brms chapter.\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 2: OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-os/0_setup.html#setup",
    "href": "session-os/0_setup.html#setup",
    "title": "0. Setup and data preparation",
    "section": "",
    "text": "This part is required for both the jmpost chapter as well as the brms chapter.\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\nlibrary(fuzzyjoin)\nlibrary(sn)\nlibrary(glue)\n\nif (require(cmdstanr)) {\n    # If cmdstanr is available, instruct brms to use cmdstanr as backend\n    # and cache all Stan binaries\n    options(\n        brms.backend = \"cmdstanr\",\n        cmdstanr_write_stan_file_dir = here(\"_brms-cache\")\n    )\n    dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n    rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n    stopifnot(is.logical(x))\n    ifelse(x, 1, 0)\n}",
    "crumbs": [
      "Session 2: OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-os/0_setup.html#data-preparation",
    "href": "session-os/0_setup.html#data-preparation",
    "title": "0. Setup and data preparation",
    "section": "Data preparation",
    "text": "Data preparation\nThis part is only required for the jmpost chapter.\nWe will again use the OAK study data.\nFor the tumor growth data, we are using the S1 data set from here. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\ntumor_data &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\") |&gt; \n    read_excel(sheet = \"Study4\") |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm)),\n        arm = fct_recode(arm, \"Docetaxel\" = \"1\", \"MPDL3280A\" = \"2\")\n    ) |&gt; \n    select(id, year, sld, arm)\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 4\n  id                       year   sld arm      \n  &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1 -594245265511412736  -0.00821  42   Docetaxel\n2 -594245265511412736   0.0986   44   Docetaxel\n3 -594245265511412736   0.222    44   Docetaxel\n4 -4078394139718744064 -0.0110   65.8 Docetaxel\n5 -4078394139718744064  0.832    66   Docetaxel\n6 -4078394139718744064  1.07     68   Docetaxel\n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        \n 5394902984416364544 :  16   Min.   :-0.1068   Min.   :  2.00  \n 7160320731596789760 :  16   1st Qu.: 0.1123   1st Qu.: 19.05  \n -4159496062492130816:  15   Median : 0.2519   Median : 32.00  \n -7900338178541499392:  15   Mean   : 0.3995   Mean   : 38.16  \n 4878279915140903936 :  15   3rd Qu.: 0.5749   3rd Qu.: 51.00  \n 9202154259772598272 :  15   Max.   : 2.0753   Max.   :228.00  \n (Other)             :4034                     NA's   :27      \n        arm      \n Docetaxel:1658  \n MPDL3280A:2468  \n                 \n                 \n                 \n                 \n                 \n\n\nHere we have 701 patients, and we know from Fig. 1 in the publication that this is the subset of patients with at least 3 tumor size measurements. We have guessed from the second Excel sheet with parameter estimates that arm 1 means docetaxel and arm 2 means MPDL3280A. This also seems reasonable given the larger number of measurements per patient:\n\n\nShow the code\n# make a table\ntumor_data |&gt; \n    group_by(id) |&gt; \n    summarize(arm = arm[1], n = n())  |&gt; \n    group_by(arm) |&gt; \n    summarize(mean_n = mean(n))\n\n\n# A tibble: 2 × 2\n  arm       mean_n\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Docetaxel   5.10\n2 MPDL3280A   6.56\n\n\nFor the OS data, we will be using the Supplementary Table 8 from here. Again, we have copied the data set in this GitHub repository.\nFrom the supplementary material, we know that:\n\nPFS/OS: time in months\nPFS.CNSR/OS.CNSR: 1 for censor, 0 for event\n\nTherefore we will convert the time to years and the censoring to a logical variable:\n\n\nShow the code\nos_data &lt;- here(\"data/41591_2018_134_MOESM3_ESM.xlsx\") |&gt; \n    read_excel(sheet = \"OAK_Clinical_Data\") |&gt; \n    clean_names() |&gt; \n    rename(\n        age = bage,\n        race = race2,\n        ecog = ecoggr,\n        arm = trt01p,\n        response = bcor\n    ) |&gt; \n    mutate(\n        id = factor(as.character(pt_id)),\n        sld = as.numeric(na_if(bl_sld, \".\")),\n        pfs_time = as.numeric(pfs) / 12,\n        pfs_event = as.numeric(pfs_cnsr) == 0,\n        os_time = as.numeric(os) / 12,\n        os_event = as.numeric(os_cnsr) == 0,\n        race = factor(race),\n        ecog = factor(ecog),\n        arm = factor(arm),\n        response = factor(\n          response, \n          levels = c(\"CR\", \"PR\", \"SD\", \"PD\", \"NE\")\n        ),\n        sex = factor(sex)\n    ) |&gt; \n    select(\n      id,\n      arm,\n      ecog,\n      age,\n      race,\n      sex,\n      sld,\n      response,\n      pfs_time,\n      pfs_event,\n      os_time,\n      os_event\n    )\n\nhead(os_data)\n\n\n# A tibble: 6 × 12\n  id    arm    ecog    age race  sex     sld response pfs_time pfs_event os_time\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;lgl&gt;       &lt;dbl&gt;\n1 318   Docet… 1        64 WHITE M     152   PD          0.107 TRUE        0.433\n2 1088  Docet… 0        65 WHITE M      36   PD          0.194 TRUE        0.402\n3 345   Docet… 1        75 WHITE M      92   &lt;NA&gt;        0.162 TRUE        0.162\n4 588   Docet… 0        61 WHITE F      36   SD          1.02  TRUE        2.05 \n5 306   MPDL3… 1        53 WHITE F      86   PD          0.118 TRUE        0.498\n6 718   Docet… 1        80 WHITE M      45.7 SD          0.712 TRUE        1.60 \n# ℹ 1 more variable: os_event &lt;lgl&gt;\n\n\nShow the code\nsummary(os_data)\n\n\n       id             arm      ecog         age           race     sex    \n 1000   :  1   Docetaxel:425   0:315   Min.   :33.00   ASIAN:180   F:330  \n 1001   :  1   MPDL3280A:425   1:535   1st Qu.:57.00   OTHER: 72   M:520  \n 1002   :  1                           Median :64.00   WHITE:598          \n 1003   :  1                           Mean   :63.23                      \n 1004   :  1                           3rd Qu.:70.00                      \n 1005   :  1                           Max.   :85.00                      \n (Other):844                                                              \n      sld         response      pfs_time        pfs_event      \n Min.   : 10.00   CR  :  7   Min.   :0.002738   Mode :logical  \n 1st Qu.: 41.00   PR  :108   1st Qu.:0.117728   FALSE:95       \n Median : 66.00   SD  :327   Median :0.240931   TRUE :755      \n Mean   : 76.69   PD  :304   Mean   :0.443174                  \n 3rd Qu.: 99.00   NE  : 24   3rd Qu.:0.580424                  \n Max.   :316.00   NA's: 80   Max.   :2.242300                  \n NA's   :1                                                     \n    os_time          os_event      \n Min.   :0.002738   Mode :logical  \n 1st Qu.:0.364134   FALSE:281      \n Median :0.803559   TRUE :569      \n Mean   :0.949841                  \n 3rd Qu.:1.620808                  \n Max.   :2.253251                  \n                                   \n\n\nNow we would like to join the two data sets, i.e. know which tumor growth data belongs to which baseline covariates and overall survival time and event indicator. Unfortunately we cannot use the id variable as it is not consistent between the two data sets, because both were anonymized. Instead, we will use the sld variable in combination with the treatment arm. We can also double check with the best overall response variable, because that is closely related to the tumor growth.\nFirst, we will summarize the patient information int he tumor data set. We will calculate the baseline SLD, determine the time of the last tumor measurement and approximate the best overall response by looking at the change from baseline:\n\n\nShow the code\nget_baseline &lt;- function(sld, year) {\n    which_base &lt;- tail(which(year &lt;= 0), 1L)\n    if (length(which_base) == 0) {\n      which_base &lt;- which.min(year)\n    }\n    sld[which_base]\n}\n\nget_contig_below_thresh &lt;- function(sld, year, bsld, thresh) {\n    rle_res &lt;- with(\n        rle(((sld[year &gt; 0] - bsld) / bsld) &lt; 0.2), \n        lengths[values]\n    )\n    if (length(rle_res) == 0) {\n      0\n    } else {\n      max(rle_res)\n    }\n}\n\ntumor_data_summary &lt;- tumor_data |&gt; \n    group_by(id) |&gt; \n    arrange(year) |&gt;\n    summarize(\n      arm = arm[1L],\n      bsld = get_baseline(sld, year),\n      last_year = tail(year, 1L),\n      nadir = min(sld[year &gt;= 0], na.rm = TRUE),\n      max_cfn = max((sld[year &gt;= 0] - nadir) / nadir, na.rm = TRUE),\n      min_cfb = min((sld[year &gt;= 0] - bsld) / bsld, na.rm = TRUE),\n      contig_below_0.2 = get_contig_below_thresh(sld, year, bsld, thresh = 0.2),\n      approx_response = case_when(\n        min_cfb &lt;= -0.3 ~ \"PR\",\n        contig_below_0.2 &gt;= 2 ~ \"SD\",\n        max_cfn &gt;= 0.2 ~ \"PD\",        \n        .default = \"NE\"\n      )   \n    )\nhead(tumor_data_summary)\n\n\n# A tibble: 6 × 9\n  id                arm    bsld last_year nadir max_cfn min_cfb contig_below_0.2\n  &lt;fct&gt;             &lt;fct&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n1 -100096848245159… Doce…    53     0.356    55  0       0.0377                3\n2 -102777882269110… MPDL…    11     0.901     2  2      -0.818                 7\n3 -105049343897269… MPDL…    20     0.345    21  1.62    0.05                  2\n4 -106934201929572… Doce…    15     1.72     11  0.273  -0.267                12\n5 -108949367808374… Doce…    33     0.709    31  0.0968 -0.0606                6\n6 -110624773747042… MPDL…    24     1.32     16  0.562  -0.333                 9\n# ℹ 1 more variable: approx_response &lt;chr&gt;\n\n\nNow let’s try to fuzzy join this with the OS data set, based on the baseline SLD and the treatment arm. In addition, we know that the last time point in the tumor data needs to be before the OS time point. We will also check that the best overall response is the same in both data sets. There will be some errors here: For example, we don’t know whether there were new lesions leading to a progression assessment, or whether the patient had later better results. So we need to expect that the matching will not be perfect.\n\n\nShow the code\nos_data_keys &lt;- os_data |&gt; \n    select(id, arm, sld, pfs_time, os_time, response)\nhead(os_data_keys)\n\n\n# A tibble: 6 × 6\n  id    arm         sld pfs_time os_time response\n  &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n1 318   Docetaxel 152      0.107   0.433 PD      \n2 1088  Docetaxel  36      0.194   0.402 PD      \n3 345   Docetaxel  92      0.162   0.162 &lt;NA&gt;    \n4 588   Docetaxel  36      1.02    2.05  SD      \n5 306   MPDL3280A  86      0.118   0.498 PD      \n6 718   Docetaxel  45.7    0.712   1.60  SD      \n\n\nShow the code\ndist_match &lt;- function(v1, v2) {\n    dist &lt;- abs(v1 - v2)\n    data.frame(include = (dist &lt;= 0.05))\n}\n\nless_match &lt;- function(lower, upper) {\n    data.frame(include = lower &lt;= upper)\n}\n\ntumor_os_data_joined &lt;- tumor_data_summary |&gt; \n    fuzzy_left_join(\n        os_data_keys,\n        by = c(\n          \"arm\" = \"arm\", \n          \"bsld\" = \"sld\",          \n          \"last_year\" = \"os_time\",\n          \"approx_response\" = \"response\"\n        ),\n        match_fun = list(\n          `==`, \n          dist_match, \n          less_match,\n          `==`\n        )\n    )\nnrow(tumor_os_data_joined)\n\n\n[1] 954\n\n\nWe see that we cannot match the two data sets exactly:\n\nWe have less patients to start with in the tumor size data set.\nWe have patients in the tumor size data set which match multiple patients in the OS data set based on the treatment arm and the baseline SLD, and also the last measurement time point cannot disambiguate them.\nWe have patients in the tumor size data set which do not match any patient in the OS data set.\n\nHowever, for the purpose of this training we don’t need to be able to recover the true full data set. Therefore we will just subset the matched data set to a reasonable combination of the two data sets:\n\n\nShow the code\ntumor_os_data_joined_subset &lt;- tumor_os_data_joined |&gt; \n    na.omit() |&gt; \n    filter(!duplicated(id.y)) |&gt;\n    filter(!duplicated(id.x))\nnrow(tumor_os_data_joined_subset)\n\n\n[1] 203\n\n\nNow let’s save the joining information:\n\n\nShow the code\ntgi_os_join_keys &lt;- tumor_os_data_joined_subset |&gt; \n    select(id.x, id.y, arm.x) |&gt; \n    rename(\n        id_tgi = id.x,\n        id_os = id.y,\n        arm = arm.x\n    )\ntable(tgi_os_join_keys$arm)\n\n\n\nDocetaxel MPDL3280A \n       96       107 \n\n\nSo we have about 100 patients in each arm.\nWe subset the two data sets accordingly:\n\n\nShow the code\ntumor_data &lt;- tumor_data |&gt; \n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_tgi\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id) |&gt; \n    rename(id = id_os)\n\nos_data &lt;- os_data |&gt;\n    inner_join(\n      tgi_os_join_keys, \n      by = c(\"id\" = \"id_os\", \"arm\" = \"arm\")\n    ) |&gt; \n    select(-id_tgi)",
    "crumbs": [
      "Session 2: OS",
      "0. Setup and data preparation"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html",
    "href": "session-os/2_os_weibull_brms.html",
    "title": "2. OS model minimal workflow with brms",
    "section": "",
    "text": "Let’s try to fit the same model now with brms. We will use the same data as in the previous notebook.",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html#setup-and-load-data",
    "href": "session-os/2_os_weibull_brms.html#setup-and-load-data",
    "title": "2. OS model minimal workflow with brms",
    "section": "Setup and load data",
    "text": "Setup and load data\nHere we directly start from the overall survival data with kg estimates, as we have obtained them in the previous notebook:\n\n\nShow the code\nos_data_with_kg &lt;- readRDS(here(\"session-os/os_data_with_kg.rds\"))\nhead(os_data_with_kg)\n\n\n# A tibble: 6 × 9\n  id    arm       ecog    age race  sex   os_time os_event kg_est\n  &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;lgl&gt;     &lt;dbl&gt;\n1 588   Docetaxel 0        61 WHITE F       2.05  FALSE     0.514\n2 330   MPDL3280A 1        56 WHITE F       1.68  FALSE     0.479\n3 791   Docetaxel 0        72 WHITE F       0.901 TRUE      0.387\n4 635   Docetaxel 0        42 OTHER F       1.66  TRUE      0.549\n5 365   MPDL3280A 0        64 WHITE F       1.43  TRUE      0.473\n6 773   Docetaxel 0        65 WHITE M       1.63  FALSE     0.307",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html#model-fitting-with-brms",
    "href": "session-os/2_os_weibull_brms.html#model-fitting-with-brms",
    "title": "2. OS model minimal workflow with brms",
    "section": "Model fitting with brms",
    "text": "Model fitting with brms\nLet’s first fit the model with brms. We will use the same model as in the previous notebook, but we will use the brms package to fit it.\nAn important ingredient for the model formula is the censoring information, passed via the cens() syntax: This should point to a variable containing the value 0 for observed events, i.e. no censoring, and the value 1 for right censored times (see ?brmsformula for more details). Therefore we first add such a variable to the data set:\n\n\nShow the code\nos_data_with_kg &lt;- os_data_with_kg |&gt;\n    mutate(\n        os_cens = ifelse(os_event, 0, 1)\n    )\n\n\nWe define our own design matrix with a column of ones:\n\n\nShow the code\nos_data_with_kg_design &lt;- model.matrix(\n    ~ os_time + os_cens + arm + ecog + age + race + sex + kg_est,\n    data = os_data_with_kg\n) |&gt;\n    as.data.frame() |&gt;\n    rename(ones = \"(Intercept)\")\nhead(os_data_with_kg_design)\n\n\n  ones   os_time os_cens armMPDL3280A ecog1 age raceOTHER raceWHITE sexM\n1    1 2.0506502       1            0     0  61         0         1    0\n2    1 1.6755647       1            1     1  56         0         1    0\n3    1 0.9007529       0            0     0  72         0         1    0\n4    1 1.6591376       0            0     0  42         1         0    0\n5    1 1.4291581       0            1     0  64         0         1    0\n6    1 1.6290212       1            0     0  65         0         1    1\n     kg_est\n1 0.5138783\n2 0.4789927\n3 0.3865749\n4 0.5486864\n5 0.4729831\n6 0.3070734\n\n\nNow we can define the model formula:\n\n\nShow the code\nformula &lt;- bf(\n    os_time | cens(os_cens) ~\n        0 +\n            ones +\n            armMPDL3280A +\n            ecog1 +\n            age +\n            raceOTHER +\n            raceWHITE +\n            sexM +\n            kg_est\n)\n\n\nSo here we suppress the automatic intercept provided by brms by using the 0 + syntax, and instead we our “own” vector of ones. This is because we want to avoid the default centering of covariates which is performed by brms when using an automatic intercept. Otherwise it would be difficult to exactly match the prior distributions we used in the jmpost model further below.\nIn order to find out about the parametrization of the Weibull model with brms and Stan here, let’s check the Stan code generated by brms for it:\n\n\nShow the code\nstancode(formula, data = os_data_with_kg_design, family = weibull())\n\n\n// generated with brms 2.22.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; shape;  // shape parameter\n}\ntransformed parameters {\n  real lprior = 0;  // prior contributions to the log posterior\n  lprior += gamma_lpdf(shape | 0.01, 0.01);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += X * b;\n    mu = exp(mu);\n    // vectorized log-likelihood contributions of censored data\n    target += weibull_lpdf(Y[Jevent[1:Nevent]] | shape, mu[Jevent[1:Nevent]] / tgamma(1 + 1 / shape));\n    target += weibull_lccdf(Y[Jrcens[1:Nrcens]] | shape, mu[Jrcens[1:Nrcens]] / tgamma(1 + 1 / shape));\n    target += weibull_lcdf(Y[Jlcens[1:Nlcens]] | shape, mu[Jlcens[1:Nlcens]] / tgamma(1 + 1 / shape));\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\nWe can see that the Stan code uses the weibull_* function family to define the log-likelihood contributions.\nWe can check the Stan reference doc here for details of the parametrization. We can see that this is the so-called “standard” parametrization (see Wikipedia) with shape parameter \\(\\alpha\\) and scale parameter \\(\\sigma\\). The mean of this distribution is \\(\\sigma \\Gamma(1 + 1/\\alpha)\\), where \\(\\Gamma\\) is the gamma function. We can see in the brms generated code that accordingly the sigma parameter is defined as mu / tgamma(1 + 1 / shape), such that mu is really the mean of the distribution.\nNow the problem is that this is a different parametrization than what we have used in jmpost (see the specification), which is the proportional hazards parametrization (see Wikipedia), where the covariate effects are on the log hazard scale instead of on the log mean scale. This has been identified by other brms users as a gap in the package, see e.g. here.\nFortunately, we can define a custom distribution in brms to use the proportional hazards parametrization. This parametrization relates to the Stan Weibull density definition with the transformation of \\(\\sigma := \\gamma^{-1 / \\alpha}\\). The code here has been first written by Bjoern Holzhauer and was extended by Sebastian Weber to integrate more tightly with brms (source). One thing to keep in mind here is that for technical reasons the first parameter of the custom distribution needs to be named mu and not gamma.\n\n\nShow the code\nfamily_weibull_ph &lt;- function(link_gamma = \"log\", link_alpha = \"log\") {\n    brms::custom_family(\n        name = \"weibull_ph\",\n        # first param needs to be \"mu\" cannot be \"gamma\"; alpha is the shape:\n        dpars = c(\"mu\", \"alpha\"),\n        links = c(link_gamma, link_alpha),\n        lb = c(0, 0),\n        # ub = c(NA, NA), # would be redundant\n        # no need for `vars` like for `cens`, brms can handle this.\n        type = \"real\",\n        loop = TRUE\n    )\n}\n\nsv_weibull_ph &lt;- brms::stanvar(\n    name = \"weibull_ph_stan_code\",\n    scode = \"\nreal weibull_ph_lpdf(real y, real mu, real alpha) {\n  // real sigma = pow(1 / mu, 1 / alpha);\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lpdf(y | alpha, sigma);\n}\nreal weibull_ph_lccdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lccdf(y | alpha, sigma);\n}\nreal weibull_ph_lcdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lcdf(y | alpha, sigma);\n}\nreal weibull_ph_rng(real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_rng(alpha, sigma);\n}\n\",\n    block = \"functions\"\n)\n\n## R definitions of auxilary helper functions of brms, these are based\n## on the respective weibull (internal) brms implementations:\n\nlog_lik_weibull_ph &lt;- function(i, prep) {\n    shape &lt;- get_dpar(prep, \"alpha\", i = i)\n    sigma &lt;- get_dpar(prep, \"mu\", i = i)^(-1 / shape)\n    args &lt;- list(shape = shape, scale = sigma)\n    out &lt;- brms:::log_lik_censor(\n        dist = \"weibull\",\n        args = args,\n        i = i,\n        prep = prep\n    )\n    out &lt;- brms:::log_lik_truncate(\n        out,\n        cdf = pweibull,\n        args = args,\n        i = i,\n        prep = prep\n    )\n    brms:::log_lik_weight(out, i = i, prep = prep)\n}\n\nposterior_predict_weibull_ph &lt;- function(i, prep, ntrys = 5, ...) {\n    shape &lt;- get_dpar(prep, \"alpha\", i = i)\n    sigma &lt;- get_dpar(prep, \"mu\", i = i)^(-1 / shape)\n    brms:::rcontinuous(\n        n = prep$ndraws,\n        dist = \"weibull\",\n        shape = shape,\n        scale = sigma,\n        lb = prep$data$lb[i],\n        ub = prep$data$ub[i],\n        ntrys = ntrys\n    )\n}\n\nposterior_epred_weibull_ph &lt;- function(prep) {\n    shape &lt;- get_dpar(prep, \"alpha\")\n    sigma &lt;- get_dpar(prep, \"mu\")^(-1 / shape)\n    sigma * gamma(1 + 1 / shape)\n}\n\n\nWe can again check the Stan code that is generated for this custom distribution:\n\n\nShow the code\nstancode(\n    formula,\n    data = os_data_with_kg_design,\n    stanvars = sv_weibull_ph, # We pass the custom Stan functions' code here.\n    family = family_weibull_ph()\n)\n\n\n// generated with brms 2.22.0\nfunctions {\n  \nreal weibull_ph_lpdf(real y, real mu, real alpha) {\n  // real sigma = pow(1 / mu, 1 / alpha);\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lpdf(y | alpha, sigma);\n}\nreal weibull_ph_lccdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lccdf(y | alpha, sigma);\n}\nreal weibull_ph_lcdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lcdf(y | alpha, sigma);\n}\nreal weibull_ph_rng(real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_rng(alpha, sigma);\n}\n\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; alpha;  // skewness parameter\n}\ntransformed parameters {\n  real lprior = 0;  // prior contributions to the log posterior\n  lprior += normal_lpdf(alpha | 0, 4)\n    - 1 * normal_lccdf(0 | 0, 4);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += X * b;\n    mu = exp(mu);\n    for (n in 1:N) {\n      // special treatment of censored data\n      if (cens[n] == 0) {\n        target += weibull_ph_lpdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == 1) {\n        target += weibull_ph_lccdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == -1) {\n        target += weibull_ph_lcdf(Y[n] | mu[n], alpha);\n      }\n    }\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\nIndeed we can now use the custom distribution. We also see the default priors in the transformed parameters block on the shape parameter (alpha). We don’t see an explicit prior on the regression coefficients (b), which means an improper flat prior is used by default.\nThe remaining challenge is that in jmpost we specified a Gamma prior for \\(\\lambda\\) which is now here the exponentiated intercept parameter. So in principle, we would need an ExpGamma prior on the intercept, meaning that if we exponentiate the intercept, it has a gamma distribution. However, this would again require a custom distribution. Let’s try to go with an approximation: we can just draw samples from the ExpGamma distribution (by sampling from a gamma distribution and taking the log) and then approximate this with a skewed normal distribution (see here for the Stan documentation):\n\n\nShow the code\nset.seed(123)\nintercept_samples &lt;- log(rgamma(1000, 0.7, 1))\n\nlibrary(sn)\nfit &lt;- selm(intercept_samples ~ 1, family = \"SN\")\nxi &lt;- coef(fit, \"DP\")[1]\nomega &lt;- coef(fit, \"DP\")[2]\nalpha &lt;- coef(fit, \"DP\")[3]\n\nhist(intercept_samples, probability = TRUE)\ncurve(dsn(x, xi, omega, alpha), add = TRUE, col = \"red\", lwd = 3)\n\n\n\n\n\n\n\n\n\nNow we can finally specify the priors:\n\n\nShow the code\npriors &lt;- c(\n    set_prior(\n        glue::glue(\"skew_normal({xi}, {omega}, {alpha})\"),\n        class = \"b\",\n        coef = \"ones\"\n    ),\n    prior(normal(0, 20), class = \"b\"),\n    prior(gamma(0.7, 1), class = \"alpha\")\n)\n\n\nLet’s do a final check of the Stan code:\n\n\nShow the code\nstancode(\n    formula,\n    data = os_data_with_kg_design,\n    prior = priors,\n    stanvars = sv_weibull_ph,\n    family = family_weibull_ph()\n)\n\n\n// generated with brms 2.22.0\nfunctions {\n  \nreal weibull_ph_lpdf(real y, real mu, real alpha) {\n  // real sigma = pow(1 / mu, 1 / alpha);\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lpdf(y | alpha, sigma);\n}\nreal weibull_ph_lccdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lccdf(y | alpha, sigma);\n}\nreal weibull_ph_lcdf(real y, real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_lcdf(y | alpha, sigma);\n}\nreal weibull_ph_rng(real mu, real alpha) {\n  real sigma = pow(mu, -1 * inv( alpha ));\n  return weibull_rng(alpha, sigma);\n}\n\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  // censoring indicator: 0 = event, 1 = right, -1 = left, 2 = interval censored\n  array[N] int&lt;lower=-1,upper=2&gt; cens;\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  // indices of censored data\n  int Nevent = 0;\n  int Nrcens = 0;\n  int Nlcens = 0;\n  array[N] int Jevent;\n  array[N] int Jrcens;\n  array[N] int Jlcens;\n  // collect indices of censored data\n  for (n in 1:N) {\n    if (cens[n] == 0) {\n      Nevent += 1;\n      Jevent[Nevent] = n;\n    } else if (cens[n] == 1) {\n      Nrcens += 1;\n      Jrcens[Nrcens] = n;\n    } else if (cens[n] == -1) {\n      Nlcens += 1;\n      Jlcens[Nlcens] = n;\n    }\n  }\n}\nparameters {\n  vector[K] b;  // regression coefficients\n  real&lt;lower=0&gt; alpha;  // skewness parameter\n}\ntransformed parameters {\n  real lprior = 0;  // prior contributions to the log posterior\n  lprior += skew_normal_lpdf(b[1] | 0.758392879263355, 2.57341668661882, -4.8061893638208);\n  lprior += normal_lpdf(b[2] | 0, 20);\n  lprior += normal_lpdf(b[3] | 0, 20);\n  lprior += normal_lpdf(b[4] | 0, 20);\n  lprior += normal_lpdf(b[5] | 0, 20);\n  lprior += normal_lpdf(b[6] | 0, 20);\n  lprior += normal_lpdf(b[7] | 0, 20);\n  lprior += normal_lpdf(b[8] | 0, 20);\n  lprior += gamma_lpdf(alpha | 0.7, 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += X * b;\n    mu = exp(mu);\n    for (n in 1:N) {\n      // special treatment of censored data\n      if (cens[n] == 0) {\n        target += weibull_ph_lpdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == 1) {\n        target += weibull_ph_lccdf(Y[n] | mu[n], alpha);\n      } else if (cens[n] == -1) {\n        target += weibull_ph_lcdf(Y[n] | mu[n], alpha);\n      }\n    }\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-os/brms1.rds\")\nif (file.exists(save_file)) {\n    fit &lt;- readRDS(save_file)\n} else {\n    fit &lt;- brm(\n        formula = formula,\n        data = os_data_with_kg_design,\n        prior = priors,\n        stanvars = sv_weibull_ph,\n        family = family_weibull_ph(),\n        chains = CHAINS,\n        iter = ITER + WARMUP,\n        warmup = WARMUP,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveRDS(fit, save_file)\n}\n\nsummary(fit)\n\n\n Family: weibull_ph \n  Links: mu = log; alpha = identity \nFormula: os_time | cens(os_cens) ~ 0 + ones + armMPDL3280A + ecog1 + age + raceOTHER + raceWHITE + sexM + kg_est \n   Data: os_data_with_kg_design (Number of observations: 203) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nones            -2.01      0.62    -3.23    -0.81 1.00     2021     2550\narmMPDL3280A    -0.34      0.20    -0.74     0.05 1.00     3251     2953\necog1            0.61      0.21     0.21     1.03 1.00     3548     2904\nage              0.00      0.01    -0.02     0.02 1.00     2298     2696\nraceOTHER        0.49      0.40    -0.32     1.24 1.00     3046     2777\nraceWHITE        0.07      0.23    -0.37     0.51 1.00     2948     3129\nsexM             0.28      0.20    -0.12     0.68 1.00     3762     2971\nkg_est           0.32      0.20    -0.09     0.67 1.00     3297     2217\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha     1.66      0.14     1.39     1.94 1.00     3208     2581\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSo the model converged fast and well.",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/2_os_weibull_brms.html#comparison-of-results",
    "href": "session-os/2_os_weibull_brms.html#comparison-of-results",
    "title": "2. OS model minimal workflow with brms",
    "section": "Comparison of results",
    "text": "Comparison of results\nLet’s compare the results of the brms model with the jmpost model.\nFirst we load again the jmpost results:\n\n\nShow the code\ndraws_jmpost &lt;- readRDS(here(\"session-os/os_draws.rds\")) |&gt;\n    rename_variables(\n        \"gamma\" = \"sm_weibull_ph_gamma\",\n        \"lambda\" = \"sm_weibull_ph_lambda\"\n    )\nsummary(draws_jmpost)\n\n\n# A tibble: 9 × 10\n  variable         mean   median      sd     mad      q5     q95  rhat ess_bulk\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 armMPDL3280A -0.346   -0.342   0.191   0.191   -0.667  -0.0394 1.01      990.\n2 ecog1         0.616    0.621   0.204   0.211    0.294   0.944  0.998     925.\n3 age           0.00225  0.00196 0.00978 0.00913 -0.0135  0.0194 1.00      744.\n4 raceOTHER     0.508    0.509   0.409   0.402   -0.164   1.17   1.00      935.\n5 raceWHITE     0.0790   0.0741  0.231   0.224   -0.301   0.481  1.00      984.\n6 sexM          0.282    0.278   0.209   0.212   -0.0738  0.622  0.998     903.\n7 kg_est        0.302    0.317   0.201   0.202   -0.0435  0.602  1.00      922.\n8 gamma         1.66     1.67    0.139   0.143    1.43    1.90   1.00     1089.\n9 lambda        0.176    0.145   0.126   0.0889   0.0484  0.419  1.00      740.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nWe prepare above brms results in the same format:\n\n\nShow the code\ndraws_brms &lt;- as_draws_array(fit) |&gt;\n    mutate_variables(lambda = exp(b_ones)) |&gt;\n    rename_variables(\n        \"gamma\" = \"alpha\",\n        \"armMPDL3280A\" = \"b_armMPDL3280A\",\n        \"ecog1\" = \"b_ecog1\",\n        \"age\" = \"b_age\",\n        \"raceOTHER\" = \"b_raceOTHER\",\n        \"raceWHITE\" = \"b_raceWHITE\",\n        \"sexM\" = \"b_sexM\",\n        \"kg_est\" = \"b_kg_est\"\n    ) |&gt;\n    subset_draws(\n        variable = c(\n            \"armMPDL3280A\",\n            \"ecog1\",\n            \"age\",\n            \"raceOTHER\",\n            \"raceWHITE\",\n            \"sexM\",\n            \"kg_est\",\n            \"gamma\",\n            \"lambda\"\n        )\n    )\nsummary(draws_brms)\n\n\n# A tibble: 9 × 10\n  variable         mean   median      sd     mad      q5      q95  rhat ess_bulk\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 armMPDL3280A -0.344   -0.347   0.200   0.192   -0.678  -0.00722  1.00    3251.\n2 ecog1         0.610    0.607   0.210   0.212    0.270   0.962    1.00    3548.\n3 age           0.00330  0.00335 0.00935 0.00948 -0.0120  0.0189   1.00    2298.\n4 raceOTHER     0.494    0.515   0.403   0.406   -0.195   1.14     1.00    3046.\n5 raceWHITE     0.0665   0.0654  0.228   0.226   -0.305   0.440    1.00    2948.\n6 sexM          0.278    0.272   0.204   0.202   -0.0551  0.623    1.00    3762.\n7 kg_est        0.320    0.334   0.197   0.193   -0.0218  0.624    1.00    3297.\n8 gamma         1.66     1.65    0.140   0.140    1.43    1.89     1.00    3208.\n9 lambda        0.163    0.134   0.110   0.0792   0.0481  0.368    1.00    2021.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nSo the results agree well. We can also see this in density plots:\n\n\nShow the code\n# Combine the draws into one data frame\ndraws_combined &lt;- bind_rows(\n    mutate(as_draws_df(draws_jmpost), source = \"jmpost\"),\n    mutate(as_draws_df(draws_brms), source = \"brms\")\n) |&gt;\n    select(-.chain, -.iteration, -.draw)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\n# Convert to long format for ggplot2\ndraws_long &lt;- pivot_longer(\n    draws_combined,\n    cols = -source,\n    names_to = \"parameter\",\n    values_to = \"value\"\n)\n\n# Plot the densities\nggplot(draws_long, aes(x = value, fill = source)) +\n    geom_density(alpha = 0.5) +\n    facet_wrap(~parameter, scales = \"free\") +\n    theme_minimal() +\n    labs(\n        title = \"Posterior Parameter Samples Comparison\",\n        x = \"Value\",\n        y = \"Density\"\n    )\n\n\n\n\n\n\n\n\n\nGenerally these agree very well with each other. For the \\(\\lambda\\) parameter we do expect a slight difference, because we just used approximately the same prior distribution in brms compared to jmpost.",
    "crumbs": [
      "Session 2: OS",
      "2. OS model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html",
    "href": "session-tgi/0_setup.html",
    "title": "0. Setup",
    "section": "",
    "text": "This is a repository with training material for Tumor Growth Inhibition (TGI) and joint TGI-OS (Overall Survival) modeling.\nHere is an overview of the required setup steps, which are described in more detail below:\n\nInstall RTools (if you are on Windows)\nInstall necessary R packages\nInstall cmdstanr (optional but highly recommended)\nClone the repository from GitHub (https://github.com/RCONIS/tgi-os-training)\nOpen the folder in RStudio or VSCode\n\n\n\nIf you are running the examples on a Windows PC, you will need to install RTools, see\nhere.\nYou need to use the version of RTools that matches your R version. You can check your R version by running R.Version()$version.string.\nYou can then afterwards check the installation of RTools with:\nif(!require(pkgbuild)) install.packages(\"pkgbuild\")\npkgbuild::has_build_tools()\n\n\n\nThe following code will install the packages that are required to run the examples in this repository.\ninstall_if_not_available &lt;- function(pkg, min_version = NULL) {\n    is_installed &lt;- suppressWarnings(suppressPackageStartupMessages(\n      require(pkg, character.only = TRUE)\n    ))\n    if (is_installed & !is.null(min_version)) {\n      version_ok &lt;- packageVersion(pkg) &gt;= min_version\n    }    \n    if (!is_installed | !version_ok) {\n        install.packages(pkg)\n    }\n  }\npackages &lt;- c(\n  \"bayesplot\", \n  \"brms\", \n  \"ggplot2\",\n  \"gt\",\n  \"here\", \n  \"janitor\",\n  \"modelr\",\n  \"posterior\",\n  \"readxl\",\n  \"rstan\",\n  \"tidybayes\", \n  \"tidyverse\",   \n  \"truncnorm\",\n  \"sn\",\n  \"fuzzyjoin\",\n  \"glue\"\n)\nsapply(packages, install_if_not_available)\nremotes::install_github(\"genentech/jmpost\")\n\n\n\nOptionally, you can use cmdstanr as the backend of brms for fitting the models.\nThere are a few advantages of using cmdstanr over the default rstan:\n\nWith cmdstanr, you can cache the compiled model, therefore you don’t need to recompile the model as long as the same formula is used, even after restarting R session (or re-opening RStudio).\ncmdstanr is more up-to-date and more actively maintained compared to rstan.\n\nA detailed installation guide for cmdstanr is available here. Here is a brief summary:\n\nInstall cmdstanr with:\n# Typically, you install cmdstanr from the R-universe as follows:\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\n\n# If the above does not work, e.g. because you in a company network or VPN, you can install cmdstanr from GitHub instead:\nremotes::install_github(\"stan-dev/cmdstanr@*release\")    \nCheck the compiler toolchain with:\ncmdstanr::check_cmdstan_toolchain()\n\nThis should not be a problem on Mac and Linux (including RStudio Cloud instances), but might be a problem on Windows.\nIf you have problems, please check the installation guide for Windows.\n\nRTools is the easiest way.\nEven if you have RTools, you might still see an error like \"Rtools44 installation found but the toolchain was not installed.\".\nIn this case, you can run cmdstanr::check_cmdstan_toolchain(fix = TRUE) and this will likely resolve the issue.\n\n\nInstall the CmdStan backend with:\ncmdstanr::install_cmdstan(cores = 2)\nTesting the cmdstanr installation\n\nIf the above installation was successful, you should now be able to run the following simple model.\n\nlibrary(cmdstanr)\nfile &lt;- file.path(\n  cmdstan_path(), \n  \"examples\", \"bernoulli\", \"bernoulli.stan\"\n)\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500\n)",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#install-rtools",
    "href": "session-tgi/0_setup.html#install-rtools",
    "title": "0. Setup",
    "section": "",
    "text": "If you are running the examples on a Windows PC, you will need to install RTools, see\nhere.\nYou need to use the version of RTools that matches your R version. You can check your R version by running R.Version()$version.string.\nYou can then afterwards check the installation of RTools with:\nif(!require(pkgbuild)) install.packages(\"pkgbuild\")\npkgbuild::has_build_tools()",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#install-necessary-r-packages",
    "href": "session-tgi/0_setup.html#install-necessary-r-packages",
    "title": "0. Setup",
    "section": "",
    "text": "The following code will install the packages that are required to run the examples in this repository.\ninstall_if_not_available &lt;- function(pkg, min_version = NULL) {\n    is_installed &lt;- suppressWarnings(suppressPackageStartupMessages(\n      require(pkg, character.only = TRUE)\n    ))\n    if (is_installed & !is.null(min_version)) {\n      version_ok &lt;- packageVersion(pkg) &gt;= min_version\n    }    \n    if (!is_installed | !version_ok) {\n        install.packages(pkg)\n    }\n  }\npackages &lt;- c(\n  \"bayesplot\", \n  \"brms\", \n  \"ggplot2\",\n  \"gt\",\n  \"here\", \n  \"janitor\",\n  \"modelr\",\n  \"posterior\",\n  \"readxl\",\n  \"rstan\",\n  \"tidybayes\", \n  \"tidyverse\",   \n  \"truncnorm\",\n  \"sn\",\n  \"fuzzyjoin\",\n  \"glue\"\n)\nsapply(packages, install_if_not_available)\nremotes::install_github(\"genentech/jmpost\")",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/0_setup.html#optional-install-cmdstanr",
    "href": "session-tgi/0_setup.html#optional-install-cmdstanr",
    "title": "0. Setup",
    "section": "",
    "text": "Optionally, you can use cmdstanr as the backend of brms for fitting the models.\nThere are a few advantages of using cmdstanr over the default rstan:\n\nWith cmdstanr, you can cache the compiled model, therefore you don’t need to recompile the model as long as the same formula is used, even after restarting R session (or re-opening RStudio).\ncmdstanr is more up-to-date and more actively maintained compared to rstan.\n\nA detailed installation guide for cmdstanr is available here. Here is a brief summary:\n\nInstall cmdstanr with:\n# Typically, you install cmdstanr from the R-universe as follows:\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\n\n# If the above does not work, e.g. because you in a company network or VPN, you can install cmdstanr from GitHub instead:\nremotes::install_github(\"stan-dev/cmdstanr@*release\")    \nCheck the compiler toolchain with:\ncmdstanr::check_cmdstan_toolchain()\n\nThis should not be a problem on Mac and Linux (including RStudio Cloud instances), but might be a problem on Windows.\nIf you have problems, please check the installation guide for Windows.\n\nRTools is the easiest way.\nEven if you have RTools, you might still see an error like \"Rtools44 installation found but the toolchain was not installed.\".\nIn this case, you can run cmdstanr::check_cmdstan_toolchain(fix = TRUE) and this will likely resolve the issue.\n\n\nInstall the CmdStan backend with:\ncmdstanr::install_cmdstan(cores = 2)\nTesting the cmdstanr installation\n\nIf the above installation was successful, you should now be able to run the following simple model.\n\nlibrary(cmdstanr)\nfile &lt;- file.path(\n  cmdstan_path(), \n  \"examples\", \"bernoulli\", \"bernoulli.stan\"\n)\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500\n)",
    "crumbs": [
      "Session 1: TGI",
      "0. Setup"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html",
    "href": "session-tgi/4_tgi_cb_brms.html",
    "title": "4. Claret-Bruno model",
    "section": "",
    "text": "This appendix shows how the Claret-Bruno model can be implemented in a Bayesian framework using the brms package in R.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#setup-and-load-data",
    "href": "session-tgi/4_tgi_cb_brms.html#setup-and-load-data",
    "title": "4. Claret-Bruno model",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#claret-bruno-model",
    "href": "session-tgi/4_tgi_cb_brms.html#claret-bruno-model",
    "title": "4. Claret-Bruno model",
    "section": "Claret-Bruno model",
    "text": "Claret-Bruno model\nIn the Claret-Bruno model we have again the baseline SLD and the growth rate as in the Stein-Fojo model. Then in addition we have the inhibition response rate \\(\\psi_{p}\\) and the treatment resistance rate \\(\\psi_{c}\\). The model is then:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\exp \\left\\{\n  \\psi_{k_{g}i} t_{ij} -\n  \\frac{\\psi_{pi}}{\\psi_{ci}} (1 - \\exp(-\\psi_{ci} t_{ij}))\n\\right\\}\n\\]\nfor positive times \\(t_{ij}\\). Again, if the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. That is, we have then \\(\\psi_{pi} = 0\\). Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\exp \\{\n  \\psi_{k_{g}i} t_{ij} -\n  \\frac{\\psi_{pi}}{\\psi_{ci}} (1 - \\exp(-\\psi_{ci} t_{ij}))\n\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\nFor the new model parameters we can again use log-normal prior distributions.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#fit-model",
    "href": "session-tgi/4_tgi_cb_brms.html#fit-model",
    "title": "4. Claret-Bruno model",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * exp(kg * year - (p / c) * (1 - exp(-c * year)))) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # Define the standard deviation (called sigma in brms) as a \n  # coefficient tau times the mean.\n  # sigma = tau * ystar is modelled on the log scale though, therefore:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations\n  nlf(b0 ~ exp(lb0)) +\n  nlf(kg ~ exp(lkg)) +\n  nlf(p ~ exp(lp)) +\n  nlf(c ~ exp(lc)) +\n  # Define random effect structure\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lkg ~ 1 + (1 | id)) +\n  lf(lp ~ 1 + (1 | id)) + \n  lf(lc ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.5), 0.1), nlpar = \"lkg\"),\n  prior(normal(0, 1), nlpar = \"lp\"),\n  prior(normal(log(0.5), 0.1), nlpar = \"lc\"),\n  prior(normal(2, 1), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(1, 1), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 0.5), lb = 0, nlpar = \"lp\", class = \"sd\"),\n  prior(normal(0, 0.5), lb = 0, nlpar = \"lc\", class = \"sd\"),\n  prior(normal(0, 1), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lkg = array(-0.69),\n  b_lp = array(0),\n  b_lc = array(-0.69),\n  sd_1 = array(0.5),\n  sd_2 = array(0.5),\n  sd_3 = array(0.1),\n  sd_4 = array(0.1),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients),\n  z_4 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/cb3.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else if (interactive()) {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = WARMUP + ITER, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH,\n    adapt_delta = 0.9,\n    max_treedepth = 15\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsave_fit_sum_file &lt;- here(\"session-tgi/cb3_fit_sum.RData\")\nif (file.exists(save_fit_sum_file)) {\n  load(save_fit_sum_file)\n} else {\n  fit_sum &lt;- summary(fit)\n  save(fit_sum, file = save_fit_sum_file)\n}\nfit_sum\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ ystar \n         ystar ~ int_step(year &gt; 0) * (b0 * exp(kg * year - (p/c) * (1 - exp(-c * year)))) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(ystar)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         kg ~ exp(lkg)\n         p ~ exp(lp)\n         c ~ exp(lc)\n         lb0 ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n         lp ~ 1 + (1 | id)\n         lc ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.01      419     1044\nsd(lkg_Intercept)     1.04      0.06     0.92     1.17 1.01      696     1181\nsd(lp_Intercept)      1.58      0.11     1.39     1.80 1.01      443     1110\nsd(lc_Intercept)      1.57      0.15     1.28     1.87 1.01      657      935\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.15      0.00     0.15     0.16 1.00     1345     3193\nlb0_Intercept     3.62      0.02     3.57     3.66 1.01      200      330\nlkg_Intercept    -1.00      0.07    -1.14    -0.86 1.01     1020     1906\nlp_Intercept     -0.82      0.14    -1.10    -0.57 1.01      643     1594\nlc_Intercept     -0.13      0.10    -0.34     0.07 1.00     1335     2478\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe did obtain here a warning about divergent transitions, see stan documentation for details:\nWarning message:\nThere were 4489 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup \nHowever, the effective sample size is high, i.e. the Rhat values are close to 1. This indicates that the chains have converged. We can proceed with the post-processing.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#parameter-estimates",
    "href": "session-tgi/4_tgi_cb_brms.html#parameter-estimates",
    "title": "4. Claret-Bruno model",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\n\nShow the code\npost_df_file &lt;- here(\"session-tgi/cb3_post_df.RData\")\nif (file.exists(post_df_file)) {\n  load(post_df_file)\n} else {\n  post_df &lt;- as_draws_df(fit) |&gt; \n    subset_draws(iteration = (1:1000) * 2)\n  save(post_df, file = post_df_file)\n}\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lkg_Intercept\"       \n [4] \"b_lp_Intercept\"         \"b_lc_Intercept\"         \"sd_id__lb0_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"sd_id__lp_Intercept\"    \"sd_id__lc_Intercept\"   \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_p = exp(b_lp_Intercept + sd_id__lp_Intercept^2 / 2),\n    theta_c = exp(b_lc_Intercept + sd_id__lc_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    omega_p = sd_id__lp_Intercept,\n    omega_c = sd_id__lc_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    cv_p = sqrt(exp(sd_id__lp_Intercept^2) - 1),\n    cv_c = sqrt(exp(sd_id__lc_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nLet’s first look at the population level parameters:\n\n\nShow the code\ncb_pop_params &lt;- c(\"theta_b0\", \"theta_kg\", \"theta_p\", \"theta_c\", \"sigma\")\n\nmcmc_trace(post_df, pars = cb_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_dens_overlay(post_df, pars = cb_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = cb_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nThe trace plots look good. The chains seem to have converged and the pairs plot shows no strong correlations between the parameters. Let’s check the table:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  dplyr::select(theta_b0, theta_kg, theta_p, theta_c, omega_0, omega_g, omega_p, omega_c,\n  cv_0, cv_g, cv_p, cv_c,\n  sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.142\n44.118\n1.093\n1.090\n42.411\n45.949\n1.003\n198.908\n390.883\n\n\ntheta_kg\n0.636\n0.634\n0.042\n0.041\n0.570\n0.711\n1.000\n1,093.102\n1,749.378\n\n\ntheta_p\n1.539\n1.515\n0.183\n0.179\n1.284\n1.871\n1.002\n361.463\n395.589\n\n\ntheta_c\n3.098\n2.984\n0.678\n0.607\n2.234\n4.366\n1.001\n552.128\n669.150\n\n\nomega_0\n0.581\n0.581\n0.016\n0.016\n0.556\n0.608\n1.003\n429.108\n1,002.446\n\n\nomega_g\n1.043\n1.040\n0.063\n0.062\n0.943\n1.149\n1.004\n664.633\n1,063.643\n\n\nomega_p\n1.576\n1.570\n0.105\n0.104\n1.413\n1.757\n1.001\n434.576\n976.113\n\n\nomega_c\n1.569\n1.565\n0.150\n0.153\n1.326\n1.822\n1.001\n638.934\n941.209\n\n\ncv_0\n0.634\n0.634\n0.020\n0.021\n0.602\n0.669\n1.003\n429.108\n1,002.446\n\n\ncv_g\n1.409\n1.396\n0.140\n0.136\n1.197\n1.655\n1.004\n664.633\n1,063.643\n\n\ncv_p\n3.383\n3.278\n0.645\n0.578\n2.521\n4.577\n1.001\n434.576\n976.113\n\n\ncv_c\n3.415\n3.254\n0.930\n0.838\n2.192\n5.165\n1.001\n638.934\n941.209\n\n\nsigma\n0.154\n0.154\n0.002\n0.002\n0.150\n0.158\n1.000\n1,258.184\n2,273.384\n\n\n\n\n\n\n\nWe see similar estimated values as before for \\(\\theta_{b_{0}}\\), \\(\\theta_{k_{g}}\\) and \\(\\sigma\\).",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#observation-vs-model-fit",
    "href": "session-tgi/4_tgi_cb_brms.html#observation-vs-model-fit",
    "title": "4. Claret-Bruno model",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim_save_file &lt;- here(\"session-tgi/cb3_sim_df.RData\")\nif (file.exists(df_sim_save_file)) {\n  load(df_sim_save_file)\n} else {\n  df_sim &lt;- df_subset |&gt; \n    data_grid(\n      id = pt_subset, \n      year = seq_range(year, 101)\n    ) |&gt;\n    add_epred_draws(fit) |&gt;\n    median_qi()\n  save(df_sim, file = df_sim_save_file)\n}\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = .epred, ymin = .lower, ymax = .upper), \n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = .epred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"CB model fit\")\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThis also looks good. The model seems to capture the data well.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "session-tgi/4_tgi_cb_brms.html#with-jmpost",
    "href": "session-tgi/4_tgi_cb_brms.html#with-jmpost",
    "title": "4. Claret-Bruno model",
    "section": "With jmpost",
    "text": "With jmpost\nThis model can also be fit with the jmpost package. The corresponding function is LongitudinalClaretBruno. The statistical model is specified in the vignette here.\nHomework: Implement the generalized Claret-Bruno model with jmpost and compare the results with the brms implementation.",
    "crumbs": [
      "Session 1: TGI",
      "4. Claret-Bruno model"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Contents",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n2025-03-25\n\n\n0. Setup and data preparation\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n2025-03-25\n\n\n1. OS model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-03-25\n\n\n2. OS model minimal workflow with brms\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#session-2-os",
    "href": "index.html#session-2-os",
    "title": "Contents",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n2025-03-25\n\n\n0. Setup and data preparation\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n2025-03-25\n\n\n1. OS model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-03-25\n\n\n2. OS model minimal workflow with brms\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#session-1-tgi",
    "href": "index.html#session-1-tgi",
    "title": "Contents",
    "section": "Session 1: TGI",
    "text": "Session 1: TGI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n2025-03-25\n\n\n0. Setup\n\n\nFrancois Mercier, Daniel Sabanés Bové\n\n\n\n\n2025-03-25\n\n\n1. TGI model minimal workflow with brms\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-03-25\n\n\n2. TGI model minimal workflow with jmpost\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-03-25\n\n\n3. Generalized Stein-Fojo model\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n2025-03-25\n\n\n4. Claret-Bruno model\n\n\nDaniel Sabanés Bové, Francois Mercier\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Contents",
    "section": "License",
    "text": "License\nThe training material is licensed under a Creative Commons Attribution 4.0 International License. If you wish to reuse any part of this material, please ensure proper attribution is given to the original authors as specified by the Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Contents",
    "section": "Copyright",
    "text": "Copyright\n© 2025 Genentech Inc. All rights reserved.",
    "crumbs": [
      "Contents"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html",
    "href": "session-tgi/3_tgi_gsf_brms.html",
    "title": "3. Generalized Stein-Fojo model",
    "section": "",
    "text": "This appendix shows how the generalized Stein-Fojo model can be implemented in a Bayesian framework using the brms package in R.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#setup-and-load-data",
    "href": "session-tgi/3_tgi_gsf_brms.html#setup-and-load-data",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#generalized-stein-fojo-model",
    "href": "session-tgi/3_tgi_gsf_brms.html#generalized-stein-fojo-model",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Generalized Stein-Fojo model",
    "text": "Generalized Stein-Fojo model\nHere we have an additional parameter \\(\\phi\\), which is the weight for the shrinkage in the double exponential model. The model is then:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\{\n  \\psi_{\\phi i} \\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) +\n  (1 - \\psi_{\\phi i}) \\exp(\\psi_{k_{g}i} \\cdot t_{ij})\n\\}\n\\]\nfor positive times \\(t_{ij}\\). Again, if the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. That is, we have then \\(\\phi_i = 0\\). Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\{\n  \\psi_{\\phi i} \\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) +\n  (1 - \\psi_{\\phi i}) \\exp(\\psi_{k_{g}i} \\cdot t_{ij})\n\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\nIn terms of likelihood and priors, we can use the same assumptions as in the previous model. The only difference is that we have to model the \\(\\phi\\) parameter. We can use a logit-normal distribution for this parameter. This is a normal distribution on the logit scale, which is then transformed to the unit interval. \\[\n\\psi_{\\phi i} \\sim \\text{LogitNormal}(\\text{logit}(0.5) = 0, 0.5)\n\\]",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#fit-model",
    "href": "session-tgi/3_tgi_gsf_brms.html#fit-model",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (phi * exp(-ks * year) + (1 - phi) * exp(kg * year))) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # As before:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(phi ~ inv_logit(tphi)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(tphi ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + (1 | id)) +\n  lf(lkg ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\"),\n  prior(normal(log(1.04), 1), nlpar = \"lkg\"),\n  prior(normal(0, 0.5), nlpar = \"tphi\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(student_t(3, 0, 22.2), lb = 0, nlpar = \"tphi\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lks = array(-1.25),\n  b_lkg = array(-1.33),\n  b_tphi = array(0),\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  sd_4 = array(0.1),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients),\n  z_4 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/gsf1.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = WARMUP + ITER, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsummary(fit)\n\n\nWarning: There were 41 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (phi * exp(-ks * year) + (1 - phi) * exp(kg * year))) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         phi ~ inv_logit(tphi)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         tphi ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)      0.58      0.02     0.55     0.61 1.01      382      606\nsd(tphi_Intercept)     2.12      0.18     1.78     2.49 1.01      611     1216\nsd(lks_Intercept)      2.16      0.14     1.90     2.46 1.00      520     1282\nsd(lkg_Intercept)      1.18      0.09     1.01     1.36 1.00      899     1766\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept      0.15      0.00     0.14     0.15 1.00     1611     2351\nlb0_Intercept      3.63      0.02     3.59     3.68 1.03      221      547\ntphi_Intercept    -0.14      0.22    -0.57     0.27 1.00      424      839\nlks_Intercept     -0.62      0.10    -0.81    -0.43 1.00     1170     1868\nlkg_Intercept     -1.15      0.14    -1.43    -0.89 1.00      699     1488\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn total this took 76 minutes on my laptop.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#parameter-estimates",
    "href": "session-tgi/3_tgi_gsf_brms.html#parameter-estimates",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\n\nShow the code\npost_df &lt;- as_draws_df(fit)\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_tphi_Intercept\"      \n [4] \"b_lks_Intercept\"        \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"  \n [7] \"sd_id__tphi_Intercept\"  \"sd_id__lks_Intercept\"   \"sd_id__lkg_Intercept\"  \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_phi = plogis(b_tphi_Intercept),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    omega_phi = sd_id__tphi_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nLet’s first look at the population level parameters:\n\n\nShow the code\ngsf_pop_params &lt;- c(\"theta_b0\", \"theta_ks\", \"theta_kg\", \"theta_phi\", \"sigma\")\n\nmcmc_trace(post_df, pars = gsf_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = gsf_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nThe trace plots look good. The chains seem to have converged and the pairs plot shows no strong correlations between the parameters. Let’s check the table:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  select(theta_b0, theta_ks, theta_kg, theta_phi, omega_0, omega_s, omega_g, omega_phi, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(decimals = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.766\n44.746\n1.057\n1.073\n43.074\n46.528\n1.007\n179.361\n378.906\n\n\ntheta_ks\n5.857\n5.444\n1.924\n1.582\n3.589\n9.560\n1.003\n384.622\n905.388\n\n\ntheta_kg\n0.640\n0.637\n0.061\n0.062\n0.541\n0.745\n1.000\n685.379\n1,885.264\n\n\ntheta_phi\n0.467\n0.466\n0.055\n0.058\n0.376\n0.555\n1.003\n419.761\n831.991\n\n\nomega_0\n0.578\n0.578\n0.016\n0.017\n0.553\n0.606\n1.001\n376.483\n628.807\n\n\nomega_s\n2.159\n2.155\n0.145\n0.145\n1.929\n2.412\n1.002\n521.866\n1,265.639\n\n\nomega_g\n1.177\n1.174\n0.090\n0.090\n1.037\n1.333\n1.002\n868.075\n1,746.640\n\n\nomega_phi\n2.117\n2.108\n0.182\n0.184\n1.833\n2.423\n1.003\n603.806\n1,215.066\n\n\nsigma\n0.147\n0.147\n0.002\n0.002\n0.143\n0.150\n1.000\n1,575.916\n2,168.010\n\n\n\n\n\n\n\nSo \\(\\theta_{\\phi}\\) is estimated around 0.5. The other parameters are similar to the previous Stein-Fojo model, but we see a larger \\(\\theta_{k_s}\\) e.g.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#observation-vs-model-fit",
    "href": "session-tgi/3_tgi_gsf_brms.html#observation-vs-model-fit",
    "title": "3. Generalized Stein-Fojo model",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_epred_draws(fit) |&gt;\n  median_qi()\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = .epred, ymin = .lower, ymax = .upper), \n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = .epred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"GSF model fit\")\n\n\n\n\n\n\n\n\n\nThis also looks good. The model seems to capture the data well.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/3_tgi_gsf_brms.html#with-jmpost",
    "href": "session-tgi/3_tgi_gsf_brms.html#with-jmpost",
    "title": "3. Generalized Stein-Fojo model",
    "section": "With jmpost",
    "text": "With jmpost\nThis model can also be fit with the jmpost package. The corresponding function is LongitudinalGSF. The statistical model is specified in the vignette here.\nHomework: Implement the generalized Stein-Fojo model with jmpost and compare the results with the brms implementation.",
    "crumbs": [
      "Session 1: TGI",
      "3. Generalized Stein-Fojo model"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html",
    "href": "session-tgi/2_tgi_sf_jmpost.html",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "",
    "text": "Let’s try to fit the same model now with jmpost. We will use the same data as in the previous notebook.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#setup-and-load-data",
    "href": "session-tgi/2_tgi_sf_jmpost.html#setup-and-load-data",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#data-preparation",
    "href": "session-tgi/2_tgi_sf_jmpost.html#data-preparation",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Data preparation",
    "text": "Data preparation\nWe start with the subject level data set. For the beginning, we want to treat all observations as if they come from a single arm and single study for now. Therefore we insert constant study and arm values here.\n\n\nShow the code\nsubj_df &lt;- data.frame(\n  id = unique(df$id),\n  arm = \"arm\",\n  study = \"study\"\n)\nsubj_data &lt;- DataSubject(\n  data = subj_df,\n  subject = \"id\",\n  arm = \"arm\",\n  study = \"study\"\n)\n\n\nNext we prepare the longitudinal data object.\n\n\nShow the code\nlong_df &lt;- df |&gt;\n  select(id, year, sld)\nlong_data &lt;- DataLongitudinal(\n  data = long_df,\n  formula = sld ~ year\n)\n\n\nNow we can create the JointData object:\n\n\nShow the code\njoint_data &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data\n)",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#model-specification",
    "href": "session-tgi/2_tgi_sf_jmpost.html#model-specification",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Model specification",
    "text": "Model specification\nThe statistical model is specified in the jmpost vignette here.\nHere we just want to fit the longitudinal data, therefore:\n\n\nShow the code\ntgi_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    )\n)\n\n\nNote that the priors on the standard deviations, omega_* and sigma, are truncated to the positive domain. So we used here truncated normal priors.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#fit-model",
    "href": "session-tgi/2_tgi_sf_jmpost.html#fit-model",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using jmpost.\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/jm5.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  mcmc_results &lt;- sampleStanModel(\n      tgi_mod,\n      data = joint_data,\n      iter_sampling = ITER,\n      iter_warmup = WARMUP,\n      chains = CHAINS,\n      parallel_chains = CHAINS,\n      thin = CHAINS,\n      seed = BAYES.SEED,\n      refresh = REFRESH\n  )\n  save(mcmc_results, file = save_file)\n}\n\n\nLet’s check the convergence of the population parameters:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nsave_overall_file &lt;- here(\"session-tgi/jm5_more.RData\")\nif (file.exists(save_overall_file)) {\n  load(save_overall_file)\n} else {\n  mcmc_res_cmdstan &lt;- cmdstanr::as.CmdStanMCMC(mcmc_results)\n  mcmc_res_sum &lt;- mcmc_res_cmdstan$summary(vars)\n  vars_draws &lt;- mcmc_res_cmdstan$draws(vars)\n  loo_res &lt;- mcmc_res_cmdstan$loo(r_eff = FALSE)\n  save(mcmc_res_sum, vars_draws, loo_res, file = save_overall_file)\n}\nmcmc_res_sum\n\n\n# A tibble: 7 × 10\n  variable     mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lm_sf_mu_…  3.61   3.61  0.0236  0.0232   3.57   3.65   1.02     142.     188.\n2 lm_sf_mu_… -1.27  -1.27  0.160   0.152   -1.54  -1.03   1.01     352.     500.\n3 lm_sf_mu_… -1.35  -1.35  0.0828  0.0789  -1.49  -1.22   1.00     230.     456.\n4 lm_sf_sig…  0.161  0.161 0.00233 0.00244  0.158  0.165  1.00     513.     875.\n5 lm_sf_ome…  0.579  0.578 0.0156  0.0153   0.555  0.608  1.00     341.     576.\n6 lm_sf_ome…  1.62   1.62  0.117   0.112    1.45   1.82   1.01     321.     509.\n7 lm_sf_ome…  0.997  0.993 0.0505  0.0503   0.920  1.08   1.00     390.     595.\n\n\nThis looks good, let’s check the traceplots:\n\n\nShow the code\n# vars_draws &lt;- mcmc_res_cmdstan$draws(vars)\nmcmc_trace(vars_draws)\n\n\n\n\n\n\n\n\n\nThey also look ok, all chains are mixing well in the same range of parameter values.\nAlso here we could look at the pairs plot:\n\n\nShow the code\nmcmc_pairs(\n  vars_draws,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#observation-vs.-model-fit",
    "href": "session-tgi/2_tgi_sf_jmpost.html#observation-vs.-model-fit",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Observation vs. model fit",
    "text": "Observation vs. model fit\nLet’s check the fit of the model to the data:\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\n\nsave_fit_file &lt;- here(\"session-tgi/jm5_fit.RData\")\nif (file.exists(save_fit_file)) {\n  load(save_fit_file)\n} else {\n  fit_subset &lt;- LongitudinalQuantities(\n    mcmc_results, \n    grid = GridObserved(subjects = pt_subset)\n  )\n  save(fit_subset, file = save_fit_file)\n}\n\nautoplot(fit_subset)+\n  labs(x = \"Time (years)\", y = \"SLD (mm)\")\n\n\n\n\n\n\n\n\n\nSo this works very nicely.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#prior-vs.-posterior",
    "href": "session-tgi/2_tgi_sf_jmpost.html#prior-vs.-posterior",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Prior vs. posterior",
    "text": "Prior vs. posterior\nLet’s check the prior vs. posterior for the parameters:\n\n\nShow the code\npost_samples &lt;- as_draws_df(vars_draws) |&gt; \n  rename(\n    mu_bsld = \"lm_sf_mu_bsld[1]\",\n    mu_ks = \"lm_sf_mu_ks[1]\",\n    mu_kg = \"lm_sf_mu_kg[1]\",\n    omega_bsld = \"lm_sf_omega_bsld[1]\",\n    omega_ks = \"lm_sf_omega_ks[1]\",\n    omega_kg = \"lm_sf_omega_kg[1]\",\n    sigma = lm_sf_sigma\n  ) |&gt; \n  mutate(type = \"posterior\") |&gt; \n  select(mu_bsld, mu_ks, mu_kg, omega_bsld, omega_ks, omega_kg, sigma, type)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\nn_prior_samples &lt;- nrow(post_samples)\nprior_samples &lt;- data.frame(\n    mu_bsld = rnorm(n_prior_samples, log(65), 1),\n    mu_ks = rnorm(n_prior_samples, log(0.52), 1),\n    mu_kg = rnorm(n_prior_samples, log(1.04), 1),\n    omega_bsld = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    omega_ks = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    omega_kg = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3),\n    sigma = rtruncnorm(n_prior_samples, a = 0, mean = 0, sd = 3)\n  ) |&gt; \n  mutate(type = \"prior\")\n\n# Combine the two\ncombined_samples &lt;- rbind(post_samples, prior_samples) |&gt; \n  pivot_longer(cols = -type, names_to = \"parameter\", values_to = \"value\")\n\nggplot(combined_samples, aes(x = value, fill = type)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~parameter, scales = \"free\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis looks good, because the priors are covering the range of the posterior samples and are not too informative.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#parameter-estimates",
    "href": "session-tgi/2_tgi_sf_jmpost.html#parameter-estimates",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nHere we need again to be careful: We are interested in the posterior mean estimates of the baseline, shrinkage and growth population rates on the original scale. Because we model them on the log scale as normal distributed, we need to use the mean of the log-normal distribution to get the mean on the original scale.\n\n\nShow the code\npost_sum &lt;- post_samples |&gt;\n  mutate(\n    theta_b0 = exp(mu_bsld + omega_bsld^2 / 2), \n    theta_ks = exp(mu_ks + omega_ks^2 / 2), \n    theta_kg = exp(mu_kg + omega_kg^2 / 2),\n    cv_0 = sqrt(exp(omega_bsld^2) - 1),\n    cv_s = sqrt(exp(omega_ks^2) - 1),\n    cv_g = sqrt(exp(omega_kg^2) - 1)\n  ) |&gt;\n  select(theta_b0, theta_ks, theta_kg, omega_bsld, omega_ks, omega_kg, cv_0, cv_s, cv_g, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.7\n43.8\n1.10\n1.12\n42.0\n45.5\n1.00\n151\n201\n\n\ntheta_ks\n1.06\n1.05\n0.129\n0.120\n0.871\n1.30\n1.00\n226\n408\n\n\ntheta_kg\n0.428\n0.427\n0.0311\n0.0306\n0.378\n0.479\n1.00\n318\n472\n\n\nomega_bsld\n0.579\n0.578\n0.0156\n0.0153\n0.555\n0.608\n1.00\n346\n545\n\n\nomega_ks\n1.62\n1.62\n0.117\n0.112\n1.45\n1.82\n1.00\n318\n494\n\n\nomega_kg\n0.997\n0.993\n0.0505\n0.0503\n0.920\n1.08\n1.00\n390\n576\n\n\ncv_0\n0.631\n0.630\n0.0200\n0.0196\n0.600\n0.668\n1.00\n346\n545\n\n\ncv_s\n3.70\n3.57\n0.818\n0.685\n2.66\n5.16\n1.00\n318\n494\n\n\ncv_g\n1.31\n1.30\n0.106\n0.103\n1.15\n1.49\n1.00\n390\n576\n\n\nsigma\n0.161\n0.161\n0.00233\n0.00244\n0.158\n0.165\n1.00\n495\n844\n\n\n\n\n\n\n\nWe can see that these are consistent with the estimates from the brms model earlier.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#separate-arm-estimates",
    "href": "session-tgi/2_tgi_sf_jmpost.html#separate-arm-estimates",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Separate arm estimates",
    "text": "Separate arm estimates\nWhile there is no general covariates support in jmpost for the longitudinal models as of now, we can obtain separate estimates for the longitudinal model parameters: As detailed in the model specification here, as soon as we have the arm defined, then separate estimates for the arm-specific shrinkage and growth parameters will be obtained: Both the population means and standard deviation parameters are here arm-specific. (Note that this is slightly different from brms where we assumed earlier the same standard deviation independent of the treatment arm.)\nSo we need to define the subject data accordingly now with the arm information:\n\n\nShow the code\nsubj_df_by_arm &lt;- df |&gt;\n  select(id, arm) |&gt;\n  distinct() |&gt; \n  mutate(study = \"study\")\n\nsubj_data_by_arm &lt;- DataSubject(\n  data = subj_df_by_arm,\n  subject = \"id\",\n  arm = \"arm\",\n  study = \"study\"\n)\n\n\nWe redefine the JointData object and can then fit the model, because the prior specification does not need to change: We assume iid priors on the arm-specific parameters here.\n\n\nShow the code\njoint_data_by_arm &lt;- DataJoint(\n    subject = subj_data_by_arm,\n    longitudinal = long_data\n)\n\n\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/jm6.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  mcmc_results_by_arm &lt;- sampleStanModel(\n      tgi_mod,\n      data = joint_data_by_arm,\n      iter_sampling = ITER,\n      iter_warmup = WARMUP,\n      chains = CHAINS,\n      parallel_chains = CHAINS,\n      thin = CHAINS,\n      seed = BAYES.SEED,\n      refresh = REFRESH\n  )\n  save(mcmc_results_by_arm, file = save_file)\n}\n\n\nLet’s again check the convergence:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nsave_arm_file &lt;- here(\"session-tgi/jm6_more.RData\")\nif (file.exists(save_arm_file)) {\n  load(save_arm_file)\n} else {\n  mcmc_res_cmdstan_by_arm &lt;- cmdstanr::as.CmdStanMCMC(mcmc_results_by_arm)\n  mcmc_res_sum_by_arm &lt;- mcmc_res_cmdstan_by_arm$summary(vars)\n  vars_draws_by_arm &lt;- mcmc_res_cmdstan_by_arm$draws(vars)\n  loo_by_arm &lt;- mcmc_res_cmdstan_by_arm$loo(r_eff = FALSE)\n  save(mcmc_res_sum_by_arm, vars_draws_by_arm, loo_by_arm, file = save_arm_file)\n}\nmcmc_res_sum_by_arm\n\n\n# A tibble: 11 × 10\n   variable    mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu…  3.61   3.61  0.0211  0.0217   3.58   3.65  1.00      244.     502.\n 2 lm_sf_mu… -0.752 -0.730 0.222   0.216   -1.14  -0.398 1.01      592.     673.\n 3 lm_sf_mu… -1.52  -1.51  0.222   0.217   -1.90  -1.17  1.00      706.     850.\n 4 lm_sf_mu… -1.11  -1.11  0.138   0.139   -1.35  -0.900 1.00      594.     814.\n 5 lm_sf_mu… -1.35  -1.35  0.103   0.104   -1.52  -1.19  1.00      560.     776.\n 6 lm_sf_si…  0.161  0.161 0.00224 0.00228  0.157  0.165 1.00      892.     941.\n 7 lm_sf_om…  0.577  0.577 0.0159  0.0153   0.551  0.603 1.00      511.     672.\n 8 lm_sf_om…  1.34   1.33  0.150   0.152    1.11   1.60  1.00      630.     844.\n 9 lm_sf_om…  1.76   1.76  0.161   0.150    1.51   2.05  1.00      662.     990.\n10 lm_sf_om…  0.765  0.758 0.0710  0.0685   0.662  0.891 0.999     809.     951.\n11 lm_sf_om…  1.10   1.10  0.0705  0.0684   0.991  1.22  0.999     669.     736.\n\n\nLet’s again tabulate the parameter estimates:\n\n\nShow the code\npost_samples_by_arm &lt;- as_draws_df(vars_draws_by_arm) |&gt; \n  rename(\n    mu_bsld = \"lm_sf_mu_bsld[1]\",\n    mu_ks1 = \"lm_sf_mu_ks[1]\",\n    mu_ks2 = \"lm_sf_mu_ks[2]\",\n    mu_kg1 = \"lm_sf_mu_kg[1]\",\n    mu_kg2 = \"lm_sf_mu_kg[2]\",\n    omega_bsld = \"lm_sf_omega_bsld[1]\",\n    omega_ks1 = \"lm_sf_omega_ks[1]\",\n    omega_ks2 = \"lm_sf_omega_ks[2]\",\n    omega_kg1 = \"lm_sf_omega_kg[1]\",\n    omega_kg2 = \"lm_sf_omega_kg[2]\",\n    sigma = lm_sf_sigma\n  ) |&gt; \n  mutate(\n    theta_b0 = exp(mu_bsld + omega_bsld^2 / 2), \n    theta_ks1 = exp(mu_ks1 + omega_ks1^2 / 2), \n    theta_ks2 = exp(mu_ks2 + omega_ks2^2 / 2),\n    theta_kg1 = exp(mu_kg1 + omega_kg1^2 / 2),\n    theta_kg2 = exp(mu_kg2 + omega_kg2^2 / 2),\n    cv_0 = sqrt(exp(omega_bsld^2) - 1),\n    cv_s1 = sqrt(exp(omega_ks1^2) - 1),\n    cv_s2 = sqrt(exp(omega_ks2^2) - 1),\n    cv_g1 = sqrt(exp(omega_kg1^2) - 1),\n    cv_g2 = sqrt(exp(omega_kg2^2) - 1)\n  ) \n  \npost_sum_by_arm &lt;- post_samples_by_arm |&gt;\n  select(\n    theta_b0, theta_ks1, theta_ks2, theta_kg1, theta_kg2, \n    omega_bsld, omega_ks1, omega_ks2, omega_kg1, omega_kg2, \n    cv_0, cv_s1, cv_s2, cv_g1, cv_g2, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum_by_arm\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.7\n43.7\n0.999\n1.05\n42.1\n45.4\n1.00\n227\n558\n\n\ntheta_ks1\n1.18\n1.17\n0.152\n0.146\n0.961\n1.44\n1.00\n530\n696\n\n\ntheta_ks2\n1.07\n1.04\n0.209\n0.183\n0.779\n1.42\n1.00\n344\n443\n\n\ntheta_kg1\n0.444\n0.443\n0.0490\n0.0489\n0.365\n0.527\n1.00\n539\n712\n\n\ntheta_kg2\n0.479\n0.478\n0.0487\n0.0479\n0.399\n0.563\n1.00\n735\n908\n\n\nomega_bsld\n0.577\n0.577\n0.0159\n0.0153\n0.551\n0.603\n1.00\n504\n659\n\n\nomega_ks1\n1.34\n1.33\n0.150\n0.152\n1.11\n1.60\n1.00\n620\n838\n\n\nomega_ks2\n1.76\n1.76\n0.161\n0.150\n1.51\n2.05\n1.00\n651\n972\n\n\nomega_kg1\n0.765\n0.758\n0.0710\n0.0685\n0.662\n0.891\n0.999\n799\n943\n\n\nomega_kg2\n1.10\n1.10\n0.0705\n0.0684\n0.991\n1.22\n1.00\n649\n693\n\n\ncv_0\n0.629\n0.628\n0.0205\n0.0196\n0.596\n0.662\n1.00\n504\n659\n\n\ncv_s1\n2.33\n2.22\n0.601\n0.547\n1.55\n3.46\n1.00\n620\n838\n\n\ncv_s2\n4.90\n4.58\n1.64\n1.25\n2.96\n8.11\n1.00\n651\n972\n\n\ncv_g1\n0.897\n0.880\n0.112\n0.105\n0.742\n1.10\n0.999\n799\n943\n\n\ncv_g2\n1.55\n1.54\n0.175\n0.163\n1.29\n1.85\n1.00\n649\n693\n\n\nsigma\n0.161\n0.161\n0.00224\n0.00228\n0.157\n0.165\n1.00\n885\n881\n\n\n\n\n\n\n\nHere again the shrinkage rate in the treatment arm 1 seems higher than in the treatment arm 2. However, the difference is not as pronounced as in the brms model before with the same standard deviation for both arms. We can again calculate the posterior probability that the shrinkage rate in arm 1 is higher than in arm 2:\n\n\nShow the code\nprob_ks1_greater_ks2 &lt;- mean(post_samples_by_arm$theta_ks1 &gt; post_samples_by_arm$theta_ks2)\nprob_ks1_greater_ks2\n\n\n[1] 0.706\n\n\nSo the posterior probability is now only around 71%.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#model-comparison-with-loo",
    "href": "session-tgi/2_tgi_sf_jmpost.html#model-comparison-with-loo",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Model comparison with LOO",
    "text": "Model comparison with LOO\nAs we have seen for brms, also for jmpost we can easily compute the LOO criterion:\n\n\nShow the code\n# loo_res &lt;- mcmc_res_cmdstan$loo(r_eff = FALSE)\nloo_res\n\n\n\nComputed from 1000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12957.3  95.8\np_loo      1137.1  38.4\nlooic     25914.5 191.6\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nPareto k diagnostic values:\n                          Count Pct.    Min. ESS\n(-Inf, 0.67]   (good)     3665  89.4%   26      \n   (0.67, 1]   (bad)       363   8.9%   &lt;NA&gt;    \n    (1, Inf)   (very bad)   71   1.7%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nUnderneath this is using the $loo() method from cmdstanr.\nAnd we can compare this to the LOO of the model with separate arm estimates:\n\n\nShow the code\n# loo_by_arm &lt;- mcmc_res_cmdstan_by_arm$loo(r_eff = FALSE)\nloo_by_arm\n\n\n\nComputed from 1000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12931.8  95.6\np_loo      1121.5  36.8\nlooic     25863.5 191.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nPareto k diagnostic values:\n                          Count Pct.    Min. ESS\n(-Inf, 0.67]   (good)     3652  89.1%   48      \n   (0.67, 1]   (bad)       374   9.1%   &lt;NA&gt;    \n    (1, Inf)   (very bad)   73   1.8%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nSo the model by treatment arm performs here better than the model without treatment arm specific growth and shrinkage parameters.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/2_tgi_sf_jmpost.html#tipps-and-tricks",
    "href": "session-tgi/2_tgi_sf_jmpost.html#tipps-and-tricks",
    "title": "2. TGI model minimal workflow with jmpost",
    "section": "Tipps and tricks",
    "text": "Tipps and tricks\n\nAlso here it is possible to look at the underlying Stan code:\n\n\nShow the code\ntmp &lt;- tempfile()\nwrite_stan(tgi_mod, destination = tmp)\nfile.edit(tmp) # opens the Stan file in the default editor\n\n\nIt is not trivial to transport saved models from one computer to another. This is because cmdstanr only loads the results it currently needs from disk into memory, and thus into the R session. If you want to transport the model to another computer, you need to save the Stan code and the data, and then re-run the model on the other computer. This is because the model object in R is only a reference to the model on disk, not the model itself. Note that there is the $save_object() method, see here, however this leads to very large files (here about 300 MB for one fit) and can thus not be uploaded to typical git repositories. Therefore above we saved interim result objects separately as needed.\nIt is important to explicitly define the truncation boundaries for the truncated normal priors, because otherwise the MCMC results will not be correct.",
    "crumbs": [
      "Session 1: TGI",
      "2. TGI model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html",
    "href": "session-tgi/1_tgi_sf_brms.html",
    "title": "1. TGI model minimal workflow with brms",
    "section": "",
    "text": "The purpose of this document is to show a minimal workflow for fitting a TGI model using the brms package.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#setup-and-load-data",
    "href": "session-tgi/1_tgi_sf_brms.html#setup-and-load-data",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Setup and load data",
    "text": "Setup and load data\nFirst we need to load the necessary packages and set some default options for the MCMC sampling. We also set the theme for the plots to theme_bw with a base size of 12.\n\n\nShow the code\nlibrary(bayesplot)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(here)\nlibrary(janitor)\nlibrary(jmpost)\nlibrary(modelr)\nlibrary(posterior)\nlibrary(readxl)\nlibrary(rstan)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(truncnorm)\n\nif (require(cmdstanr)) {\n  # If cmdstanr is available, instruct brms to use cmdstanr as backend \n  # and cache all Stan binaries\n  options(brms.backend = \"cmdstanr\", cmdstanr_write_stan_file_dir = here(\"_brms-cache\"))\n  dir.create(here(\"_brms-cache\"), FALSE) # create cache directory if not yet available\n} else {\n  rstan::rstan_options(auto_write = TRUE)\n}\n\n# MCMC options\noptions(mc.cores = 4)\nITER &lt;- 1000 # number of sampling iterations after warm up\nWARMUP &lt;- 2000 # number of warm up iterations\nCHAINS &lt;- 4\nBAYES.SEED &lt;- 878\nREFRESH &lt;- 500\n\ntheme_set(theme_bw(base_size = 12))\n\n\nWe also need a small function definition, which is still missing in brms:\n\n\nShow the code\nint_step &lt;- function(x) {\n  stopifnot(is.logical(x))\n  ifelse(x, 1, 0)\n}\n\n\nWe will use the publicly published tumor size data from the OAK study, see here. In particular we are using the S1 data set, which is the fully anonymized data set used in the publication. For simplicity, we have copied the data set in this GitHub repository.\n\n\nShow the code\nfile_path &lt;- here(\"data/journal.pcbi.1009822.s006.xlsx\")\n\nread_one_sheet &lt;- function(sheet) {\n    read_excel(file_path, sheet = sheet) |&gt; \n    clean_names() |&gt; \n    mutate(\n        id = factor(as.character(patient_anonmyized)),\n        day = as.integer(treatment_day),\n        year = day / 365.25,\n        target_lesion_long_diam_mm = case_match(\n            target_lesion_long_diam_mm,\n            \"TOO SMALL TO MEASURE\" ~ \"2\",\n            \"NOT EVALUABLE\" ~ NA_character_,\n            .default = target_lesion_long_diam_mm\n        ),\n        sld = as.numeric(target_lesion_long_diam_mm),\n        sld = ifelse(sld == 0, 2, sld),\n        study = factor(gsub(\"^Study_(\\\\d+)_Arm_\\\\d+$\", \"\\\\1\", study_arm)),\n        arm = factor(gsub(\"^Study_\\\\d+_Arm_(\\\\d+)$\", \"\\\\1\", study_arm))\n    ) |&gt; \n    select(id, year, sld, study, arm)\n}\n\ntumor_data &lt;- excel_sheets(file_path) |&gt; \n    map(read_one_sheet) |&gt; \n    bind_rows()\n\nhead(tumor_data)\n\n\n# A tibble: 6 × 5\n  id                      year   sld study arm  \n  &lt;fct&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n1 3657015667902160896 -0.00548    33 1     1    \n2 3657015667902160896  0.101      33 1     1    \n3 3657015667902160896  0.230      32 1     1    \n4 3657015667902160896  0.342      37 1     1    \n5 3657015667902160896  0.446      49 1     1    \n6 2080619628198763008 -0.00548    12 1     1    \n\n\nShow the code\nsummary(tumor_data)\n\n\n                    id            year              sld        study   \n 4308512445673410048 :  18   Min.   :-0.1314   Min.   :  2.0   1: 456  \n -4902532987801034752:  17   1st Qu.: 0.1123   1st Qu.: 17.0   2: 966  \n 5394902984416364544 :  16   Median : 0.2847   Median : 30.0   3:2177  \n 7160320731596789760 :  16   Mean   : 0.3822   Mean   : 36.1   4:4126  \n -4159496062492130816:  15   3rd Qu.: 0.5722   3rd Qu.: 49.0   5: 835  \n -7900338178541499392:  15   Max.   : 2.0753   Max.   :228.0           \n (Other)             :8463   NA's   :1         NA's   :69              \n arm     \n 1:3108  \n 2:3715  \n 3: 291  \n 4: 608  \n 5: 227  \n 6: 611  \n         \n\n\nFor simplicity, we will for now just use study 4 (this is the OAK study), and we rename the patient IDs:\n\n\nShow the code\ndf &lt;- tumor_data |&gt; \n  filter(study == \"4\") |&gt; \n  na.omit() |&gt;\n  droplevels() |&gt; \n  mutate(id = factor(as.numeric(id)))\n\n\nHere we have 701 patients. It is always a good idea to make a plot of the data. Let’s look at the first 20 patients e.g.:\n\n\nShow the code\ndf |&gt; \n  filter(as.integer(id) &lt;= 20) |&gt;\n  ggplot(aes(x = year, y = sld, group = id)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ id) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#stein-fojo-model",
    "href": "session-tgi/1_tgi_sf_brms.html#stein-fojo-model",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Stein-Fojo model",
    "text": "Stein-Fojo model\nWe start from the Stein-Fojo model as shown in the slides:\n\\[\ny^{*}(t_{ij}) = \\psi_{b_{0}i} \\{\\exp(- \\psi_{k_{s}i} \\cdot t_{ij}) + \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) - 1\\}\n\\]\n\nMean\nWe will make one more tweak here. If the time \\(t\\) is negative, i.e. the treatment has not started yet, then it is reasonable to assume that the tumor cannot shrink yet. Therefore, the final model for the mean SLD is:\n\\[\ny^{*}(t_{ij}) =\n\\begin{cases}\n\\psi_{b_{0}i} \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) & \\text{if } t_{ij} &lt; 0 \\\\\n\\psi_{b_{0}i} \\{\\exp(-\\psi_{k_{s}i} \\cdot t_{ij}) + \\exp(\\psi_{k_{g}i} \\cdot t_{ij}) - 1\\} & \\text{if } t_{ij} \\geq 0\n\\end{cases}\n\\]\n\n\nLikelihood\nFor the likelihood given the mean SLD \\(y^{*}\\), we will assume a normal distribution with a constant coefficient of variation \\(\\tau\\):\n\\[\ny(t_{ij}) \\sim \\text{N}(y^{*}(t_{ij}), y^{*}(t_{ij})\\tau)\n\\]\nNote that for consistency with the brms and Stan convention, here we denote the standard deviation as the second parameter of the normal distribution. So in this case, the variance would be \\((y^{*}(t_{ij})\\tau)^2\\).\nThis can also be written as:\n\\[\ny(t_{ij}) = (1 + \\epsilon_{ij}) \\cdot y^{*}(t_{ij})\n\\]\nwhere \\(\\epsilon_{ij} \\sim \\text{N}(0, \\tau)\\).\nNote that also the additive model is a possible choice, where\n\\[\ny(t_{ij}) = y^{*}(t_{ij}) + \\epsilon_{ij}\n\\]\nsuch that the error does not depend on the scale of the SLD any longer.\n\n\nRandom effects\nNext, we define the distributions of the random effects \\(\\psi_{b_{0}i}\\), \\(\\psi_{k_{s}i}\\), \\(\\psi_{k_{g}i}\\), for \\(i = 1, \\dotsc, n\\):\n\\[\n\\begin{align*}\n\\psi_{b_{0}i} &\\sim \\text{LogNormal}(\\mu_{b_{0}}, \\omega_{0}) \\\\\n\\psi_{k_{s}i} &\\sim \\text{LogNormal}(\\mu_{k_{s}}, \\omega_{s}) \\\\\n\\psi_{k_{g}i} &\\sim \\text{LogNormal}(\\mu_{k_{g}}, \\omega_{g})\n\\end{align*}\n\\]\nThis can be rewritten as:\n\\[\n\\begin{align*}\n\\psi_{b_{0}i} &= \\exp(\\mu_{b_{0}} + \\omega_{0} \\cdot \\eta_{b_{0}i}) \\\\\n\\psi_{k_{s}i} &= \\exp(\\mu_{k_{s}} + \\omega_{s} \\cdot \\eta_{k_{s}i}) \\\\\n\\psi_{k_{g}i} &= \\exp(\\mu_{k_{g}} + \\omega_{g} \\cdot \\eta_{k_{g}i})\n\\end{align*}\n\\]\nwhere \\(\\eta_{b_{0}i}\\), \\(\\eta_{k_{s}i}\\), \\(\\eta_{k_{g}i}\\) are the standard normal distributed random effects.\nThis is important for two reasons:\n\nThis parametrization can help the sampler to converge faster. See here for more information.\nThis shows a bit more explicitly that the population mean of the random effects is not equal to the \\(\\mu\\) parameter, but to \\(\\theta = \\exp(\\mu + \\omega^2 / 2)\\), because they are log-normally distributed (see e.g. Wikipedia for the formulas). This is important for the interpretation of the parameter estimates.\n\n\n\nPriors\nFinally, we need to define the priors for the hyperparameters.\nThere are different principles we could use to define these priors:\n\nNon-informative priors: We could use priors that are as non-informative as possible. This is especially useful if we do not have any prior knowledge about the parameters. For example, we could use normal priors with a large standard deviation for the population means of the random effects.\nInformative priors: If we have some prior knowledge about the parameters, we can use this to define the priors. For example, if we have literature data about the \\(k_g\\) parameter estimate, we could use this to define the prior for the population mean of the growth rate. (Here we just need to be careful to consider the time scale and the log-normal distribution, as mentioned above)\n\nHere we use relatively informative priors for the log-normal location parameters, motivated by prior analyses of the same study:\n\\[\n\\begin{align*}\n\\mu_{b_{0}} &\\sim \\text{Normal}(\\log(65), 1) \\\\\n\\mu_{k_{s}} &\\sim \\text{Normal}(\\log(0.52), 0.1) \\\\\n\\mu_{k_{g}} &\\sim \\text{Normal}(\\log(1.04), 1)\n\\end{align*}\n\\]\nFor all standard deviations we use truncated normal priors:\n\\[\n\\begin{align*}\n\\omega_{0} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\omega_{s} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\omega_{g} &\\sim \\text{PositiveNormal}(0, 3) \\\\\n\\tau &\\sim \\text{PositiveNormal}(0, 3)\n\\end{align*}\n\\]\nwhere \\(\\text{PositiveNormal}(0, 3)\\) denotes a truncated normal distribution with mean \\(0\\) and standard deviation \\(3\\), truncated to the positive real numbers.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#fit-model",
    "href": "session-tgi/1_tgi_sf_brms.html#fit-model",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Fit model",
    "text": "Fit model\nWe can now fit the model using brms. The structure is determined by the model formula:\n\n\nShow the code\nformula &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood:\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (exp(-ks * year) + exp(kg * year) -  1)) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # Define the standard deviation (called sigma in brms) as a \n  # coefficient tau times the mean ystar.\n  # sigma is modelled on the log scale though, therefore:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  # This line is needed to declare tau as a model parameter:\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + (1 | id)) +\n  lf(lkg ~ 1 + (1 | id))\n\n# Define the priors\npriors &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\"),\n  prior(normal(log(1.04), 1), nlpar = \"lkg\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\n\n# Initial values to avoid problems at the beginning\nn_patients &lt;- nlevels(df$id)\ninits &lt;- list(\n  b_lb0 = array(3.61),\n  b_lks = array(-1.25),\n  b_lkg = array(-1.33),\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n# Fit the model\nsave_file &lt;- here(\"session-tgi/fit9.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit, file = save_file)\n}\n\n# Summarize the fit\nsummary(fit)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.00      689     1248\nsd(lks_Intercept)     1.40      0.08     1.25     1.55 1.00      702     1586\nsd(lkg_Intercept)     0.96      0.04     0.88     1.05 1.01      902     1750\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.16      0.00     0.16     0.17 1.00     2250     2892\nlb0_Intercept     3.62      0.02     3.57     3.66 1.01      420      723\nlks_Intercept    -0.86      0.08    -1.01    -0.72 1.00     1661     2415\nlkg_Intercept    -1.20      0.07    -1.34    -1.08 1.01      676     1419\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote that it is crucial to use here int_step() to properly define the two pieces of the linear predictor for negative and non-negative time values: If you used step() like I did for a few days, then you will have the wrong model! This is because in Stan, step(false) = step(0) = 1 and not 0 as you would expect. Only int_step(0) = 0 as we need it here.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#parameter-estimates",
    "href": "session-tgi/1_tgi_sf_brms.html#parameter-estimates",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nHere we extract all parameter estimates using the as_draws_df method for brmsfit objects. Note that we could also just extract a subset of parameters, see ?as_draws.brmsfit for more information.\nAs mentioned above, we use the expectation of the log-normal distribution to get the population level estimates (called \\(\\theta\\) with the corresponding subscript) for the \\(b_0\\), \\(k_s\\), and \\(k_g\\) parameters. We also calculate the coefficient of variation for each parameter.\n\n\nShow the code\npost_df &lt;- as_draws_df(fit)\nhead(names(post_df), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"r_id__lb0[1,Intercept]\" \"r_id__lb0[2,Intercept]\"\n[10] \"r_id__lb0[3,Intercept]\"\n\n\nShow the code\npost_df &lt;- post_df |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_kg = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\n\nFor a graphical check of the convergence, we can use the mcmc_trace and mcmc_pairs functions from the bayesplot package:\n\n\nShow the code\nsf_pop_params &lt;- c(\"theta_b0\", \"theta_ks\", \"theta_kg\", \"omega_0\", \"omega_s\", \"omega_g\", \"sigma\")\n\nmcmc_trace(post_df, pars = sf_pop_params)\n\n\n\n\n\n\n\n\n\nShow the code\nmcmc_pairs(\n  post_df, \n  pars = sf_pop_params,\n  off_diag_args = list(size = 1, alpha = 0.1)\n)\n\n\n\n\n\n\n\n\n\nNext, we can create a nice summary table of the parameter estimates, using summarize_draws from the posterior package in combination with functions from the gt package:\n\n\nShow the code\npost_sum &lt;- post_df |&gt;\n  select(theta_b0, theta_ks, theta_kg, omega_0, omega_s, omega_g, cv_0, cv_s, cv_g, sigma) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n44.0\n44.0\n1.03\n1.01\n42.3\n45.7\n1.00\n423\n726\n\n\ntheta_ks\n1.13\n1.12\n0.104\n0.101\n0.972\n1.32\n1.00\n445\n915\n\n\ntheta_kg\n0.476\n0.475\n0.0311\n0.0318\n0.426\n0.528\n1.00\n941\n1,320\n\n\nomega_0\n0.579\n0.579\n0.0162\n0.0166\n0.552\n0.605\n1.00\n676\n1,220\n\n\nomega_s\n1.40\n1.40\n0.0753\n0.0745\n1.28\n1.52\n1.00\n699\n1,560\n\n\nomega_g\n0.958\n0.956\n0.0446\n0.0446\n0.888\n1.04\n1.00\n882\n1,720\n\n\ncv_0\n0.631\n0.631\n0.0208\n0.0212\n0.597\n0.665\n1.00\n676\n1,220\n\n\ncv_s\n2.49\n2.46\n0.311\n0.295\n2.02\n3.04\n1.00\n699\n1,560\n\n\ncv_g\n1.23\n1.22\n0.0883\n0.0869\n1.10\n1.39\n1.00\n882\n1,720\n\n\nsigma\n0.161\n0.161\n0.00230\n0.00229\n0.157\n0.165\n1.00\n2,190\n2,860\n\n\n\n\n\n\n\nWe can also look at parameters for individual patients. This is simplified by using the spread_draws() function:\n\n\nShow the code\n# Understand the names of the random effects:\nhead(get_variables(fit), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lkg_Intercept\"        \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"  \n [7] \"sd_id__lkg_Intercept\"   \"r_id__lb0[1,Intercept]\" \"r_id__lb0[2,Intercept]\"\n[10] \"r_id__lb0[3,Intercept]\"\n\n\nShow the code\npost_ind &lt;- fit |&gt; \n  spread_draws(\n    b_lb0_Intercept,\n    r_id__lb0[id, ],\n    b_lks_Intercept,\n    r_id__lks[id, ],\n    b_lkg_Intercept,\n    r_id__lkg[id, ]\n  ) |&gt; \n  mutate(\n    b0 = exp(b_lb0_Intercept + r_id__lb0),\n    ks = exp(b_lks_Intercept + r_id__lks),\n    kg = exp(b_lkg_Intercept + r_id__lkg)\n  ) |&gt; \n  select(\n    .chain, .iteration, .draw,\n    id, b0, ks, kg    \n  )\n\n\nNote that here we do not need to use the log-normal expectation formula, because we just calculate the individual random effects here, in contrast to the population level parameters above.\nWith this we can e.g. report the estimates for the first patient:\n\n\nShow the code\npost_sum_id1 &lt;- post_ind |&gt;\n  filter(id == \"1\") |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_sum_id1\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\n1\n\n\nb0\n43.3\n42.8\n4.80\n4.35\n36.5\n52.2\n1.00\n9,080\n2,890\n\n\nks\n0.486\n0.313\n0.529\n0.309\n0.0392\n1.50\n1.00\n6,300\n3,140\n\n\nkg\n0.395\n0.294\n0.338\n0.241\n0.0620\n1.04\n1.00\n7,690\n3,080",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#observation-vs-model-fit",
    "href": "session-tgi/1_tgi_sf_brms.html#observation-vs-model-fit",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nWe can now compare the model fit to the observations. Let’s do this for the first 20 patients again.\n\n\nShow the code\npt_subset &lt;- as.character(1:20)\ndf_subset &lt;- df |&gt; \n  filter(id %in% pt_subset)\n\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_linpred_draws(fit) |&gt; \n  median_qi() |&gt; \n  mutate(sld_pred = .linpred)\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = sld_pred, ymin = .lower, ymax = .upper),\n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = sld_pred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"SF model fit\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#summary-statistics",
    "href": "session-tgi/1_tgi_sf_brms.html#summary-statistics",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Summary statistics",
    "text": "Summary statistics\nThe time-to-growth formula is:\n\\[\n\\max \\left(\n    \\frac{\n        \\log(k_s) - \\log(k_g)\n    }{\n        k_s + k_g\n    },\n    0\n\\right)\n\\]\nSimilary, we can look at the tumor-ratio at time \\(t\\):\n\\[\n\\frac{y^{*}(t)}{y^{*}(0)} = \\exp(-k_s \\cdot t) + \\exp(k_g \\cdot t) - 1\n\\]\nSo with the posterior samples from above, we can calculate these statistics, separate for each patient, with the tumor-ratio e.g. for 12 weeks (and being careful with the year time scale we used in this model):\n\n\nShow the code\npost_ind_stat &lt;- post_ind |&gt; \n  mutate(\n    ttg = pmax((log(ks) - log(kg)) / (ks + kg), 0),\n    tr12 = exp(-ks * 12/52) + exp(kg * 12/52) - 1\n  )\n\n\nThen we can look at e.g. the first 3 patients parameter estimates:\n\n\nShow the code\npost_ind_stat_sum &lt;- post_ind_stat |&gt;\n  filter(id %in% c(\"1\", \"2\", \"3\")) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\npost_ind_stat_sum\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\n1\n\n\nb0\n43.3\n42.8\n4.80\n4.35\n36.5\n52.2\n1.00\n9,080\n2,890\n\n\nks\n0.486\n0.313\n0.529\n0.309\n0.0392\n1.50\n1.00\n6,300\n3,140\n\n\nkg\n0.395\n0.294\n0.338\n0.241\n0.0620\n1.04\n1.00\n7,690\n3,080\n\n\nttg\n0.785\n0.106\n1.36\n0.158\n0\n3.47\n1.00\n4,440\n3,180\n\n\ntr12\n0.999\n1.00\n0.115\n0.0834\n0.809\n1.18\n1.00\n5,740\n3,400\n\n\n2\n\n\nb0\n37.9\n37.6\n4.10\n3.79\n31.9\n45.1\n1.00\n7,520\n2,750\n\n\nks\n0.396\n0.246\n0.491\n0.237\n0.0355\n1.21\n1.00\n5,330\n3,380\n\n\nkg\n0.548\n0.416\n0.445\n0.357\n0.0735\n1.42\n1.00\n5,860\n2,870\n\n\nttg\n0.501\n0\n1.21\n0\n0\n2.92\n0.999\n3,500\n2,820\n\n\ntr12\n1.06\n1.04\n0.125\n0.0939\n0.886\n1.29\n1.00\n5,810\n3,220\n\n\n3\n\n\nb0\n21.0\n20.7\n2.49\n2.23\n17.6\n25.6\n1.00\n4,310\n2,900\n\n\nks\n1.00\n0.817\n0.662\n0.541\n0.229\n2.34\n1.00\n2,930\n2,760\n\n\nkg\n0.275\n0.249\n0.165\n0.185\n0.0529\n0.575\n1.00\n3,480\n2,790\n\n\nttg\n1.50\n1.01\n1.39\n0.681\n0.418\n4.27\n1.00\n4,440\n2,740\n\n\ntr12\n0.869\n0.884\n0.0815\n0.0720\n0.709\n0.980\n1.00\n3,170\n2,920",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#adding-covariates",
    "href": "session-tgi/1_tgi_sf_brms.html#adding-covariates",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Adding covariates",
    "text": "Adding covariates\nWith brms it is straightforward to add covariates to the model. In this example, an obvious choice for a covariate is the treatment the patients received in the study:\n\n\nShow the code\ndf |&gt; \n  select(id, arm) |&gt; \n  distinct() |&gt;\n  pull(arm) |&gt; \n  table()\n\n\n\n  1   2 \n325 376 \n\n\nHere it makes sense to assume that only the shrinkage and growth parameters differ systematically between the two arms. For example, the baseline SLD should be the same in expectation, because of the randomization between the treatment arms.\nTherefore we can add this binary arm covariate as follows as a fixed effect for the two population level mean parameters \\(\\mu_{k_s}\\) and \\(\\mu_{k_g}\\) on the log scale:\n\n\nShow the code\nformula_by_arm &lt;- bf(sld ~ ystar, nl = TRUE) +\n  # Define the mean for the likelihood:\n  nlf(\n    ystar ~ \n      int_step(year &gt; 0) * \n        (b0 * (exp(-ks * year) + exp(kg * year) -  1)) +\n      int_step(year &lt;= 0) * \n        (b0 * exp(kg * year))\n  ) +\n  # As above we also use these formulas here:\n  nlf(sigma ~ log(tau) + log(ystar)) +\n  lf(tau ~ 1) +\n  # Define nonlinear parameter transformations:\n  nlf(b0 ~ exp(lb0)) +\n  nlf(ks ~ exp(lks)) +\n  nlf(kg ~ exp(lkg)) +\n  # Define random effect structure:\n  lf(lb0 ~ 1 + (1 | id)) + \n  lf(lks ~ 1 + arm + (1 | id)) +\n  lf(lkg ~ 1 + arm + (1 | id))\n\n\nIt is instructive to see that the Stan code that is generated in the background is actually identical to the previous model, except for the prior specification. This is because brms uses a design matrix to model the fixed effects, and the arm variable is just added to this design matrix, which before only contained the intercept column with 1s.\nHowever, now the coefficient vector is no longer of length 1 but of length 2, with the first element corresponding to the intercept and the second element to the effect of the arm variable (for the level “2”). For the latter, we want to assume a standard normal prior on the log scale.\nSo we need to adjust the prior and initial values accordingly:\n\n\nShow the code\npriors_by_arm &lt;- c(\n  prior(normal(log(65), 1), nlpar = \"lb0\"),\n  # Note the changes here:\n  prior(normal(log(0.52), 0.1), nlpar = \"lks\", coef = \"Intercept\"),\n  prior(normal(0, 1), nlpar = \"lks\", coef = \"arm2\"), \n  prior(normal(log(1.04), 1), nlpar = \"lkg\", coef = \"Intercept\"),\n  prior(normal(0, 1), nlpar = \"lkg\", coef = \"arm2\"),\n  # Same as before:\n  prior(normal(0, 3), lb = 0, nlpar = \"lb0\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lks\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"lkg\", class = \"sd\"),\n  prior(normal(0, 3), lb = 0, nlpar = \"tau\")\n)\ninits_by_arm &lt;- list(\n  b_lb0 = array(3.61),\n  # Note the changes here:\n  b_lks = array(c(-1.25, 0)),\n  b_lkg = array(c(-1.33, 0)),\n  # Same as before:\n  sd_1 = array(0.58),\n  sd_2 = array(1.6),\n  sd_3 = array(0.994),\n  b_tau = array(0.161),\n  z_1 = matrix(0, nrow = 1, ncol = n_patients),\n  z_2 = matrix(0, nrow = 1, ncol = n_patients),\n  z_3 = matrix(0, nrow = 1, ncol = n_patients)\n)\n\n\nNow we can fit the model as before:\n\n\nShow the code\nsave_file &lt;- here(\"session-tgi/fit10.RData\")\nif (file.exists(save_file)) {\n  load(save_file)\n} else {\n  fit_by_arm &lt;- brm(\n    formula = formula_by_arm,\n    data = df,\n    prior = priors_by_arm,\n    family = gaussian(),\n    init = rep(list(inits_by_arm), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH\n  )\n  save(fit_by_arm, file = save_file)\n}\nsummary(fit_by_arm)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ eta \n         eta ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(eta)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + arm + (1 | id)\n         lkg ~ 1 + arm + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 4 chains, each with iter = 3000; warmup = 2000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     0.58      0.02     0.55     0.61 1.01      356      987\nsd(lks_Intercept)     1.48      0.09     1.31     1.66 1.01      402      917\nsd(lkg_Intercept)     0.97      0.05     0.88     1.07 1.01      323      789\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.16      0.00     0.16     0.17 1.00     1725     2755\nlb0_Intercept     3.61      0.02     3.57     3.66 1.00      199      519\nlks_Intercept    -0.78      0.09    -0.95    -0.61 1.00     1204     2303\nlks_arm2         -0.40      0.16    -0.73    -0.09 1.01      383      823\nlkg_Intercept    -1.26      0.11    -1.48    -1.05 1.01      494      853\nlkg_arm2          0.03      0.13    -0.22     0.28 1.01      460      975\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s have a look at the parameter estimates then:\n\n\nShow the code\npost_df_by_arm &lt;- as_draws_df(fit_by_arm)\nhead(names(post_df_by_arm), 10)\n\n\n [1] \"b_tau_Intercept\"        \"b_lb0_Intercept\"        \"b_lks_Intercept\"       \n [4] \"b_lks_arm2\"             \"b_lkg_Intercept\"        \"b_lkg_arm2\"            \n [7] \"sd_id__lb0_Intercept\"   \"sd_id__lks_Intercept\"   \"sd_id__lkg_Intercept\"  \n[10] \"r_id__lb0[1,Intercept]\"\n\n\nShow the code\npost_df_by_arm &lt;- post_df_by_arm |&gt;\n  mutate(\n    theta_b0 = exp(b_lb0_Intercept + sd_id__lb0_Intercept^2 / 2),\n    theta_ks_arm1 = exp(b_lks_Intercept + sd_id__lks_Intercept^2 / 2),\n    theta_ks_arm2 = exp(b_lks_Intercept + b_lks_arm2 + sd_id__lks_Intercept^2 / 2),\n    theta_kg_arm1 = exp(b_lkg_Intercept + sd_id__lkg_Intercept^2 / 2),\n    theta_kg_arm2 = exp(b_lkg_Intercept + b_lkg_arm2 + sd_id__lkg_Intercept^2 / 2),\n    omega_0 = sd_id__lb0_Intercept,\n    omega_s = sd_id__lks_Intercept,\n    omega_g = sd_id__lkg_Intercept,\n    cv_0 = sqrt(exp(sd_id__lb0_Intercept^2) - 1),\n    cv_s = sqrt(exp(sd_id__lks_Intercept^2) - 1),\n    cv_g = sqrt(exp(sd_id__lkg_Intercept^2) - 1),\n    sigma = b_tau_Intercept\n  )\n\npost_sum_by_arm &lt;- post_df_by_arm |&gt;\n  select(\n    theta_b0, theta_ks_arm1, theta_ks_arm2, theta_kg_arm1, theta_kg_arm2, \n    omega_0, omega_s, omega_g, cv_0, cv_s, cv_g, sigma\n  ) |&gt;\n  summarize_draws() |&gt;\n  gt() |&gt;\n  fmt_number(n_sigfig = 3)\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\nShow the code\npost_sum_by_arm\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta_b0\n43.9\n43.9\n1.10\n1.11\n42.1\n45.8\n1.00\n195\n515\n\n\ntheta_ks_arm1\n1.39\n1.36\n0.193\n0.178\n1.11\n1.74\n1.01\n396\n841\n\n\ntheta_ks_arm2\n0.925\n0.918\n0.112\n0.108\n0.755\n1.12\n1.00\n306\n814\n\n\ntheta_kg_arm1\n0.456\n0.454\n0.0461\n0.0456\n0.382\n0.535\n1.01\n575\n1,000\n\n\ntheta_kg_arm2\n0.469\n0.468\n0.0425\n0.0423\n0.402\n0.544\n1.00\n496\n923\n\n\nomega_0\n0.578\n0.578\n0.0161\n0.0160\n0.552\n0.605\n1.00\n345\n979\n\n\nomega_s\n1.48\n1.48\n0.0909\n0.0899\n1.33\n1.63\n1.01\n397\n900\n\n\nomega_g\n0.968\n0.965\n0.0485\n0.0474\n0.892\n1.05\n1.00\n343\n808\n\n\ncv_0\n0.630\n0.630\n0.0206\n0.0205\n0.597\n0.665\n1.00\n345\n979\n\n\ncv_s\n2.84\n2.79\n0.445\n0.416\n2.21\n3.65\n1.01\n397\n900\n\n\ncv_g\n1.25\n1.24\n0.0976\n0.0929\n1.10\n1.43\n1.00\n343\n808\n\n\nsigma\n0.161\n0.161\n0.00229\n0.00230\n0.157\n0.165\n1.00\n1,700\n2,710\n\n\n\n\n\n\n\nSo we see that the shrinkage is stronger in arm 1 compared to arm 2, while the growth rates are similar. We could also calculate the posterior probability that the growth rate is different between the two arms, for example:\n\n\nShow the code\npost_df_by_arm |&gt;\n  mutate(diff_pos = theta_ks_arm1 - theta_ks_arm2 &gt; 0) |&gt;\n  summarize(diff_prob = mean(diff_pos))\n\n\n# A tibble: 1 × 1\n  diff_prob\n      &lt;dbl&gt;\n1     0.996\n\n\nSo we see that the posterior probability that the shrinkage rate is higher in arm 1 compared to arm 2 is quite high.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#model-comparison-with-loo",
    "href": "session-tgi/1_tgi_sf_brms.html#model-comparison-with-loo",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Model comparison with LOO",
    "text": "Model comparison with LOO\nThe LOO criterion is a widely used method for comparing models. It is based on the idea of leave-one-out cross-validation, but is more efficient to compute.\nWith brms, it is easy to compute the LOO criterion:\n\n\nShow the code\nloo(fit)\n\n\nWarning: Found 374 observations with a pareto_k &gt; 0.7 in model 'fit'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\n\nComputed from 4000 by 4099 log-likelihood matrix.\n\n         Estimate    SE\nelpd_loo -12990.0  97.3\np_loo      1185.2  40.4\nlooic     25980.0 194.7\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.1, 2.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     3725  90.9%   56      \n   (0.7, 1]   (bad)       303   7.4%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   71   1.7%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nA helpful glossary explaining the LOO statistics can be found here.\nDifferent criteria are available, for example the elpd_loo is the expected log pointwise predictive density, and the looic = -2 * elpd_loo is the LOO information criterion. For comparing models, the looic is often used, where smaller numbers are better. it is kind of an equivalent to the AIC, but based on the LOO criterion.\nLet’s e.g. compare the two models we fitted above, one for the whole dataset and one with the treatment arm as a covariate for the shrinkage and growth rates:\n\n\nShow the code\nfit &lt;- add_criterion(fit, \"loo\")\n\n\nWarning: Found 374 observations with a pareto_k &gt; 0.7 in model 'fit'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nShow the code\nfit_by_arm &lt;- add_criterion(fit_by_arm, \"loo\")\n\n\nWarning: Found 359 observations with a pareto_k &gt; 0.7 in model 'fit_by_arm'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nShow the code\nloo_compare(fit, fit_by_arm)\n\n\n           elpd_diff se_diff\nfit         0.0       0.0   \nfit_by_arm -0.2       5.9   \n\n\nSo the model without treatment arm seems to be very slightly preferred here by the LOO criterion. Nevertheless, the model with treatment arm might answer exactly the question we need to answer, so it is important to consider the context of the analysis.",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-tgi/1_tgi_sf_brms.html#tips-and-tricks",
    "href": "session-tgi/1_tgi_sf_brms.html#tips-and-tricks",
    "title": "1. TGI model minimal workflow with brms",
    "section": "Tips and tricks",
    "text": "Tips and tricks\n\nWhen the model breaks down completely, that is a sign that likely the model specification is wrong. For example, in above code I first did not understand how to model the sigma parameter correctly, which led to a model where each chain was completely stuck at its initial values.\nIn that case and in general if you are not sure whether the brms model specification is correct, you can check the Stan code that is generated by brms:\n\n\nShow the code\n# Good to check the stan code:\n# (This also helps to find the names of the parameters\n# for which to define the initial values below)\nstancode(formula, prior = priors, data = df, family = gaussian())\n\n\n// generated with brms 2.22.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=1&gt; K_tau;  // number of population-level effects\n  matrix[N, K_tau] X_tau;  // population-level design matrix\n  int&lt;lower=1&gt; K_lb0;  // number of population-level effects\n  matrix[N, K_lb0] X_lb0;  // population-level design matrix\n  int&lt;lower=1&gt; K_lks;  // number of population-level effects\n  matrix[N, K_lks] X_lks;  // population-level design matrix\n  int&lt;lower=1&gt; K_lkg;  // number of population-level effects\n  matrix[N, K_lkg] X_lkg;  // population-level design matrix\n  // covariates for non-linear functions\n  vector[N] C_ystar_1;\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_lb0_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_lks_1;\n  // data for group-level effects of ID 3\n  int&lt;lower=1&gt; N_3;  // number of grouping levels\n  int&lt;lower=1&gt; M_3;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_3;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_3_lkg_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector&lt;lower=0&gt;[K_tau] b_tau;  // regression coefficients\n  vector[K_lb0] b_lb0;  // regression coefficients\n  vector[K_lks] b_lks;  // regression coefficients\n  vector[K_lkg] b_lkg;  // regression coefficients\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_3] sd_3;  // group-level standard deviations\n  array[M_3] vector[N_3] z_3;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_lb0_1;  // actual group-level effects\n  vector[N_2] r_2_lks_1;  // actual group-level effects\n  vector[N_3] r_3_lkg_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_lb0_1 = (sd_1[1] * (z_1[1]));\n  r_2_lks_1 = (sd_2[1] * (z_2[1]));\n  r_3_lkg_1 = (sd_3[1] * (z_3[1]));\n  lprior += normal_lpdf(b_tau | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(b_lb0 | log(65), 1);\n  lprior += normal_lpdf(b_lks | log(0.52), 0.1);\n  lprior += normal_lpdf(b_lkg | log(1.04), 1);\n  lprior += normal_lpdf(sd_1 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(sd_2 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n  lprior += normal_lpdf(sd_3 | 0, 3)\n    - 1 * normal_lccdf(0 | 0, 3);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] nlp_tau = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lb0 = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lks = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_lkg = rep_vector(0.0, N);\n    // initialize non-linear predictor term\n    vector[N] nlp_b0;\n    // initialize non-linear predictor term\n    vector[N] nlp_ks;\n    // initialize non-linear predictor term\n    vector[N] nlp_kg;\n    // initialize non-linear predictor term\n    vector[N] nlp_ystar;\n    // initialize non-linear predictor term\n    vector[N] mu;\n    // initialize non-linear predictor term\n    vector[N] sigma;\n    nlp_tau += X_tau * b_tau;\n    nlp_lb0 += X_lb0 * b_lb0;\n    nlp_lks += X_lks * b_lks;\n    nlp_lkg += X_lkg * b_lkg;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lb0[n] += r_1_lb0_1[J_1[n]] * Z_1_lb0_1[n];\n    }\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lks[n] += r_2_lks_1[J_2[n]] * Z_2_lks_1[n];\n    }\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      nlp_lkg[n] += r_3_lkg_1[J_3[n]] * Z_3_lkg_1[n];\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_b0[n] = (exp(nlp_lb0[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_ks[n] = (exp(nlp_lks[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_kg[n] = (exp(nlp_lkg[n]));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      nlp_ystar[n] = (int_step(C_ystar_1[n] &gt; 0) * (nlp_b0[n] * (exp( - nlp_ks[n] * C_ystar_1[n]) + exp(nlp_kg[n] * C_ystar_1[n]) - 1)) + int_step(C_ystar_1[n] &lt;= 0) * (nlp_b0[n] * exp(nlp_kg[n] * C_ystar_1[n])));\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      mu[n] = (nlp_ystar[n]);\n    }\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      sigma[n] = exp(log(nlp_tau[n]) + log(nlp_ystar[n]));\n    }\n    target += normal_lpdf(Y | mu, sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n  target += std_normal_lpdf(z_3[1]);\n}\ngenerated quantities {\n}\n\n\nHere e.g. it is important to see that the sigma parameter is modelled on the log scale.\nWe can also extract the actual data that is passed to the Stan program:\n\n\nShow the code\n# Extract the data that is passed to the Stan program\nstan_data &lt;- standata(formula, prior = priors, data = df, family = gaussian())\n\n# Check that the time variable is correct\nall(stan_data$C_eta_1 == df$year)\n\n\n[1] TRUE\n\n\nThis can be useful to check if the data is passed correctly to the Stan program.\nIt is important to take divergence warnings seriously. For the model parameters where Rhat is larger than 1.05 or so, you can just have a look at the traceplot. For example, in an earlier model fit the \\(\\log(\\mu_{\\phi})\\) population parameter had this traceplot:\n\n\nShow the code\nfit_save &lt;- fit\nload(here(\"session-tgi/fit.RData\"))\nplot(fit, pars = \"b_tphi\")\n\n\nWarning: Argument 'pars' is deprecated. Please use 'variable' instead.\n\n\n\n\n\n\n\n\n\nShow the code\nfit &lt;- fit_save\n\n\nWe see that two chains led to a \\(\\log(\\mu_{\\phi})\\) value below 0, while two chains hovered around much larger values between 5 and 15, which corresponds to \\(\\mu_{\\phi} = 1\\). This shows that the model is rather overparametrized. By restricting the range of the prior for \\(\\log(\\mu_{\\phi})\\) to be below 0, we could avoid this problem.\nProviding manually realistic initial values for the model parameters can help to speed up the convergence of the Markov chains. This is especially important for the nonlinear parameters. You can get the names of the parameters from the Stan code above - note that you cannot use the names of the brms R code.\nSometimes it is not easy to get the dimensions of the inital values right. Here it can be helpful to change the backend to rstan, which provides more detailed error messages. You can do this by setting backend = \"rstan\" in the brm call.\nWhen the Stan compiler gives you an error, that can be very informative: Just open the Stan code file that is mentioned in the error message and look at the line number that is mentioned. This can give you a good idea of what went wrong: Maybe a typo in the prior definition, or a missing semicolon, or a wrong dimension in the data block. With VScode e.g. this is very easy: Just hold Ctrl and click on the file name and number, and you will be taken to the right line in the Stan code.\nIn order to quickly get results for a model, e.g. for obtaining useful starting values or prior distributions, it can be worth trying the “Pathfinder” algorithm in brms. Pathfinder is a variational method for approximately sampling from differentiable log densities. You can use it by setting algorithm = \"pathfinder\" in the brm call. In this example it takes only a minute, compared to more than an hour for the full model fit. However, the results are still quite different, so it is likely only useful as a first approximation. Nevertheless, the individual model fits look quite encouraging, in the sense that they have found good parameter values - but they are still lacking any uncertainty, so could not be used for confidence or prediction intervals:\n\n\nShow the code\nfit_fast_file &lt;- here(\"session-tgi/fit_fast.RData\")\nif (file.exists(fit_fast_file)) {\n  load(fit_fast_file)\n} else {\n  fit_fast &lt;- brm(\n    formula = formula,\n    data = df,\n    prior = priors,\n    family = gaussian(),\n    init = rep(list(inits), CHAINS),\n    chains = CHAINS, \n    iter = ITER + WARMUP, \n    warmup = WARMUP, \n    seed = BAYES.SEED,\n    refresh = REFRESH,\n    algorithm = \"pathfinder\"\n  )\n  save(fit_fast, file = fit_fast_file)\n}\n\nsummary(fit_fast)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: sld ~ ystar \n         ystar ~ int_step(year &gt; 0) * (b0 * (exp(-ks * year) + exp(kg * year) - 1)) + int_step(year &lt;= 0) * (b0 * exp(kg * year))\n         sigma ~ log(tau) + log(ystar)\n         tau ~ 1\n         b0 ~ exp(lb0)\n         ks ~ exp(lks)\n         kg ~ exp(lkg)\n         lb0 ~ 1 + (1 | id)\n         lks ~ 1 + (1 | id)\n         lkg ~ 1 + (1 | id)\n   Data: df (Number of observations: 4099) \n  Draws: 1 chains, each with iter = 1000; warmup = 0; thin = 1;\n         total post-warmup draws = 1000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 701) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(lb0_Intercept)     3.37      0.00     3.37     3.37   NA       NA       NA\nsd(lks_Intercept)     9.16      0.00     9.16     9.16   NA       NA       NA\nsd(lkg_Intercept)     6.90      0.00     6.90     6.90   NA       NA       NA\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntau_Intercept     0.13      0.00     0.13     0.13   NA       NA       NA\nlb0_Intercept     3.62      0.00     3.62     3.62   NA       NA       NA\nlks_Intercept    -0.49      0.00    -0.49    -0.49   NA       NA       NA\nlkg_Intercept    -0.66      0.00    -0.66    -0.66   NA       NA       NA\n\nDraws were sampled using pathfinder(). \n\n\nShow the code\ndf_sim &lt;- df_subset |&gt; \n  data_grid(\n    id = pt_subset, \n    year = seq_range(year, 101)\n  ) |&gt;\n  add_linpred_draws(fit_fast) |&gt; \n  median_qi() |&gt; \n  mutate(sld_pred = .linpred)\n\ndf_sim |&gt;\n  ggplot(aes(x = year, y = sld)) +\n  facet_wrap(~ id) +\n  geom_ribbon(\n    aes(y = sld_pred, ymin = .lower, ymax = .upper),\n    alpha = 0.3, \n    fill = \"deepskyblue\"\n  ) +\n  geom_line(aes(y = sld_pred), color = \"deepskyblue\") +\n  geom_point(data = df_subset, color = \"tomato\") +\n  coord_cartesian(ylim = range(df_subset$sld)) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"SF model fit (pathfinder approximation)\")",
    "crumbs": [
      "Session 1: TGI",
      "1. TGI model minimal workflow with `brms`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html",
    "href": "session-os/1_os_weibull_jmpost.html",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "",
    "text": "The purpose of this document is to show a minimal workflow for fitting a Weibull OS model using the jmpost package.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#setup-and-load-data",
    "href": "session-os/1_os_weibull_jmpost.html#setup-and-load-data",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Setup and load data",
    "text": "Setup and load data\nHere we execute the R code from the setup and data preparation chapter, see the full code here.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#tgi-model-fitting",
    "href": "session-os/1_os_weibull_jmpost.html#tgi-model-fitting",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "TGI model fitting",
    "text": "TGI model fitting\nLet’s use jmpost to fit the Stein-Fojo model to the TGI dataset. This works analogously to what we showed in the previous session.\nFirst we again prepare the data objects, starting with the subject level data:\n\n\nShow the code\nsubj_df &lt;- os_data |&gt;\n    mutate(study = \"OAK\") |&gt;\n    select(study, id, arm)\n\nsubj_data &lt;- DataSubject(\n    data = subj_df,\n    subject = \"id\",\n    arm = \"arm\",\n    study = \"study\"\n)\n\n\nNext we prepare the longitudinal data object.\n\n\nShow the code\nlong_df &lt;- tumor_data |&gt;\n    select(id, year, sld)\nlong_data &lt;- DataLongitudinal(\n    data = long_df,\n    formula = sld ~ year\n)\n\n\nNow we can create the JointData object for the TGI model:\n\n\nShow the code\ntgi_joint_data &lt;- DataJoint(\n    subject = subj_data,\n    longitudinal = long_data\n)\n\n\nWe specify the Stein-Fojo model together with the priors for the model parameters:\n\n\nShow the code\ntgi_mod &lt;- JointModel(\n    longitudinal = LongitudinalSteinFojo(\n        mu_bsld = prior_normal(log(65), 1),\n        mu_ks = prior_normal(log(0.52), 1),\n        mu_kg = prior_normal(log(1.04), 1),\n        omega_bsld = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_ks = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        omega_kg = prior_normal(0, 3) |&gt; set_limits(0, Inf),\n        sigma = prior_normal(0, 3) |&gt; set_limits(0, Inf)\n    )\n)\n\n\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-os/tgi1.rds\")\nif (file.exists(save_file)) {\n    tgi_results &lt;- readRDS(save_file)\n} else {\n    tgi_results &lt;- sampleStanModel(\n        tgi_mod,\n        data = tgi_joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(tgi_results, file = save_file)\n}\n\n\nThe function saveObject() was added to the package recently, please update your installation if it is not yet available.\nNote that this is considerably faster than fitting the larger dataset of 701 patients. Let’s check the convergence of the population parameters:\n\n\nShow the code\nvars &lt;- c(\n    \"lm_sf_mu_bsld\",\n    \"lm_sf_mu_ks\",\n    \"lm_sf_mu_kg\",\n    \"lm_sf_sigma\",\n    \"lm_sf_omega_bsld\",\n    \"lm_sf_omega_ks\",\n    \"lm_sf_omega_kg\"\n)\n\nmcmc_tgi_results &lt;- cmdstanr::as.CmdStanMCMC(tgi_results)\nmcmc_tgi_results$summary(vars)\n\n\n# A tibble: 11 × 10\n   variable    mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 lm_sf_mu…  3.75   3.75  0.0375  0.0376   3.69   3.81  1.01      298.     572.\n 2 lm_sf_mu… -0.270 -0.255 0.298   0.308   -0.789  0.163 1.00      641.     680.\n 3 lm_sf_mu… -1.22  -1.16  0.360   0.343   -1.90  -0.708 0.999     723.     852.\n 4 lm_sf_mu… -0.731 -0.723 0.174   0.167   -1.02  -0.462 1.00      604.     762.\n 5 lm_sf_mu… -0.973 -0.969 0.157   0.150   -1.25  -0.728 1.00      641.     869.\n 6 lm_sf_si…  0.129  0.129 0.00381 0.00373  0.123  0.135 1.00      883.     892.\n 7 lm_sf_om…  0.531  0.529 0.0288  0.0294   0.486  0.583 1.00      518.     751.\n 8 lm_sf_om…  1.09   1.08  0.212   0.207    0.771  1.44  1.00      704.     861.\n 9 lm_sf_om…  1.45   1.42  0.271   0.251    1.07   1.95  0.999     731.     994.\n10 lm_sf_om…  0.694  0.684 0.0963  0.0881   0.556  0.860 1.01      930.     904.\n11 lm_sf_om…  0.994  0.983 0.103   0.100    0.840  1.17  1.01      762.     869.\n\n\nShow the code\ndraws_tgi_results &lt;- mcmc_tgi_results$draws(vars)\nmcmc_trace(draws_tgi_results)\n\n\n\n\n\n\n\n\n\nSo this looks good.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#extract-individual-growth-rate-estimates",
    "href": "session-os/1_os_weibull_jmpost.html#extract-individual-growth-rate-estimates",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Extract individual growth rate estimates",
    "text": "Extract individual growth rate estimates\nWe can now extract the individual growth rate estimates from the model. Since the relevant random effect parameter samples are already stored in the mcmc_tgi_results object, we can directly extract the posterior means and credible intervals for the growth rates using the summary method. The only tricky part is that we need to match the IDs of the patients manually, because jmpost just numbers the patients in the order they appear in the data, which is then the index for all the random effects and individual growth parameters \\(\\psi_{\\text{kg}, i}\\).\n\n\nShow the code\nsubj_kg_est &lt;- mcmc_tgi_results$summary(\"lm_sf_psi_kg\") |&gt;\n    mutate(id = subj_df$id)\n\nhead(subj_kg_est)\n\n\n# A tibble: 6 × 11\n  variable    mean median    sd   mad     q5   q95  rhat ess_bulk ess_tail id   \n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;\n1 lm_sf_psi… 0.514  0.446 0.349 0.397 0.0707 1.14  1.00      937.    1037. 588  \n2 lm_sf_psi… 0.479  0.354 0.400 0.287 0.0729 1.28  0.999     929.     731. 330  \n3 lm_sf_psi… 0.387  0.402 0.115 0.111 0.179  0.554 1.00      888.     717. 791  \n4 lm_sf_psi… 0.549  0.482 0.298 0.287 0.168  1.12  1.00      857.     950. 635  \n5 lm_sf_psi… 0.473  0.356 0.390 0.304 0.0729 1.24  1.01      976.     852. 365  \n6 lm_sf_psi… 0.307  0.251 0.213 0.198 0.0518 0.732 1.00      905.     915. 773  \n\n\nWe now add the e.g. posterior mean estimate of the individual growth rates to the OS data set, such that we will be able to use it below as a covariate in the OS model:\n\n\nShow the code\nos_data_with_kg_est &lt;- os_data |&gt;\n    select(id, arm, ecog, age, race, sex, os_time, os_event) |&gt;\n    left_join(select(subj_kg_est, mean, id), by = \"id\") |&gt;\n    rename(kg_est = mean)\nhead(os_data_with_kg_est)\n\n\n# A tibble: 6 × 9\n  id    arm       ecog    age race  sex   os_time os_event kg_est\n  &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;lgl&gt;     &lt;dbl&gt;\n1 588   Docetaxel 0        61 WHITE F       2.05  FALSE     0.514\n2 330   MPDL3280A 1        56 WHITE F       1.68  FALSE     0.479\n3 791   Docetaxel 0        72 WHITE F       0.901 TRUE      0.387\n4 635   Docetaxel 0        42 OTHER F       1.66  TRUE      0.549\n5 365   MPDL3280A 0        64 WHITE F       1.43  TRUE      0.473\n6 773   Docetaxel 0        65 WHITE M       1.63  FALSE     0.307\n\n\nShow the code\nsave_file &lt;- here(\"session-os/os_data_with_kg.rds\")\nif (!file.exists(save_file)) {\n    saveRDS(os_data_with_kg_est, file = save_file)\n}",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#os-model-fitting",
    "href": "session-os/1_os_weibull_jmpost.html#os-model-fitting",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "OS model fitting",
    "text": "OS model fitting\nNow we can fit the OS model. We start by preparing the data objects.\n\n\nShow the code\nsurv_data &lt;- DataSurvival(\n    data = os_data_with_kg_est,\n    formula = Surv(os_time, os_event) ~ arm + ecog + age + race + sex + kg_est\n)\n\n\nNote that we are both including the treatment arm as well as the growth rate estimate here as covariates in the model, alongside the ECOG score, age, race and sex. The idea is that we want to understand whether there is additional information in the growth rate estimates, adjusting separately for the treatment arm.\nNow we can create the JointData object for the OS model:\n\n\nShow the code\nos_joint_data &lt;- DataJoint(\n    subject = subj_data,\n    survival = surv_data\n)\n\n\nWe specify the Weibull model together with the priors for the model parameters. We take vague priors for the regression coefficients beta. For lambda and gamma, we start from the scale of the survival data at hand: the average survival time is 1.3 years, just taking a crude average of all survival times.\nWe can quickly write the function that gives the mean of the Weibull distribution with fixed lambda and gamma:\n\n\nShow the code\nweibull_mean &lt;- function(lambda, gamma) {\n    base::gamma(1 + 1 / gamma) / lambda\n}\n\n\nTherefore, playing around with this a bit, we can e.g. center the prior for lambda around 0.7 and the prior for gamma around 1.5, giving a mean survival time of 1.3 years.\nIf we want to use Gamma distributions e.g. for lambda and gamma, we can use the prior_gamma function. The two parameters of this distribution are the shape and the rate. The mean is shape divided by the rate. So easiest is to keep a rate of 1 and just set the shape to the mean value we need:\n\n\nShow the code\nos_mod &lt;- JointModel(\n    survival = SurvivalWeibullPH(\n        lambda = prior_gamma(0.7, 1),\n        gamma = prior_gamma(1.5, 1),\n        beta = prior_normal(0, 20)\n    )\n)\n\n\nBecause we use a large prior variance for beta, we need to adjust the default initial value construction used in jmpost. As explained here, we can change the shrinkage of the initial values to the mean. We can then check what the initial values will be, to make sure that they are reasonable:\n\n\nShow the code\noptions(\"jmpost.prior_shrinkage\" = 0.99)\n\ninitialValues(os_mod, n_chains = CHAINS)\n\n\n[[1]]\n[[1]]$sm_weibull_ph_lambda\n[1] 0.6972239\n\n[[1]]$sm_weibull_ph_gamma\n[1] 1.510107\n\n[[1]]$beta_os_cov\n[1] 0.3756408\n\n\n[[2]]\n[[2]]$sm_weibull_ph_lambda\n[1] 0.7050865\n\n[[2]]$sm_weibull_ph_gamma\n[1] 1.493971\n\n[[2]]$beta_os_cov\n[1] -0.06619016\n\n\n[[3]]\n[[3]]$sm_weibull_ph_lambda\n[1] 0.6978574\n\n[[3]]$sm_weibull_ph_gamma\n[1] 1.488545\n\n[[3]]$beta_os_cov\n[1] -0.07924588\n\n\n[[4]]\n[[4]]$sm_weibull_ph_lambda\n[1] 0.7009439\n\n[[4]]$sm_weibull_ph_gamma\n[1] 1.509077\n\n[[4]]$beta_os_cov\n[1] 0.2440759\n\n\nNow we can fit the model:\n\n\nShow the code\nsave_file &lt;- here(\"session-os/os1.rds\")\nif (file.exists(save_file)) {\n    os_results &lt;- readRDS(save_file)\n} else {\n    os_results &lt;- sampleStanModel(\n        os_mod,\n        data = os_joint_data,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(os_results, file = save_file)\n}\n\n\nNote that here we can get warnings at the beginning of the chains’ sampling process (“The current Metropolis proposal is about to be rejected …”). As long as this only happens in the beginning, and not during the sampling later, then this is not a cause for concern.\nLet’s check the convergence of the population parameters:\n\n\nShow the code\nvars &lt;- c(\n    \"beta_os_cov\",\n    \"sm_weibull_ph_gamma\",\n    \"sm_weibull_ph_lambda\"\n)\n\nmcmc_os_results &lt;- cmdstanr::as.CmdStanMCMC(os_results)\nmcmc_os_results$summary(vars)\n\n\n# A tibble: 9 × 10\n  variable          mean   median      sd     mad      q5     q95  rhat ess_bulk\n  &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 beta_os_cov[… -0.346   -0.342   0.191   0.191   -0.667  -0.0394 1.01      990.\n2 beta_os_cov[…  0.616    0.621   0.204   0.211    0.294   0.944  0.998     925.\n3 beta_os_cov[…  0.00225  0.00196 0.00978 0.00913 -0.0135  0.0194 1.00      744.\n4 beta_os_cov[…  0.508    0.509   0.409   0.402   -0.164   1.17   1.00      935.\n5 beta_os_cov[…  0.0790   0.0741  0.231   0.224   -0.301   0.481  1.00      984.\n6 beta_os_cov[…  0.282    0.278   0.209   0.212   -0.0738  0.622  0.998     903.\n7 beta_os_cov[…  0.302    0.317   0.201   0.202   -0.0435  0.602  1.00      922.\n8 sm_weibull_p…  1.66     1.67    0.139   0.143    1.43    1.90   1.00     1089.\n9 sm_weibull_p…  0.176    0.145   0.126   0.0889   0.0484  0.419  1.00      740.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nShow the code\ndraws_os_results &lt;- mcmc_os_results$draws(vars)\nmcmc_trace(draws_os_results)",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#interpret-covariate-effects",
    "href": "session-os/1_os_weibull_jmpost.html#interpret-covariate-effects",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Interpret covariate effects",
    "text": "Interpret covariate effects\nIn order to better see which of the coefficients relate to which covariates, we can rename them as follows:\n\n\nShow the code\nsurv_data_design &lt;- as_stan_list(surv_data)$os_cov_design\nos_cov_names &lt;- colnames(surv_data_design)\nold_coef_names &lt;- glue::glue(\"beta_os_cov[{seq_along(os_cov_names)}]\")\ndraws_os_results &lt;- do.call(\n    rename_variables,\n    c(list(draws_os_results), setNames(old_coef_names, os_cov_names))\n)\nmcmc_dens_overlay(draws_os_results) +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nShow the code\nsummary(draws_os_results)\n\n\n# A tibble: 9 × 10\n  variable          mean   median      sd     mad      q5     q95  rhat ess_bulk\n  &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 armMPDL3280A  -0.346   -0.342   0.191   0.191   -0.667  -0.0394 1.01      990.\n2 ecog1          0.616    0.621   0.204   0.211    0.294   0.944  0.998     925.\n3 age            0.00225  0.00196 0.00978 0.00913 -0.0135  0.0194 1.00      744.\n4 raceOTHER      0.508    0.509   0.409   0.402   -0.164   1.17   1.00      935.\n5 raceWHITE      0.0790   0.0741  0.231   0.224   -0.301   0.481  1.00      984.\n6 sexM           0.282    0.278   0.209   0.212   -0.0738  0.622  0.998     903.\n7 kg_est         0.302    0.317   0.201   0.202   -0.0435  0.602  1.00      922.\n8 sm_weibull_p…  1.66     1.67    0.139   0.143    1.43    1.90   1.00     1089.\n9 sm_weibull_p…  0.176    0.145   0.126   0.0889   0.0484  0.419  1.00      740.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nShow the code\nsave_file &lt;- here(\"session-os/os_draws.rds\")\nif (!file.exists(save_file)) {\n    saveRDS(draws_os_results, file = save_file)\n}\n\n\nSo we can see that the 90% credible interval (CI) for the covariates arm and ecog1 excludes 0, so these are “significant” predictors of the hazard rate. On the other hand, the race variable indicator and age variables’ CIs clearly include 0. The situation is less clear for sex and kg_est, the estimated growth rate: here the CIs barely include 0. The posterior probabilities for a hazard ratio above 1 are:\n\n\nShow the code\ndraws_os_results |&gt;\n    as_draws_df() |&gt;\n    select(sexM, kg_est) |&gt;\n    summarise_all(~ mean(. &gt; 0))\n\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n# A tibble: 1 × 2\n   sexM kg_est\n  &lt;dbl&gt;  &lt;dbl&gt;\n1 0.904  0.927\n\n\nSo we have a more than 90% posterior probability that male patients have a higher hazard than females, and that patients with a higher estimated growth rate have a higher hazard than those with a lower growth rate - and this holds true even after adjusting for the treatment arm.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#observation-vs-model-fit",
    "href": "session-os/1_os_weibull_jmpost.html#observation-vs-model-fit",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Observation vs model fit",
    "text": "Observation vs model fit\nA useful plot displays the model predicted survival function and overlays the non-parametric Kaplan-Meier plot to it. Such a plot is easily obtained using the autoplot() function, as we will see below.\nThe first step consists in generating the survival predictions at the group level with the SurvivalQuantities() function. It is recommended to specify the sequence of time points at which the predictions should be made (using the argument times):\n\n\nShow the code\ntime_grid &lt;- seq(from = 0, to = max(os_data_with_kg_est$os_time), length = 100)\nos_surv_group_grid &lt;- GridGrouped(\n    times = time_grid,\n    groups = with(\n        subj_df,\n        split(as.character(id), arm)\n    )\n)\nos_surv_pred &lt;- SurvivalQuantities(\n    object = os_results,\n    grid = os_surv_group_grid,\n    type = \"surv\"\n)\n\n\nNow we can use the autoplot() method:\n\n\nShow the code\nautoplot(os_surv_pred, add_km = TRUE, add_wrap = FALSE)",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#hazard-and-hazard-rate-estimation",
    "href": "session-os/1_os_weibull_jmpost.html#hazard-and-hazard-rate-estimation",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Hazard and hazard rate estimation",
    "text": "Hazard and hazard rate estimation\nSimilarly to the survival function estimation, we can also estimate the hazard function by treatment group.\n\n\nShow the code\nos_hazard_pred &lt;- SurvivalQuantities(\n    object = os_results,\n    grid = os_surv_group_grid,\n    type = \"haz\"\n)\n\n\nAlso this can be plotted using the autoplot() method:\n\n\nShow the code\nautoplot(os_hazard_pred, add_wrap = FALSE)\n\n\n\n\n\n\n\n\n\nFinally, we can also estimate the hazard rate, which is constant over time here - because we use the Weibull proportional hazards model. We still show this more complicated code here because it will also work later for joint TGI-OS models, where the hazard rate is not constant any longer.\n\n\nShow the code\nos_hr_est &lt;- os_hazard_pred |&gt;\n    as.data.frame() |&gt;\n    group_by(group, time) |&gt;\n    mutate(sample = row_number()) |&gt;\n    pivot_wider(names_from = group, values_from = values) |&gt;\n    mutate(hr = MPDL3280A / Docetaxel) |&gt;\n    group_by(time) |&gt;\n    summarize(\n        mean = mean(hr, na.rm = TRUE),\n        lower = quantile(hr, 0.05, na.rm = TRUE),\n        upper = quantile(hr, 0.95, na.rm = TRUE)\n    ) |&gt;\n    na.omit() # Omit the time = 0 which has NA\nsummary(os_hr_est)\n\n\n      time              mean            lower            upper       \n Min.   :0.02276   Min.   :0.7251   Min.   :0.5166   Min.   :0.9638  \n 1st Qu.:0.58038   1st Qu.:0.7251   1st Qu.:0.5166   1st Qu.:0.9638  \n Median :1.13801   Median :0.7251   Median :0.5166   Median :0.9638  \n Mean   :1.13801   Mean   :0.7251   Mean   :0.5166   Mean   :0.9638  \n 3rd Qu.:1.69563   3rd Qu.:0.7251   3rd Qu.:0.5166   3rd Qu.:0.9638  \n Max.   :2.25325   Max.   :0.7251   Max.   :0.5166   Max.   :0.9638  \n\n\nNow we can plot this:\n\n\nShow the code\nggplot(os_hr_est, aes(x = time, y = mean, ymin = lower, ymax = upper)) +\n    geom_line() +\n    geom_ribbon(alpha = 0.3)\n\n\n\n\n\n\n\n\n\nSimilar, but not identical numbers we can obtain here of course directly from the group covariate coefficient:\n\n\nShow the code\ndraws_os_results |&gt;\n    mutate_variables(hr = exp(armMPDL3280A)) |&gt;\n    subset(variable = \"hr\") |&gt;\n    summary()\n\n\n# A tibble: 1 × 10\n  variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 hr       0.720  0.710 0.138 0.136 0.513 0.961  1.01     990.     789.\n\n\nThe difference is due to the fact that the other covariates are ignored here by this simpler calculation.",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  },
  {
    "objectID": "session-os/1_os_weibull_jmpost.html#model-comparison",
    "href": "session-os/1_os_weibull_jmpost.html#model-comparison",
    "title": "1. OS model minimal workflow with jmpost",
    "section": "Model comparison",
    "text": "Model comparison\nWe can use the Brier score to compare two different survival models. The Brier score is a measure of the mean squared difference between the predicted survival probability and the actual survival status. The lower the Brier score, the better the model.\nTo calculate it, we need to use the GridFixed input for SurvivalQuantities():\n\n\nShow the code\nos_fixed_surv &lt;- SurvivalQuantities(\n    object = os_results,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\n\n# Current workaround if we have a logical event indicator:\nos_fixed_surv@data@survival@data$os_event &lt;- as.numeric(\n    os_fixed_surv@data@survival@data$os_event\n)\n\nos_mod1_bs &lt;- brierScore(os_fixed_surv)\n\n\nWe can also look at the LOOIC. As for the TGI model, we can use the loo() method in the CmdStanMCMC object to calculate it:\n\n\nShow the code\nos_mod1_looic &lt;- mcmc_os_results$loo(r_eff = FALSE)\n\n\nNow suppose we have a second model, where we omit the kg_est covariate. We can fit this model, just by omitting the kg_est covariate in the formula of the DataSurvival construction:\n\n\nShow the code\nsurv_data2 &lt;- DataSurvival(\n    data = os_data_with_kg_est,\n    formula = update(surv_data@formula, . ~ . - kg_est)\n)\nos_joint_data2 &lt;- DataJoint(\n    subject = subj_data,\n    survival = surv_data2\n)\nsave_file &lt;- here(\"session-os/os2.rds\")\nif (file.exists(save_file)) {\n    os_results2 &lt;- readRDS(save_file)\n} else {\n    os_results2 &lt;- sampleStanModel(\n        os_mod,\n        data = os_joint_data2,\n        iter_sampling = ITER,\n        iter_warmup = WARMUP,\n        chains = CHAINS,\n        parallel_chains = CHAINS,\n        thin = CHAINS,\n        seed = BAYES.SEED,\n        refresh = REFRESH\n    )\n    saveObject(os_results2, file = save_file)\n}\n\nmcmc_os_results2 &lt;- cmdstanr::as.CmdStanMCMC(os_results2)\n\n\nThen we can calculate the Brier score and LOOIC as well:\n\n\nShow the code\nos_fixed_surv2 &lt;- SurvivalQuantities(\n    object = os_results2,\n    grid = GridFixed(times = time_grid),\n    type = \"surv\"\n)\n\n# Current workaround if we have a logical event indicator:\nos_fixed_surv2@data@survival@data$os_event &lt;- as.numeric(\n    os_fixed_surv2@data@survival@data$os_event\n)\n\nos_mod2_bs &lt;- brierScore(os_fixed_surv2)\nos_mod2_looic &lt;- mcmc_os_results2$loo(r_eff = FALSE)\n\n\nNow we can compare the two models:\n\n\nShow the code\nos_mod1_looic\n\n\n\nComputed from 1000 by 203 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -192.9  5.7\np_loo         9.6  0.8\nlooic       385.7 11.5\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nShow the code\nos_mod2_looic\n\n\n\nComputed from 1000 by 203 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -193.3  5.6\np_loo         8.6  0.7\nlooic       386.6 11.2\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nShow the code\nloo_compare(os_mod1_looic, os_mod2_looic)\n\n\n       elpd_diff se_diff\nmodel1  0.0       0.0   \nmodel2 -0.4       1.4   \n\n\nSo we see that according to the LOOIC, the model with the kg_est covariate is slightly better than the model without it. However, the difference is small, and considering the difference in the expected log pointwise predictive density (ELPD) is small compared to the standard error, the improvement is not significant.\nWe can plot the Brier scores:\n\n\nShow the code\ndata.frame(\n    time = time_grid,\n    brier_score_diff = os_mod1_bs - os_mod2_bs\n) |&gt;\n    ggplot(aes(x = time, y = brier_score_diff)) +\n    geom_line() +\n    labs(y = \"Brier score difference (1 - 2)\") +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\nAlso here we can see that the differences are very small, but for most time points the model with kg_est is slightly better than the one without (because lower numbers are better and we looked at the difference model 1 minus model 2 scores).",
    "crumbs": [
      "Session 2: OS",
      "1. OS model minimal workflow with `jmpost`"
    ]
  }
]